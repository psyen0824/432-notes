<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Science for Biological, Medical and Health Research: Notes for 432</title>
  <meta name="description" content="These are the Course Notes for 432.">
  <meta name="generator" content="bookdown 0.6 and GitBook 2.6.7">

  <meta property="og:title" content="Data Science for Biological, Medical and Health Research: Notes for 432" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the Course Notes for 432." />
  <meta name="github-repo" content="thomaselove/432-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Science for Biological, Medical and Health Research: Notes for 432" />
  
  <meta name="twitter:description" content="These are the Course Notes for 432." />
  

<meta name="author" content="Thomas E. Love, Ph.D.">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="other-variable-selection-strategies.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">432 Course Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="r-packages-used-in-these-notes.html"><a href="r-packages-used-in-these-notes.html"><i class="fa fa-check"></i>R Packages used in these notes</a></li>
<li class="chapter" data-level="" data-path="data-used-in-these-notes.html"><a href="data-used-in-these-notes.html"><i class="fa fa-check"></i>Data used in these notes</a></li>
<li class="chapter" data-level="" data-path="special-functions-used-in-these-notes.html"><a href="special-functions-used-in-these-notes.html"><i class="fa fa-check"></i>Special Functions used in these notes</a></li>
<li class="chapter" data-level="1" data-path="building-table-1.html"><a href="building-table-1.html"><i class="fa fa-check"></i><b>1</b> Building Table 1</a><ul>
<li class="chapter" data-level="1.1" data-path="building-table-1.html"><a href="building-table-1.html#two-examples-from-the-new-england-journal-of-medicine"><i class="fa fa-check"></i><b>1.1</b> Two examples from the <em>New England Journal of Medicine</em></a><ul>
<li class="chapter" data-level="1.1.1" data-path="building-table-1.html"><a href="building-table-1.html#a-simple-table-1"><i class="fa fa-check"></i><b>1.1.1</b> A simple Table 1</a></li>
<li class="chapter" data-level="1.1.2" data-path="building-table-1.html"><a href="building-table-1.html#a-group-comparison"><i class="fa fa-check"></i><b>1.1.2</b> A group comparison</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="building-table-1.html"><a href="building-table-1.html#the-mr-clean-trial"><i class="fa fa-check"></i><b>1.2</b> The MR CLEAN trial</a></li>
<li class="chapter" data-level="1.3" data-path="building-table-1.html"><a href="building-table-1.html#simulated-fakestroke-data"><i class="fa fa-check"></i><b>1.3</b> Simulated <code>fakestroke</code> data</a></li>
<li class="chapter" data-level="1.4" data-path="building-table-1.html"><a href="building-table-1.html#building-table-1-for-fakestroke-attempt-1"><i class="fa fa-check"></i><b>1.4</b> Building Table 1 for <code>fakestroke</code>: Attempt 1</a><ul>
<li class="chapter" data-level="1.4.1" data-path="building-table-1.html"><a href="building-table-1.html#some-of-this-is-very-useful-and-other-parts-need-to-be-fixed."><i class="fa fa-check"></i><b>1.4.1</b> Some of this is very useful, and other parts need to be fixed.</a></li>
<li class="chapter" data-level="1.4.2" data-path="building-table-1.html"><a href="building-table-1.html#fakestroke-cleaning-up-categorical-variables"><i class="fa fa-check"></i><b>1.4.2</b> <code>fakestroke</code> Cleaning Up Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="building-table-1.html"><a href="building-table-1.html#fakestroke-table-1-attempt-2"><i class="fa fa-check"></i><b>1.5</b> <code>fakestroke</code> Table 1: Attempt 2</a><ul>
<li class="chapter" data-level="1.5.1" data-path="building-table-1.html"><a href="building-table-1.html#what-summaries-should-we-show"><i class="fa fa-check"></i><b>1.5.1</b> What summaries should we show?</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="building-table-1.html"><a href="building-table-1.html#obtaining-a-more-detailed-summary"><i class="fa fa-check"></i><b>1.6</b> Obtaining a more detailed Summary</a></li>
<li class="chapter" data-level="1.7" data-path="building-table-1.html"><a href="building-table-1.html#exporting-the-completed-table-1-from-r-to-excel-or-word"><i class="fa fa-check"></i><b>1.7</b> Exporting the Completed Table 1 from R to Excel or Word</a><ul>
<li class="chapter" data-level="1.7.1" data-path="building-table-1.html"><a href="building-table-1.html#approach-a-save-and-open-in-excel"><i class="fa fa-check"></i><b>1.7.1</b> Approach A: Save and open in Excel</a></li>
<li class="chapter" data-level="1.7.2" data-path="building-table-1.html"><a href="building-table-1.html#approach-b-produce-the-table-so-you-can-cut-and-paste-it"><i class="fa fa-check"></i><b>1.7.2</b> Approach B: Produce the Table so you can cut and paste it</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="building-table-1.html"><a href="building-table-1.html#a-controlled-biological-experiment---the-blood-brain-barrier"><i class="fa fa-check"></i><b>1.8</b> A Controlled Biological Experiment - The Blood-Brain Barrier</a></li>
<li class="chapter" data-level="1.9" data-path="building-table-1.html"><a href="building-table-1.html#the-bloodbrain.csv-file"><i class="fa fa-check"></i><b>1.9</b> The <code>bloodbrain.csv</code> file</a></li>
<li class="chapter" data-level="1.10" data-path="building-table-1.html"><a href="building-table-1.html#a-table-1-for-bloodbrain"><i class="fa fa-check"></i><b>1.10</b> A Table 1 for <code>bloodbrain</code></a><ul>
<li class="chapter" data-level="1.10.1" data-path="building-table-1.html"><a href="building-table-1.html#generate-final-table-1-for-bloodbrain"><i class="fa fa-check"></i><b>1.10.1</b> Generate final Table 1 for <code>bloodbrain</code></a></li>
<li class="chapter" data-level="1.10.2" data-path="building-table-1.html"><a href="building-table-1.html#a-more-finished-version-after-cleanup-in-word"><i class="fa fa-check"></i><b>1.10.2</b> A More Finished Version (after Cleanup in Word)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html"><i class="fa fa-check"></i><b>2</b> Linear Regression on a small SMART data set</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#brfss-and-smart"><i class="fa fa-check"></i><b>2.1</b> BRFSS and SMART</a><ul>
<li class="chapter" data-level="2.1.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#key-resources"><i class="fa fa-check"></i><b>2.1.1</b> Key resources</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-smartcle1-data-cookbook"><i class="fa fa-check"></i><b>2.2</b> The <code>smartcle1</code> data: Cookbook</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#smartcle2-omitting-missing-observations-complete-case-analyses"><i class="fa fa-check"></i><b>2.3</b> <code>smartcle2</code>: Omitting Missing Observations: Complete-Case Analyses</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#summarizing-the-smartcle2-data-numerically"><i class="fa fa-check"></i><b>2.4</b> Summarizing the <code>smartcle2</code> data numerically</a><ul>
<li class="chapter" data-level="2.4.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-new-toy-the-skim-function"><i class="fa fa-check"></i><b>2.4.1</b> The New Toy: The <code>skim</code> function</a></li>
<li class="chapter" data-level="2.4.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-usual-summary-for-a-data-frame"><i class="fa fa-check"></i><b>2.4.2</b> The usual <code>summary</code> for a data frame</a></li>
<li class="chapter" data-level="2.4.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-describe-function-in-hmisc"><i class="fa fa-check"></i><b>2.4.3</b> The <code>describe</code> function in <code>Hmisc</code></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#counting-as-exploratory-data-analysis"><i class="fa fa-check"></i><b>2.5</b> Counting as exploratory data analysis</a><ul>
<li class="chapter" data-level="2.5.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-many-respondents-had-exercised-in-the-past-30-days-did-this-vary-by-sex"><i class="fa fa-check"></i><b>2.5.1</b> How many respondents had exercised in the past 30 days? Did this vary by sex?</a></li>
<li class="chapter" data-level="2.5.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#whats-the-distribution-of-sleephrs"><i class="fa fa-check"></i><b>2.5.2</b> What’s the distribution of <code>sleephrs</code>?</a></li>
<li class="chapter" data-level="2.5.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#whats-the-distribution-of-bmi"><i class="fa fa-check"></i><b>2.5.3</b> What’s the distribution of <code>BMI</code>?</a></li>
<li class="chapter" data-level="2.5.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-many-of-the-respondents-have-a-bmi-below-30"><i class="fa fa-check"></i><b>2.5.4</b> How many of the respondents have a BMI below 30?</a></li>
<li class="chapter" data-level="2.5.5" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-many-of-the-respondents-who-have-a-bmi-30-exercised"><i class="fa fa-check"></i><b>2.5.5</b> How many of the respondents who have a BMI &lt; 30 exercised?</a></li>
<li class="chapter" data-level="2.5.6" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#is-obesity-associated-with-sex-in-these-data"><i class="fa fa-check"></i><b>2.5.6</b> Is obesity associated with sex, in these data?</a></li>
<li class="chapter" data-level="2.5.7" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#comparing-sleephrs-summaries-by-obesity-status"><i class="fa fa-check"></i><b>2.5.7</b> Comparing <code>sleephrs</code> summaries by obesity status</a></li>
<li class="chapter" data-level="2.5.8" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-skim-function-within-a-pipe"><i class="fa fa-check"></i><b>2.5.8</b> The <code>skim</code> function within a pipe</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#first-modeling-attempt-can-bmi-predict-physhealth"><i class="fa fa-check"></i><b>2.6</b> First Modeling Attempt: Can <code>bmi</code> predict <code>physhealth</code>?</a><ul>
<li class="chapter" data-level="2.6.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#fitting-a-simple-regression-model"><i class="fa fa-check"></i><b>2.6.1</b> Fitting a Simple Regression Model</a></li>
<li class="chapter" data-level="2.6.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#model-summary-for-a-simple-one-predictor-regression"><i class="fa fa-check"></i><b>2.6.2</b> Model Summary for a Simple (One-Predictor) Regression</a></li>
<li class="chapter" data-level="2.6.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#using-the-broom-package"><i class="fa fa-check"></i><b>2.6.3</b> Using the <code>broom</code> package</a></li>
<li class="chapter" data-level="2.6.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-does-the-model-do-residuals-vs.fitted-values"><i class="fa fa-check"></i><b>2.6.4</b> How does the model do? (Residuals vs. Fitted Values)</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#a-new-small-study-predicting-bmi"><i class="fa fa-check"></i><b>2.7</b> A New Small Study: Predicting BMI</a><ul>
<li class="chapter" data-level="2.7.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#does-female-predict-bmi-well"><i class="fa fa-check"></i><b>2.7.1</b> Does <code>female</code> predict <code>bmi</code> well?</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m1-a-simple-t-test-model"><i class="fa fa-check"></i><b>2.8</b> <code>c2_m1</code>: A simple t-test model</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m2-adding-another-predictor-two-way-anova-without-interaction"><i class="fa fa-check"></i><b>2.9</b> <code>c2_m2</code>: Adding another predictor (two-way ANOVA without interaction)</a></li>
<li class="chapter" data-level="2.10" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m3-adding-the-interaction-term-two-way-anova-with-interaction"><i class="fa fa-check"></i><b>2.10</b> <code>c2_m3</code>: Adding the interaction term (Two-way ANOVA with interaction)</a></li>
<li class="chapter" data-level="2.11" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m4-using-female-and-sleephrs-in-a-model-for-bmi"><i class="fa fa-check"></i><b>2.11</b> <code>c2_m4</code>: Using <code>female</code> and <code>sleephrs</code> in a model for <code>bmi</code></a></li>
<li class="chapter" data-level="2.12" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#making-predictions-with-a-linear-regression-model"><i class="fa fa-check"></i><b>2.12</b> Making Predictions with a Linear Regression Model</a><ul>
<li class="chapter" data-level="2.12.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#fitting-an-individual-prediction-and-95-prediction-interval"><i class="fa fa-check"></i><b>2.12.1</b> Fitting an Individual Prediction and 95% Prediction Interval</a></li>
<li class="chapter" data-level="2.12.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#confidence-interval-for-an-average-prediction"><i class="fa fa-check"></i><b>2.12.2</b> Confidence Interval for an Average Prediction</a></li>
<li class="chapter" data-level="2.12.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#fitting-multiple-individual-predictions-to-new-data"><i class="fa fa-check"></i><b>2.12.3</b> Fitting Multiple Individual Predictions to New Data</a></li>
<li class="chapter" data-level="2.12.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#simulation-to-represent-predictive-uncertainty-in-model-4"><i class="fa fa-check"></i><b>2.12.4</b> Simulation to represent predictive uncertainty in Model 4</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#centering-the-model"><i class="fa fa-check"></i><b>2.13</b> Centering the model</a><ul>
<li class="chapter" data-level="2.13.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#plot-of-model-4-on-centered-sleephrs-c2_m4_c"><i class="fa fa-check"></i><b>2.13.1</b> Plot of Model 4 on Centered <code>sleephrs</code>: <code>c2_m4_c</code></a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#rescaling-an-input-by-subtracting-the-mean-and-dividing-by-2-standard-deviations"><i class="fa fa-check"></i><b>2.14</b> Rescaling an input by subtracting the mean and dividing by 2 standard deviations</a><ul>
<li class="chapter" data-level="2.14.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#refitting-model-c2_m4-to-the-rescaled-data"><i class="fa fa-check"></i><b>2.14.1</b> Refitting model <code>c2_m4</code> to the rescaled data</a></li>
<li class="chapter" data-level="2.14.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#interpreting-the-model-on-rescaled-data"><i class="fa fa-check"></i><b>2.14.2</b> Interpreting the model on rescaled data</a></li>
<li class="chapter" data-level="2.14.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#plot-of-model-on-rescaled-data"><i class="fa fa-check"></i><b>2.14.3</b> Plot of model on rescaled data</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m5-what-if-we-add-more-variables"><i class="fa fa-check"></i><b>2.15</b> <code>c2_m5</code>: What if we add more variables?</a></li>
<li class="chapter" data-level="2.16" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m6-would-adding-self-reported-health-help"><i class="fa fa-check"></i><b>2.16</b> <code>c2_m6</code>: Would adding self-reported health help?</a></li>
<li class="chapter" data-level="2.17" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m7-what-if-we-added-the-menthealth-variable"><i class="fa fa-check"></i><b>2.17</b> <code>c2_m7</code>: What if we added the <code>menthealth</code> variable?</a></li>
<li class="chapter" data-level="2.18" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#key-regression-assumptions-for-building-effective-prediction-models"><i class="fa fa-check"></i><b>2.18</b> Key Regression Assumptions for Building Effective Prediction Models</a><ul>
<li class="chapter" data-level="2.18.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#checking-assumptions-in-model-c2_m7"><i class="fa fa-check"></i><b>2.18.1</b> Checking Assumptions in model <code>c2_m7</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html"><i class="fa fa-check"></i><b>3</b> Analysis of Variance</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-bonding-data-a-designed-dental-experiment"><i class="fa fa-check"></i><b>3.1</b> The <code>bonding</code> data: A Designed Dental Experiment</a></li>
<li class="chapter" data-level="3.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-one-factor-analysis-of-variance"><i class="fa fa-check"></i><b>3.2</b> A One-Factor Analysis of Variance</a><ul>
<li class="chapter" data-level="3.2.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#look-at-the-data"><i class="fa fa-check"></i><b>3.2.1</b> Look at the Data!</a></li>
<li class="chapter" data-level="3.2.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#table-of-summary-statistics"><i class="fa fa-check"></i><b>3.2.2</b> Table of Summary Statistics</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-two-way-anova-looking-at-two-factors"><i class="fa fa-check"></i><b>3.3</b> A Two-Way ANOVA: Looking at Two Factors</a></li>
<li class="chapter" data-level="3.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-means-plot-with-standard-deviations-to-check-for-interaction"><i class="fa fa-check"></i><b>3.4</b> A Means Plot (with standard deviations) to check for interaction</a><ul>
<li class="chapter" data-level="3.4.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#skimming-the-data-after-grouping-by-resin-and-light"><i class="fa fa-check"></i><b>3.4.1</b> Skimming the data after grouping by <code>resin</code> and <code>light</code></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#fitting-the-two-way-anova-model-with-interaction"><i class="fa fa-check"></i><b>3.5</b> Fitting the Two-Way ANOVA model with Interaction</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-anova-table-for-our-model"><i class="fa fa-check"></i><b>3.5.1</b> The ANOVA table for our model</a></li>
<li class="chapter" data-level="3.5.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#is-the-interaction-important"><i class="fa fa-check"></i><b>3.5.2</b> Is the interaction important?</a></li>
<li class="chapter" data-level="3.5.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#interpreting-the-interaction"><i class="fa fa-check"></i><b>3.5.3</b> Interpreting the Interaction</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#comparing-individual-combinations-of-resin-and-light"><i class="fa fa-check"></i><b>3.6</b> Comparing Individual Combinations of <code>resin</code> and <code>light</code></a></li>
<li class="chapter" data-level="3.7" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-bonding-model-without-interaction"><i class="fa fa-check"></i><b>3.7</b> The <code>bonding</code> model without Interaction</a></li>
<li class="chapter" data-level="3.8" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#cortisol-a-hypothetical-clinical-trial"><i class="fa fa-check"></i><b>3.8</b> <code>cortisol</code>: A Hypothetical Clinical Trial</a><ul>
<li class="chapter" data-level="3.8.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#codebook-and-raw-data-for-cortisol"><i class="fa fa-check"></i><b>3.8.1</b> Codebook and Raw Data for <code>cortisol</code></a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#creating-a-factor-combining-sex-and-waist"><i class="fa fa-check"></i><b>3.9</b> Creating a factor combining sex and waist</a></li>
<li class="chapter" data-level="3.10" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-means-plot-for-the-cortisol-trial-with-standard-errors"><i class="fa fa-check"></i><b>3.10</b> A Means Plot for the <code>cortisol</code> trial (with standard errors)</a></li>
<li class="chapter" data-level="3.11" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-two-way-anova-model-for-cortisol-with-interaction"><i class="fa fa-check"></i><b>3.11</b> A Two-Way ANOVA model for <code>cortisol</code> with Interaction</a></li>
<li class="chapter" data-level="3.12" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-two-way-anova-model-for-cortisol-without-interaction"><i class="fa fa-check"></i><b>3.12</b> A Two-Way ANOVA model for <code>cortisol</code> without Interaction</a><ul>
<li class="chapter" data-level="3.12.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-graph"><i class="fa fa-check"></i><b>3.12.1</b> The Graph</a></li>
<li class="chapter" data-level="3.12.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-anova-model"><i class="fa fa-check"></i><b>3.12.2</b> The ANOVA Model</a></li>
<li class="chapter" data-level="3.12.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-regression-summary"><i class="fa fa-check"></i><b>3.12.3</b> The Regression Summary</a></li>
<li class="chapter" data-level="3.12.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#tukey-hsd-comparisons"><i class="fa fa-check"></i><b>3.12.4</b> Tukey HSD Comparisons</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html"><i class="fa fa-check"></i><b>4</b> Analysis of Covariance</a><ul>
<li class="chapter" data-level="4.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#an-emphysema-study"><i class="fa fa-check"></i><b>4.1</b> An Emphysema Study</a><ul>
<li class="chapter" data-level="4.1.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#codebook"><i class="fa fa-check"></i><b>4.1.1</b> Codebook</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#does-sex-affect-the-mean-change-in-theophylline"><i class="fa fa-check"></i><b>4.2</b> Does <code>sex</code> affect the mean change in theophylline?</a></li>
<li class="chapter" data-level="4.3" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#is-there-an-association-between-age-and-sex-in-this-study"><i class="fa fa-check"></i><b>4.3</b> Is there an association between <code>age</code> and <code>sex</code> in this study?</a></li>
<li class="chapter" data-level="4.4" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#adding-a-quantitative-covariate-age-to-the-model"><i class="fa fa-check"></i><b>4.4</b> Adding a quantitative covariate, <code>age</code>, to the model</a><ul>
<li class="chapter" data-level="4.4.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#the-ancova-model"><i class="fa fa-check"></i><b>4.4.1</b> The ANCOVA model</a></li>
<li class="chapter" data-level="4.4.2" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#the-ancova-table"><i class="fa fa-check"></i><b>4.4.2</b> The ANCOVA Table</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#rerunning-the-ancova-model-after-simple-imputation"><i class="fa fa-check"></i><b>4.5</b> Rerunning the ANCOVA model after simple imputation</a></li>
<li class="chapter" data-level="4.6" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#looking-at-a-factor-covariate-interaction"><i class="fa fa-check"></i><b>4.6</b> Looking at a factor-covariate interaction</a></li>
<li class="chapter" data-level="4.7" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#centering-the-covariate-to-facilitate-ancova-interpretation"><i class="fa fa-check"></i><b>4.7</b> Centering the Covariate to Facilitate ANCOVA Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html"><i class="fa fa-check"></i><b>5</b> Missing Data Mechanisms and Single Imputation</a><ul>
<li class="chapter" data-level="5.1" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#a-toy-example"><i class="fa fa-check"></i><b>5.1</b> A Toy Example</a><ul>
<li class="chapter" data-level="5.1.1" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#how-many-missing-values-do-we-have-in-each-column"><i class="fa fa-check"></i><b>5.1.1</b> How many missing values do we have in each column?</a></li>
<li class="chapter" data-level="5.1.2" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#what-is-the-pattern-of-missing-data"><i class="fa fa-check"></i><b>5.1.2</b> What is the pattern of missing data?</a></li>
<li class="chapter" data-level="5.1.3" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#how-can-we-identify-the-subjects-with-missing-data"><i class="fa fa-check"></i><b>5.1.3</b> How can we identify the subjects with missing data?</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#missing-data-mechanisms"><i class="fa fa-check"></i><b>5.2</b> Missing-data mechanisms</a></li>
<li class="chapter" data-level="5.3" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#options-for-dealing-with-missingness"><i class="fa fa-check"></i><b>5.3</b> Options for Dealing with Missingness</a></li>
<li class="chapter" data-level="5.4" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#complete-case-and-available-case-analyses"><i class="fa fa-check"></i><b>5.4</b> Complete Case (and Available Case) analyses</a></li>
<li class="chapter" data-level="5.5" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#single-imputation"><i class="fa fa-check"></i><b>5.5</b> Single Imputation</a></li>
<li class="chapter" data-level="5.6" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#multiple-imputation"><i class="fa fa-check"></i><b>5.6</b> Multiple Imputation</a></li>
<li class="chapter" data-level="5.7" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#building-a-complete-case-analysis"><i class="fa fa-check"></i><b>5.7</b> Building a Complete Case Analysis</a></li>
<li class="chapter" data-level="5.8" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#single-imputation-with-the-mean-or-mode"><i class="fa fa-check"></i><b>5.8</b> Single Imputation with the Mean or Mode</a></li>
<li class="chapter" data-level="5.9" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#doing-single-imputation-with-simputation"><i class="fa fa-check"></i><b>5.9</b> Doing Single Imputation with <code>simputation</code></a><ul>
<li class="chapter" data-level="5.9.1" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#mirroring-our-prior-approach-imputing-meansmediansmodes"><i class="fa fa-check"></i><b>5.9.1</b> Mirroring Our Prior Approach (imputing means/medians/modes)</a></li>
<li class="chapter" data-level="5.9.2" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#using-a-model-to-impute-sbp.before-and-diabetes"><i class="fa fa-check"></i><b>5.9.2</b> Using a model to impute <code>sbp.before</code> and <code>diabetes</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html"><i class="fa fa-check"></i><b>6</b> A Study of Prostate Cancer</a><ul>
<li class="chapter" data-level="6.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#data-load-and-background"><i class="fa fa-check"></i><b>6.1</b> Data Load and Background</a></li>
<li class="chapter" data-level="6.2" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#code-book"><i class="fa fa-check"></i><b>6.2</b> Code Book</a></li>
<li class="chapter" data-level="6.3" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#additions-for-later-use"><i class="fa fa-check"></i><b>6.3</b> Additions for Later Use</a></li>
<li class="chapter" data-level="6.4" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#fitting-and-evaluating-a-two-predictor-model"><i class="fa fa-check"></i><b>6.4</b> Fitting and Evaluating a Two-Predictor Model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#using-tidy"><i class="fa fa-check"></i><b>6.4.1</b> Using <code>tidy</code></a></li>
<li class="chapter" data-level="6.4.2" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#interpretation"><i class="fa fa-check"></i><b>6.4.2</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#exploring-model-c5_prost_a"><i class="fa fa-check"></i><b>6.5</b> Exploring Model <code>c5_prost_A</code></a><ul>
<li class="chapter" data-level="6.5.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#summary-for-model-c5_prost_a"><i class="fa fa-check"></i><b>6.5.1</b> <code>summary</code> for Model <code>c5_prost_A</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#adjusted-r2"><i class="fa fa-check"></i><b>6.5.2</b> Adjusted R<sup>2</sup></a></li>
<li class="chapter" data-level="6.5.3" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#coefficient-confidence-intervals"><i class="fa fa-check"></i><b>6.5.3</b> Coefficient Confidence Intervals</a></li>
<li class="chapter" data-level="6.5.4" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#anova-for-model-c5_prost_a"><i class="fa fa-check"></i><b>6.5.4</b> ANOVA for Model <code>c5_prost_A</code></a></li>
<li class="chapter" data-level="6.5.5" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#residuals-fitted-values-and-standard-errors-with-augment"><i class="fa fa-check"></i><b>6.5.5</b> Residuals, Fitted Values and Standard Errors with <code>augment</code></a></li>
<li class="chapter" data-level="6.5.6" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#making-predictions-with-c5_prost_a"><i class="fa fa-check"></i><b>6.5.6</b> Making Predictions with <code>c5_prost_A</code></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#plotting-model-c5_prost_a"><i class="fa fa-check"></i><b>6.6</b> Plotting Model <code>c5_prost_A</code></a><ul>
<li class="chapter" data-level="6.6.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#residual-plots-of-c5_prost_a"><i class="fa fa-check"></i><b>6.6.1</b> Residual Plots of <code>c5_prost_A</code></a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#cross-validation-of-model-c5_prost_a"><i class="fa fa-check"></i><b>6.7</b> Cross-Validation of Model <code>c5_prost_A</code></a><ul>
<li class="chapter" data-level="6.7.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#cross-validated-summaries-of-prediction-quality"><i class="fa fa-check"></i><b>6.7.1</b> Cross-Validated Summaries of Prediction Quality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html"><i class="fa fa-check"></i><b>7</b> Stepwise Variable Selection</a><ul>
<li class="chapter" data-level="7.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#strategy-for-model-selection"><i class="fa fa-check"></i><b>7.1</b> Strategy for Model Selection</a><ul>
<li class="chapter" data-level="7.1.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#how-do-we-choose-potential-subsets-of-predictors"><i class="fa fa-check"></i><b>7.1.1</b> How Do We Choose Potential Subsets of Predictors?</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#a-kitchen-sink-model-model-c5_prost_ks"><i class="fa fa-check"></i><b>7.2</b> A “Kitchen Sink” Model (Model <code>c5_prost_ks</code>)</a></li>
<li class="chapter" data-level="7.3" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#sequential-variable-selection-stepwise-approaches"><i class="fa fa-check"></i><b>7.3</b> Sequential Variable Selection: Stepwise Approaches</a><ul>
<li class="chapter" data-level="7.3.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#the-big-problems-with-stepwise-regression"><i class="fa fa-check"></i><b>7.3.1</b> The Big Problems with Stepwise Regression</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#forward-selection-with-the-step-function"><i class="fa fa-check"></i><b>7.4</b> Forward Selection with the <code>step</code> function</a></li>
<li class="chapter" data-level="7.5" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#backward-elimination-using-the-step-function"><i class="fa fa-check"></i><b>7.5</b> Backward Elimination using the <code>step</code> function</a></li>
<li class="chapter" data-level="7.6" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#allen-cady-modified-backward-elimination"><i class="fa fa-check"></i><b>7.6</b> Allen-Cady Modified Backward Elimination</a><ul>
<li class="chapter" data-level="7.6.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#demonstration-of-the-allen-cady-approach"><i class="fa fa-check"></i><b>7.6.1</b> Demonstration of the Allen-Cady approach</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#summarizing-the-results"><i class="fa fa-check"></i><b>7.7</b> Summarizing the Results</a><ul>
<li class="chapter" data-level="7.7.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#in-sample-testing-and-summaries"><i class="fa fa-check"></i><b>7.7.1</b> In-Sample Testing and Summaries</a></li>
<li class="chapter" data-level="7.7.2" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#validating-the-results-of-the-various-models"><i class="fa fa-check"></i><b>7.7.2</b> Validating the Results of the Various Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><i class="fa fa-check"></i><b>8</b> “Best Subsets” Variable Selection in our Prostate Cancer Study</a><ul>
<li class="chapter" data-level="8.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#four-key-summaries-well-use-to-evaluate-potential-models"><i class="fa fa-check"></i><b>8.1</b> Four Key Summaries We’ll Use to Evaluate Potential Models</a></li>
<li class="chapter" data-level="8.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#using-regsubsets-in-the-leaps-package"><i class="fa fa-check"></i><b>8.2</b> Using <code>regsubsets</code> in the <code>leaps</code> package</a><ul>
<li class="chapter" data-level="8.2.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#identifying-the-models-with-which-and-outmat"><i class="fa fa-check"></i><b>8.2.1</b> Identifying the models with <code>which</code> and <code>outmat</code></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#calculating-bias-corrected-aic"><i class="fa fa-check"></i><b>8.3</b> Calculating bias-corrected AIC</a><ul>
<li class="chapter" data-level="8.3.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#calculation-of-aic.c-in-our-setting"><i class="fa fa-check"></i><b>8.3.1</b> Calculation of aic.c in our setting</a></li>
<li class="chapter" data-level="8.3.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-uncorrected-aic-provides-no-more-useful-information-here"><i class="fa fa-check"></i><b>8.3.2</b> The Uncorrected AIC provides no more useful information here</a></li>
<li class="chapter" data-level="8.3.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#building-a-tibble-containing-the-necessary-information"><i class="fa fa-check"></i><b>8.3.3</b> Building a Tibble containing the necessary information</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#plotting-the-best-subsets-results-using-ggplot2"><i class="fa fa-check"></i><b>8.4</b> Plotting the Best Subsets Results using <code>ggplot2</code></a><ul>
<li class="chapter" data-level="8.4.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-adjusted-r2-plot"><i class="fa fa-check"></i><b>8.4.1</b> The Adjusted R<sup>2</sup> Plot</a></li>
<li class="chapter" data-level="8.4.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#mallows-c_p"><i class="fa fa-check"></i><b>8.4.2</b> Mallows’ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="8.4.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-c_p-plot"><i class="fa fa-check"></i><b>8.4.3</b> The <span class="math inline">\(C_p\)</span> Plot</a></li>
<li class="chapter" data-level="8.4.4" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#all-subsets-regression-and-information-criteria"><i class="fa fa-check"></i><b>8.4.4</b> “All Subsets” Regression and Information Criteria</a></li>
<li class="chapter" data-level="8.4.5" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-bias-corrected-aic-plot"><i class="fa fa-check"></i><b>8.4.5</b> The bias-corrected AIC plot</a></li>
<li class="chapter" data-level="8.4.6" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-bic-plot"><i class="fa fa-check"></i><b>8.4.6</b> The BIC plot</a></li>
<li class="chapter" data-level="8.4.7" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#all-four-plots-in-one-figure-via-ggplot2"><i class="fa fa-check"></i><b>8.4.7</b> All Four Plots in One Figure (via ggplot2)</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#table-of-key-results"><i class="fa fa-check"></i><b>8.5</b> Table of Key Results</a></li>
<li class="chapter" data-level="8.6" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#models-worth-considering"><i class="fa fa-check"></i><b>8.6</b> Models Worth Considering?</a></li>
<li class="chapter" data-level="8.7" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#compare-these-candidate-models-in-sample"><i class="fa fa-check"></i><b>8.7</b> Compare these candidate models in-sample?</a><ul>
<li class="chapter" data-level="8.7.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#using-anova-to-compare-nested-models"><i class="fa fa-check"></i><b>8.7.1</b> Using <code>anova</code> to compare nested models</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#aic-and-bic-comparisons-within-the-training-sample"><i class="fa fa-check"></i><b>8.8</b> AIC and BIC comparisons, within the training sample</a></li>
<li class="chapter" data-level="8.9" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#cross-validation-of-candidate-models-out-of-sample"><i class="fa fa-check"></i><b>8.9</b> Cross-Validation of Candidate Models out of Sample</a><ul>
<li class="chapter" data-level="8.9.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#fold-cross-validation-of-model-m04"><i class="fa fa-check"></i><b>8.9.1</b> 20-fold Cross-Validation of model <code>m04</code></a></li>
<li class="chapter" data-level="8.9.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#fold-cross-validation-of-model-m07"><i class="fa fa-check"></i><b>8.9.2</b> 20-fold Cross-Validation of model <code>m07</code></a></li>
<li class="chapter" data-level="8.9.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#fold-cross-validation-of-model-m08"><i class="fa fa-check"></i><b>8.9.3</b> 20-fold Cross-Validation of model <code>m08</code></a></li>
<li class="chapter" data-level="8.9.4" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#comparing-the-results-of-the-cross-validations"><i class="fa fa-check"></i><b>8.9.4</b> Comparing the Results of the Cross-Validations</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#what-about-interaction-terms"><i class="fa fa-check"></i><b>8.10</b> What about Interaction Terms?</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html"><i class="fa fa-check"></i><b>9</b> Adding Non-linear Terms to a Linear Regression Model</a><ul>
<li class="chapter" data-level="9.1" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#the-pollution-data"><i class="fa fa-check"></i><b>9.1</b> The <code>pollution</code> data</a></li>
<li class="chapter" data-level="9.2" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#fitting-a-straight-line-model-to-predict-y-from-x2"><i class="fa fa-check"></i><b>9.2</b> Fitting a straight line model to predict <code>y</code> from <code>x2</code></a></li>
<li class="chapter" data-level="9.3" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#quadratic-polynomial-model-to-predict-y-using-x2"><i class="fa fa-check"></i><b>9.3</b> Quadratic polynomial model to predict <code>y</code> using <code>x2</code></a><ul>
<li class="chapter" data-level="9.3.1" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#the-raw-quadratic-model"><i class="fa fa-check"></i><b>9.3.1</b> The raw quadratic model</a></li>
<li class="chapter" data-level="9.3.2" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#raw-quadratic-fit-after-centering-x2"><i class="fa fa-check"></i><b>9.3.2</b> Raw quadratic fit after centering <code>x2</code></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#orthogonal-polynomials"><i class="fa fa-check"></i><b>9.4</b> Orthogonal Polynomials</a></li>
<li class="chapter" data-level="9.5" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#fit-a-cubic-polynomial-to-predict-y-from-x3"><i class="fa fa-check"></i><b>9.5</b> Fit a cubic polynomial to predict <code>y</code> from <code>x3</code></a></li>
<li class="chapter" data-level="9.6" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#fitting-a-restricted-cubic-spline-in-a-linear-regression"><i class="fa fa-check"></i><b>9.6</b> Fitting a restricted cubic spline in a linear regression</a></li>
<li class="chapter" data-level="9.7" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#spending-degrees-of-freedom"><i class="fa fa-check"></i><b>9.7</b> “Spending” Degrees of Freedom</a><ul>
<li class="chapter" data-level="9.7.1" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#overfitting-and-limits-on-the-of-predictors"><i class="fa fa-check"></i><b>9.7.1</b> Overfitting and Limits on the # of Predictors</a></li>
<li class="chapter" data-level="9.7.2" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#the-importance-of-collinearity"><i class="fa fa-check"></i><b>9.7.2</b> The Importance of Collinearity</a></li>
<li class="chapter" data-level="9.7.3" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#collinearity-in-an-explanatory-model"><i class="fa fa-check"></i><b>9.7.3</b> Collinearity in an Explanatory Model</a></li>
<li class="chapter" data-level="9.7.4" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#collinearity-in-a-prediction-model"><i class="fa fa-check"></i><b>9.7.4</b> Collinearity in a Prediction Model</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#spending-df-on-non-linearity-the-spearman-rho2-plot"><i class="fa fa-check"></i><b>9.8</b> Spending DF on Non-Linearity: The Spearman <span class="math inline">\(\rho^2\)</span> Plot</a><ul>
<li class="chapter" data-level="9.8.1" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#fitting-a-big-model-to-the-pollution-data"><i class="fa fa-check"></i><b>9.8.1</b> Fitting a Big Model to the <code>pollution</code> data</a></li>
<li class="chapter" data-level="9.8.2" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#limitations-of-lm-for-fitting-complex-linear-regression-models"><i class="fa fa-check"></i><b>9.8.2</b> Limitations of <code>lm</code> for fitting complex linear regression models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html"><i class="fa fa-check"></i><b>10</b> Using <code>ols</code> from the <code>rms</code> package to fit linear models</a><ul>
<li class="chapter" data-level="10.1" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#fitting-a-model-with-ols"><i class="fa fa-check"></i><b>10.1</b> Fitting a model with <code>ols</code></a><ul>
<li class="chapter" data-level="10.1.1" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#the-model-likelihood-ratio-test"><i class="fa fa-check"></i><b>10.1.1</b> The Model Likelihood Ratio Test</a></li>
<li class="chapter" data-level="10.1.2" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#the-g-statistic"><i class="fa fa-check"></i><b>10.1.2</b> The g statistic</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#anova-for-an-ols-model"><i class="fa fa-check"></i><b>10.2</b> ANOVA for an <code>ols</code> model</a></li>
<li class="chapter" data-level="10.3" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#effect-estimates"><i class="fa fa-check"></i><b>10.3</b> Effect Estimates</a><ul>
<li class="chapter" data-level="10.3.1" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#simultaneous-confidence-intervals"><i class="fa fa-check"></i><b>10.3.1</b> Simultaneous Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#the-predict-function-for-an-ols-model"><i class="fa fa-check"></i><b>10.4</b> The <code>Predict</code> function for an <code>ols</code> model</a></li>
<li class="chapter" data-level="10.5" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#checking-influence-via-dfbeta"><i class="fa fa-check"></i><b>10.5</b> Checking Influence via <code>dfbeta</code></a><ul>
<li class="chapter" data-level="10.5.1" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#using-the-residuals-command-for-dfbetas"><i class="fa fa-check"></i><b>10.5.1</b> Using the <code>residuals</code> command for <code>dfbetas</code></a></li>
<li class="chapter" data-level="10.5.2" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#using-the-residuals-command-for-other-summaries"><i class="fa fa-check"></i><b>10.5.2</b> Using the <code>residuals</code> command for other summaries</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#model-validation-and-correcting-for-optimism"><i class="fa fa-check"></i><b>10.6</b> Model Validation and Correcting for Optimism</a></li>
<li class="chapter" data-level="10.7" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#building-a-nomogram-for-our-model"><i class="fa fa-check"></i><b>10.7</b> Building a Nomogram for Our Model</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html"><i class="fa fa-check"></i><b>11</b> Other Variable Selection Strategies</a><ul>
<li class="chapter" data-level="11.1" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#why-not-use-stepwise-procedures"><i class="fa fa-check"></i><b>11.1</b> Why not use stepwise procedures?</a></li>
<li class="chapter" data-level="11.2" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#ridge-regression"><i class="fa fa-check"></i><b>11.2</b> Ridge Regression</a><ul>
<li class="chapter" data-level="11.2.1" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#assessing-a-ridge-regression-approach"><i class="fa fa-check"></i><b>11.2.1</b> Assessing a Ridge Regression Approach</a></li>
<li class="chapter" data-level="11.2.2" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#the-lm.ridge-plot---where-do-coefficients-stabilize"><i class="fa fa-check"></i><b>11.2.2</b> The <code>lm.ridge</code> plot - where do coefficients stabilize?</a></li>
<li class="chapter" data-level="11.2.3" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#ridge-regression-the-bottom-line"><i class="fa fa-check"></i><b>11.2.3</b> Ridge Regression: The Bottom Line</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#the-lasso"><i class="fa fa-check"></i><b>11.3</b> The Lasso</a><ul>
<li class="chapter" data-level="11.3.1" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#consequences-of-the-lasso-approach"><i class="fa fa-check"></i><b>11.3.1</b> Consequences of the Lasso Approach</a></li>
<li class="chapter" data-level="11.3.2" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#how-the-lasso-works"><i class="fa fa-check"></i><b>11.3.2</b> How The Lasso Works</a></li>
<li class="chapter" data-level="11.3.3" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#cross-validation-with-the-lasso"><i class="fa fa-check"></i><b>11.3.3</b> Cross-Validation with the Lasso</a></li>
<li class="chapter" data-level="11.3.4" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#what-value-of-the-key-fraction-minimizes-cross-validated-mse"><i class="fa fa-check"></i><b>11.3.4</b> What value of the key fraction minimizes cross-validated MSE?</a></li>
<li class="chapter" data-level="11.3.5" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#coefficients-for-the-model-identified-by-the-cross-validation"><i class="fa fa-check"></i><b>11.3.5</b> Coefficients for the Model Identified by the Cross-Validation</a></li>
<li class="chapter" data-level="11.3.6" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#obtaining-fitted-values-from-lasso"><i class="fa fa-check"></i><b>11.3.6</b> Obtaining Fitted Values from Lasso</a></li>
<li class="chapter" data-level="11.3.7" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#complete-set-of-fitted-values-from-the-lasso"><i class="fa fa-check"></i><b>11.3.7</b> Complete Set of Fitted Values from the Lasso</a></li>
<li class="chapter" data-level="11.3.8" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#when-is-the-lasso-most-useful"><i class="fa fa-check"></i><b>11.3.8</b> When is the Lasso Most Useful?</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#applying-the-lasso-to-the-pollution-data"><i class="fa fa-check"></i><b>11.4</b> Applying the Lasso to the <code>pollution</code> data</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html"><i class="fa fa-check"></i><b>12</b> Logistic Regression and the <code>resect</code> data</a><ul>
<li class="chapter" data-level="12.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#the-resect-data"><i class="fa fa-check"></i><b>12.1</b> The <code>resect</code> data</a></li>
<li class="chapter" data-level="12.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#running-a-simple-logistic-regression-model"><i class="fa fa-check"></i><b>12.2</b> Running A Simple Logistic Regression Model</a><ul>
<li class="chapter" data-level="12.2.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#logistic-regression-can-be-harder-than-linear-regression"><i class="fa fa-check"></i><b>12.2.1</b> Logistic Regression Can Be Harder than Linear Regression</a></li>
<li class="chapter" data-level="12.2.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#obtaining-the-fitted-equation"><i class="fa fa-check"></i><b>12.2.2</b> Obtaining the fitted equation</a></li>
<li class="chapter" data-level="12.2.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-coefficients-of-a-logistic-regression-model"><i class="fa fa-check"></i><b>12.2.3</b> Interpreting the Coefficients of a Logistic Regression Model</a></li>
<li class="chapter" data-level="12.2.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#using-predict-to-describe-the-models-fits"><i class="fa fa-check"></i><b>12.2.4</b> Using <code>predict</code> to describe the model’s fits</a></li>
<li class="chapter" data-level="12.2.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#odds-ratio-interpretation-of-coefficients"><i class="fa fa-check"></i><b>12.2.5</b> Odds Ratio interpretation of Coefficients</a></li>
<li class="chapter" data-level="12.2.6" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-rest-of-the-model-output-from-glm"><i class="fa fa-check"></i><b>12.2.6</b> Interpreting the rest of the model output from <code>glm</code></a></li>
<li class="chapter" data-level="12.2.7" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#deviance-and-comparing-our-model-to-the-null-model"><i class="fa fa-check"></i><b>12.2.7</b> Deviance and Comparing Our Model to the Null Model</a></li>
<li class="chapter" data-level="12.2.8" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#using-glance-with-a-logistic-regression-model"><i class="fa fa-check"></i><b>12.2.8</b> Using <code>glance</code> with a logistic regression model</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-model-summary"><i class="fa fa-check"></i><b>12.3</b> Interpreting the Model Summary</a><ul>
<li class="chapter" data-level="12.3.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#wald-z-tests-for-coefficients-in-a-logistic-regression"><i class="fa fa-check"></i><b>12.3.1</b> Wald Z tests for Coefficients in a Logistic Regression</a></li>
<li class="chapter" data-level="12.3.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#confidence-intervals-for-the-coefficients"><i class="fa fa-check"></i><b>12.3.2</b> Confidence Intervals for the Coefficients</a></li>
<li class="chapter" data-level="12.3.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#deviance-residuals"><i class="fa fa-check"></i><b>12.3.3</b> Deviance Residuals</a></li>
<li class="chapter" data-level="12.3.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#dispersion-parameter"><i class="fa fa-check"></i><b>12.3.4</b> Dispersion Parameter</a></li>
<li class="chapter" data-level="12.3.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#fisher-scoring-iterations"><i class="fa fa-check"></i><b>12.3.5</b> Fisher Scoring iterations</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-a-simple-logistic-regression-model"><i class="fa fa-check"></i><b>12.4</b> Plotting a Simple Logistic Regression Model</a><ul>
<li class="chapter" data-level="12.4.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#using-augment-to-capture-the-fitted-probabilities"><i class="fa fa-check"></i><b>12.4.1</b> Using <code>augment</code> to capture the fitted probabilities</a></li>
<li class="chapter" data-level="12.4.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-a-logistic-regression-models-fitted-values"><i class="fa fa-check"></i><b>12.4.2</b> Plotting a Logistic Regression Model’s Fitted Values</a></li>
<li class="chapter" data-level="12.4.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-a-simple-logistic-model-using-binomial_smooth"><i class="fa fa-check"></i><b>12.4.3</b> Plotting a Simple Logistic Model using <code>binomial_smooth</code></a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#receiver-operating-characteristic-curve-analysis"><i class="fa fa-check"></i><b>12.5</b> Receiver Operating Characteristic Curve Analysis</a><ul>
<li class="chapter" data-level="12.5.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-area-under-the-roc-curve"><i class="fa fa-check"></i><b>12.5.1</b> Interpreting the Area under the ROC curve</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#the-roc-plot-for-res_moda"><i class="fa fa-check"></i><b>12.6</b> The ROC Plot for <code>res_modA</code></a></li>
<li class="chapter" data-level="12.7" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#assessing-residual-plots-from-model-a"><i class="fa fa-check"></i><b>12.7</b> Assessing Residual Plots from Model A</a></li>
<li class="chapter" data-level="12.8" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#model-b-a-kitchen-sink-logistic-regression-model"><i class="fa fa-check"></i><b>12.8</b> Model B: A “Kitchen Sink” Logistic Regression Model</a><ul>
<li class="chapter" data-level="12.8.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#comparing-model-a-to-model-b"><i class="fa fa-check"></i><b>12.8.1</b> Comparing Model A to Model B</a></li>
<li class="chapter" data-level="12.8.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-model-b"><i class="fa fa-check"></i><b>12.8.2</b> Interpreting Model B</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-model-b"><i class="fa fa-check"></i><b>12.9</b> Plotting Model B</a><ul>
<li class="chapter" data-level="12.9.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#using-augment-to-capture-the-fitted-probabilities-1"><i class="fa fa-check"></i><b>12.9.1</b> Using <code>augment</code> to capture the fitted probabilities</a></li>
<li class="chapter" data-level="12.9.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-model-b-fits-by-observed-mortality"><i class="fa fa-check"></i><b>12.9.2</b> Plotting Model B Fits by Observed Mortality</a></li>
<li class="chapter" data-level="12.9.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#the-roc-curve-for-model-b"><i class="fa fa-check"></i><b>12.9.3</b> The ROC curve for Model B</a></li>
<li class="chapter" data-level="12.9.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#residuals-leverage-and-influence"><i class="fa fa-check"></i><b>12.9.4</b> Residuals, Leverage and Influence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science for Biological, Medical and Health Research: Notes for 432</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression-and-the-resect-data" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Logistic Regression and the <code>resect</code> data</h1>
<div id="the-resect-data" class="section level2">
<h2><span class="header-section-number">12.1</span> The <code>resect</code> data</h2>
<p>My source for these data was <span class="citation">Riffenburgh (<a href="#ref-Riffenburgh2006">2006</a>)</span>. The data describe 134 patients who had undergone resection of the tracheal carina (most often this is done to address tumors in the trachea), and the <code>resect.csv</code> data file contains the following variables:</p>
<ul>
<li><code>id</code> = a patient ID #,</li>
<li><code>age</code>= the patient’s age at surgery,</li>
<li><code>prior</code> = prior tracheal surgery (1 = yes, 0 = no),</li>
<li><code>resection</code> = extent of the resection (in cm),</li>
<li><code>intubated</code> = whether intubation was required at the end of surgery (1 = yes, 0 = no), and</li>
<li><code>died</code> = the patient’s death status (1 = dead, 0 = alive).</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">resect <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(died) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">skim</span>(<span class="op">-</span>id)</code></pre></div>
<pre><code>Skim summary statistics
 n obs: 134 
 n variables: 6 
 group variables: died 

Variable type: integer 
 died  variable missing complete   n   mean    sd p0 p25 median p75 p100
    0       age       0      117 117 48.05  16.01  8  36     51  62   80
    0 intubated       0      117 117  0.068  0.25  0   0      0   0    1
    0     prior       0      117 117  0.24   0.43  0   0      0   0    1
    1       age       0       17  17 46.41  14.46 26  33     46  60   66
    1 intubated       0       17  17  0.65   0.49  0   0      1   1    1
    1     prior       0       17  17  0.35   0.49  0   0      0   1    1

Variable type: numeric 
 died  variable missing complete   n mean   sd p0 p25 median p75 p100
    0 resection       0      117 117 2.82 1.21  1 2      2.5 3.5    6
    1 resection       0       17  17 3.97 1     2 3.5    4   4.5    6</code></pre>
<p>We have no missing data, and 17 of the 134 patients died. Our goal will be to understand the characteristics of the patients, and how they relate to the binary outcome of interest, death.</p>
</div>
<div id="running-a-simple-logistic-regression-model" class="section level2">
<h2><span class="header-section-number">12.2</span> Running A Simple Logistic Regression Model</h2>
<p>In the most common scenario, a logistic regression model is used to predict a binary outcome (which can take on the values 0 or 1.) We will eventually fit a logistic regression model in two ways.</p>
<ol style="list-style-type: decimal">
<li>Through the <code>glm</code> function in the base package of R (similar to <code>lm</code> for linear regression)</li>
<li>Through the <code>lrm</code> function available in the <code>rms</code> package (similar to <code>ols</code> for linear regression)</li>
</ol>
<p>We’ll focus on the <code>glm</code> approach in this Chapter, and save the <code>lrm</code> ideas for later.</p>
<div id="logistic-regression-can-be-harder-than-linear-regression" class="section level3">
<h3><span class="header-section-number">12.2.1</span> Logistic Regression Can Be Harder than Linear Regression</h3>
<ul>
<li>Logistic regression models are fitted using the method of maximum likelihood in <code>glm</code>, which requires multiple iterations until convergence is reached.</li>
<li>Logistic regression models are harder to interpret (for most people) than linear regressions.</li>
<li>Logistic regression models don’t have the same set of assumptions as linear models.</li>
<li>Logistic regression outcomes (yes/no) carry much less information than quantitative outcomes. As a result, fitting a reasonable logistic regression requires more data than a linear model of similar size.
<ul>
<li>The rule I learned in graduate school was that a logistic regression requires 100 observations to fit an intercept plus another 15 observations for each candidate predictor. That’s not terrible, but it’s a very large sample size.</li>
<li>Frank Harrell recommends that 96 observations + a function of the number of candidate predictors (which depends on the amount of variation in the predictors, but 15 x the number of such predictors isn’t too bad if the signal to noise ratio is pretty good) are required just to get reasonable confidence intervals around your predictions.
<ul>
<li>In a <a href="https://twitter.com/f2harrell/status/936230071219707913">twitter note</a>, Frank suggests that 96 + 8 times the number of candidate parameters might be reasonable so long as the smallest cell of interest (combination of an outcome and a split of the covariates) is 96 or more observations.</li>
</ul></li>
<li><span class="citation">Peduzzi et al. (<a href="#ref-Peduzzi1996">1996</a>)</span> suggest that if we let <span class="math inline">\(\pi\)</span> be the smaller of the proportions of “yes” or “no” cases in the population of interest, and <em>k</em> be the number of inputs under consideration, then <span class="math inline">\(N = 10k/\pi\)</span> is the minimum number of cases to include, except that if N &lt; 100 by this standard, you should increase it to 100, according to <span class="citation">Long (<a href="#ref-Long1997">1997</a>)</span>.
<ul>
<li>That suggests that if you have an outcome that happens 10% of the time, and you are running a model with 3 predictors, then you could get away with <span class="math inline">\((10 \times 3)/(0.10) = 300\)</span> observations, but if your outcome happened 40% of the time, you could get away with only <span class="math inline">\((10 \times 3)/(0.40) = 75\)</span> observations, which you’d round up to 100.</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="obtaining-the-fitted-equation" class="section level3">
<h3><span class="header-section-number">12.2.2</span> Obtaining the fitted equation</h3>
<p>We’ll begin by attempting to predict death based on the extent of the resection.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res_modA &lt;-<span class="st"> </span><span class="kw">glm</span>(died <span class="op">~</span><span class="st"> </span>resection, <span class="dt">data=</span>resect, 
               <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))

res_modA</code></pre></div>
<pre><code>
Call:  glm(formula = died ~ resection, family = binomial(link = &quot;logit&quot;), 
    data = resect)

Coefficients:
(Intercept)    resection  
    -4.4337       0.7417  

Degrees of Freedom: 133 Total (i.e. Null);  132 Residual
Null Deviance:      101.9 
Residual Deviance: 89.49    AIC: 93.49</code></pre>
<p>Note that the <code>logit</code> link is the default approach with the <code>binomial</code> family, so we could also have used:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res_modA &lt;-<span class="st"> </span><span class="kw">glm</span>(died <span class="op">~</span><span class="st"> </span>resection, <span class="dt">data =</span> resect, 
                <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</code></pre></div>
<p>which yields the same model.</p>
</div>
<div id="interpreting-the-coefficients-of-a-logistic-regression-model" class="section level3">
<h3><span class="header-section-number">12.2.3</span> Interpreting the Coefficients of a Logistic Regression Model</h3>
<p>Our model is:</p>
<p><span class="math display">\[
logit(died = 1) = log\left(\frac{Pr(died = 1)}{1 - Pr(died = 1)}\right) = \beta_0 + \beta_1 x = -4.4337 + 0.7417 \times resection
\]</span></p>
<p>The predicted log odds of death for a subject with a resection of 4 cm is:</p>
<p><span class="math display">\[
log\left(\frac{Pr(died = 1)}{1 - Pr(died = 1)}\right) = -4.4337 + 0.7417 \times 4 = -1.467
\]</span></p>
<p>The predicted odds of death for a subject with a resection of 4 cm is thus:</p>
<p><span class="math display">\[
\frac{Pr(died = 1)}{1 - Pr(died = 1)} = e^{-4.4337 + 0.7417 \times 4} = e^{-1.467} = 0.2306
\]</span></p>
<p>Since the odds are less than 1, we should find that the probability of death is less than 1/2. With a little algebra, we see that the predicted probability of death for a subject with a resection of 4 cm is:</p>
<p><span class="math display">\[
Pr(died = 1) = \frac{e^{-4.4337 + 0.7417 \times 4}}{1 + e^{-4.4337 + 0.7417 \times 4}} = \frac{e^{-1.467}}{1 + e^{-1.467}} = \frac{0.2306}{1.2306} = 0.187
\]</span></p>
<p>In general, we can frame the model in terms of a statement about probabilities, like this:</p>
<p><span class="math display">\[
Pr(died = 1) = \frac{e^{\beta_0 + \beta_1 x}}{1 + {e^{\beta_0 + \beta_1 x}}} = \frac{e^{-4.4337 + 0.7417 \times resection}}{1 + e^{-4.4337 + 0.7417 \times resection}}
\]</span></p>
<p>and so by subtituting in values for <code>resection</code>, we can estimate the model’s fitted probabilities of death.</p>
</div>
<div id="using-predict-to-describe-the-models-fits" class="section level3">
<h3><span class="header-section-number">12.2.4</span> Using <code>predict</code> to describe the model’s fits</h3>
<p>To obtain these fitted odds and probabilities in R, we can use the <code>predict</code> function.</p>
<ul>
<li>The default predictions are on the scale of the log odds. These predictions are also available through the <code>type = &quot;link&quot;</code> command within the <code>predict</code> function for a generalized linear model like logistic regression.</li>
<li>Here are the predicted log odds of death for a subject (Sally) with a 4 cm resection and a subject (Harry) who had a 5 cm resection.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(res_modA, <span class="dt">newdata =</span> <span class="kw">data_frame</span>(<span class="dt">resection =</span> <span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">5</span>)))</code></pre></div>
<pre><code>         1          2 
-1.4669912 -0.7253027 </code></pre>
<ul>
<li>We can also obtain predictions for each subject on the original response (here, probability) scale, backing out of the logit link.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(res_modA, <span class="dt">newdata =</span> <span class="kw">data_frame</span>(<span class="dt">resection =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">5</span>)), 
        <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre></div>
<pre><code>        1         2 
0.1874004 0.3262264 </code></pre>
<p>So the predicted probability of death is 0.187 for Sally, the subject with a 4 cm resection, and 0.326 for Harry, the subject with a 5 cm resection.</p>
</div>
<div id="odds-ratio-interpretation-of-coefficients" class="section level3">
<h3><span class="header-section-number">12.2.5</span> Odds Ratio interpretation of Coefficients</h3>
<p>Often, we will exponentiate the estimated slope coefficients of a logistic regression model to help us understand the impact of changing a predictor on the odds of our outcome.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">coef</span>(res_modA))</code></pre></div>
<pre><code>(Intercept)   resection 
 0.01186995  2.09947754 </code></pre>
<p>To interpret this finding, suppose we have two subjects, Harry and Sally. Harry had a resection that was 1 cm larger than Sally. This estimated coefficient suggests that the estimated odds for death associated with Harry is 2.099 times larger than the odds for death associated with Sally. In general, the odds ratio comparing two subjects who differ by 1 cm on the resection length is 2.099.</p>
<p>To illustrate, again let’s assume that Harry’s resection was 5 cm, and Sally’s was 4 cm. Then we have:</p>
<p><span class="math display">\[
log\left(\frac{Pr(Harry died)}{1 - Pr(Harry died)}\right) = -4.4337 + 0.7417 \times 5 = -0.7253, \\
log\left(\frac{Pr(Sally died)}{1 - Pr(Sally died)}\right) = -4.4337 + 0.7417 \times 4 = -1.4667.
\]</span></p>
<p>which implies that our estimated odds of death for Harry and Sally are:</p>
<p><span class="math display">\[
Odds(Harry died) = \frac{Pr(Harry died)}{1 - Pr(Harry died)} = e^{-4.4337 + 0.7417 \times 5} = e^{-0.7253} = 0.4842 \\
Odds(Sally died) = \frac{Pr(Sally died)}{1 - Pr(Sally died)} = e^{-4.4337 + 0.7417 \times 4} = e^{-1.4667} = 0.2307
\]</span></p>
<p>and so the odds ratio is:</p>
<p><span class="math display">\[
OR = \frac{Odds(Harry died)}{Odds(Sally died)} = \frac{0.4842}{0.2307} = 2.099
\]</span></p>
<ul>
<li>If the odds ratio was 1, that would mean that Harry and Sally had the same estimated odds of death, and thus the same estimated probability of death, despite having different sizes of resections.</li>
<li>Since the odds ratio is greater than 1, it means that Harry has a higher estimated odds of death than Sally, and thus that Harry has a higher estimated probability of death than Sally.</li>
<li>If the odds ratio was less than 1, it would mean that Harry had a lower estimated odds of death than Sally, and thus that Harry had a lower estimated probability of death than Sally.</li>
</ul>
<p>Remember that the odds ratio is a fraction describing two positive numbers (odds can only be non-negative) so that the smallest possible odds ratio is 0.</p>
</div>
<div id="interpreting-the-rest-of-the-model-output-from-glm" class="section level3">
<h3><span class="header-section-number">12.2.6</span> Interpreting the rest of the model output from <code>glm</code></h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res_modA</code></pre></div>
<pre><code>
Call:  glm(formula = died ~ resection, family = &quot;binomial&quot;, data = resect)

Coefficients:
(Intercept)    resection  
    -4.4337       0.7417  

Degrees of Freedom: 133 Total (i.e. Null);  132 Residual
Null Deviance:      101.9 
Residual Deviance: 89.49    AIC: 93.49</code></pre>
<p>In addition to specifying the logistic regression coefficients, we are also presented with information on degrees of freedom, deviance (null and residual) and AIC.</p>
<ul>
<li>The degrees of freedom indicate the sample size.
<ul>
<li>Recall that we had <em>n</em> = 134 subjects in the data. The “Null” model includes only an intercept term (which uses 1 df) and we thus have <em>n</em> - 1 (here 133) degrees of freedom available for estimation.</li>
<li>In our <code>res_modA</code> model, a logistic regression is fit including a single slope (resection) and an intercept term. Each uses up one degree of freedom to build an estimate, so we have <em>n</em> - 2 = 134 - 2 = 132 residual df remaining.</li>
</ul></li>
<li>The AIC or Akaike Information Criterion (lower values are better) is also provided. This is helpful if we’re comparing multiple models for the same outcome.</li>
</ul>
</div>
<div id="deviance-and-comparing-our-model-to-the-null-model" class="section level3">
<h3><span class="header-section-number">12.2.7</span> Deviance and Comparing Our Model to the Null Model</h3>
<ul>
<li>The deviance (a measure of the model’s <em>lack of fit</em>) is available for both the null model (the model with only an intercept) and for our model (<code>res_modA</code>) predicting our outcome, mortality.</li>
<li>The deviance test, though available in R (see below) isn’t really a test of whether the model works well. Instead, it assumes the model is true, and then tests to see if the coefficients are detectably different from zero. So it isn’t of much practical use.
<ul>
<li>To compare the <code>deviance</code> statistics, we can subtract the residual deviance from the null deviance to decribe the impact of our model on fit.</li>
<li>Null Deviance - Residual Deviance can be compared to a <span class="math inline">\(\chi^2\)</span> distribution with Null DF - Residual DF degrees of freedom to obtain a global test of the in-sample predictive power of our model.</li>
<li>We can see this comparison more directly by running <code>anova</code> on our model:</li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(res_modA)</code></pre></div>
<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: died

Terms added sequentially (first to last)

          Df Deviance Resid. Df Resid. Dev
NULL                        133    101.943
resection  1    12.45       132     89.493</code></pre>
<p>To complete a deviance test and obtain a <em>p</em> value, we can run the following code to estimate the probability that a chi-square distribution with a single degree of freedom would exhibit an improvement in deviance as large as 12.45.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pchisq</span>(<span class="fl">12.45</span>, <span class="dv">1</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>[1] 0.0004179918</code></pre>
<p>The <em>p</em> value for the deviance test here is about 0.0004. But, again, this isn’t a test of whether the model is any good - it assumes the model is true, and then tests some consequences.</p>
<ul>
<li>Specifically, it tests whether (if the model is TRUE) some of the model’s coefficients are non-zero.</li>
<li>That’s not so practially useful, so I discourage you from performing global tests of a logistic regression model with a deviance test.</li>
</ul>
</div>
<div id="using-glance-with-a-logistic-regression-model" class="section level3">
<h3><span class="header-section-number">12.2.8</span> Using <code>glance</code> with a logistic regression model</h3>
<p>We can use the <code>glance</code> function from the <code>broom</code> package to obtain the null and residual deviance and degrees of freedom. Note that the deviance for our model is related to the log likelihood by -2*<code>logLik</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glance</span>(res_modA)</code></pre></div>
<pre><code>  null.deviance df.null    logLik      AIC     BIC deviance df.residual
1      101.9431     133 -44.74646 93.49292 99.2886 89.49292         132</code></pre>
<p>The <code>glance</code> result also provides the AIC, and the BIC (Bayes Information Criterion), each of which is helpful in understanding comparisons between multiple models for the same outcome (with smaller values of either criterion indicating better models.) The AIC is based on the deviance, but penalizes you for making the model more complicated. The BIC does the same sort of thing but with a different penalty.</p>
<p>Again we see that we have a null deviance of 101.94 on 133 degrees of freedom. Including the <code>resection</code> information in the model decreased the deviance to 89.49 points on 132 degrees of freedom, so that’s a decrease of 12.45 points while using one degree of freedom, a statistically significant reduction in deviance.</p>
</div>
</div>
<div id="interpreting-the-model-summary" class="section level2">
<h2><span class="header-section-number">12.3</span> Interpreting the Model Summary</h2>
<p>Let’s get a more detailed summary of our <code>res_modA</code> model, including 95% confidence intervals for the coefficients:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(res_modA)</code></pre></div>
<pre><code>
Call:
glm(formula = died ~ resection, family = &quot;binomial&quot;, data = resect)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1844  -0.5435  -0.3823  -0.2663   2.4501  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -4.4337     0.8799  -5.039 4.67e-07 ***
resection     0.7417     0.2230   3.327 0.000879 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 101.943  on 133  degrees of freedom
Residual deviance:  89.493  on 132  degrees of freedom
AIC: 93.493

Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(res_modA, <span class="dt">level =</span> <span class="fl">0.95</span>)</code></pre></div>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>                2.5 %    97.5 %
(Intercept) -6.344472 -2.855856
resection    0.322898  1.208311</code></pre>
<p>Some elements of this summary are very familiar from our work with linear models.</p>
<ul>
<li>We still have a five-number summary of residuals, although these are called <em>deviance</em> residuals.</li>
<li>We have a table of coefficients with standard errors, and hypothesis tests, although these are Wald z-tests, rather than the t tests we saw in linear modeling.</li>
<li>We have a summary of global fit in the comparison of null deviance and residual deviance, but without a formal p value. And we have the AIC, as discussed above.</li>
<li>We also have some new items related to a <em>dispersion</em> parameter and to the number of Fisher Scoring Iterations.</li>
</ul>
<p>Let’s walk through each of these elements.</p>
<div id="wald-z-tests-for-coefficients-in-a-logistic-regression" class="section level3">
<h3><span class="header-section-number">12.3.1</span> Wald Z tests for Coefficients in a Logistic Regression</h3>
<p>The coefficients output provides the estimated coefficients, and their standard errors, plus a Wald Z statistic, which is just the estimated coefficient divided by its standard error. This is compared to a standard Normal distribution to obtain the two-tailed p values summarized in the <code>Pr(&gt;|z|)</code> column.</p>
<ul>
<li>The interesting result is <code>resection</code>, which has a Wald Z = 3.327, yielding a <em>p</em> value of 0.00088.</li>
<li>The hypotheses being tested here are H_0_: <code>resection</code> does not have an effect on the log odds of <code>died</code> vs. H_A_: <code>resection</code> does have such an effect.</li>
<li>Another way of stating this is that the <em>p</em> value assesses whether the estimated coefficient of <code>resection</code>, 0.7417, is statistically detectably different from 0. If the coefficient (on the logit scale) for <code>resection</code> was truly 0, this would mean that:
<ul>
<li>the log odds of death did not change based on the <code>resection</code> size,</li>
<li>the odds of death were unchanged based on the <code>resection</code> size (the odds ratio would be 1), and</li>
<li>the probability of death was unchanged based on the <code>resection</code> size.</li>
</ul></li>
</ul>
<p>In our case, we have a statistically detectable change in the log odds of <code>died</code> associated with changes in <code>resection</code>, according to this <em>p</em> value. We conclude that <code>resection</code> size is associated with a positive impact on death rates (death rates are generally higher for people with larger resections.)</p>
</div>
<div id="confidence-intervals-for-the-coefficients" class="section level3">
<h3><span class="header-section-number">12.3.2</span> Confidence Intervals for the Coefficients</h3>
<p>As in linear regression, we can obtain 95% confidence intervals (to get other levels, change the <code>level</code> parameter in <code>confint</code>) for the intercept and slope coefficients.</p>
<p>Here, we see, for example, that the coefficient of <code>resection</code> has a point estimate of 0.7417, and a confidence interval of (0.3229, 1.208). Since this is on the logit scale, it’s not that interpretable, but we will often exponentiate the model and its confidence interval to obtain a more interpretable result on the odds ratio scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">coef</span>(res_modA))</code></pre></div>
<pre><code>(Intercept)   resection 
 0.01186995  2.09947754 </code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">confint</span>(res_modA))</code></pre></div>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>                  2.5 %     97.5 %
(Intercept) 0.001756429 0.05750655
resection   1.381124459 3.34782604</code></pre>
<p>From this output, we can estimate the odds ratio for death associated with a 1 cm increase in resection size is 2.099, with a 95% CI of (1.38, 3.35). - If the odds ratio was 1, it would indicate that the odds of death did not change based on the change in resection size. - Here, it’s clear that the estimated odds of death will be larger (odds &gt; 1) for subjects with larger resection sizes. Larger odds of death also indicate larger probabilities of death. This confidence interval indicates that with 95% confidence, we conclude that increases in resection size are associated with statistically detectable increases in the odds of death. - If the odds ratio was less than 1 (remember that it cannot be less than 0) that would mean that subjects with larger resection sizes were associated with smaller estimated odds of death.</p>
</div>
<div id="deviance-residuals" class="section level3">
<h3><span class="header-section-number">12.3.3</span> Deviance Residuals</h3>
<p>In logistic regression, it’s certainly a good idea to check to see how well the model fits the data. However, there are a few different types of residuals. The residuals presented here by default are called deviance residuals. Other types of residuals are available for generalized linear models, such as Pearson residuals, working residuals, and response residuals. Logistic regression model diagnostics often make use of multiple types of residuals.</p>
<p>The deviance residuals for each individual subject sum up to the deviance statistic for the model, and describe the contribution of each point to the model likelihood function.</p>
<p>The deviance residual, <span class="math inline">\(d_i\)</span>, for the i<sup>th</sup> observation in a model predicting <span class="math inline">\(y_i\)</span> (a binary variable), with the estimate being <span class="math inline">\(\hat{\pi}_i\)</span> is:</p>
<p><span class="math display">\[
d_i = s_i \sqrt{-2 [y_i log \hat{\pi_i} + (1 - y_i) log(1 - \hat{\pi_i})]},
\]</span></p>
<p>where <span class="math inline">\(s_i\)</span> is 1 if <span class="math inline">\(y_i = 1\)</span> and <span class="math inline">\(s_i = -1\)</span> if <span class="math inline">\(y_i = 0\)</span>.</p>
<p>Again, the sum of the deviance residuals is the deviance.</p>
</div>
<div id="dispersion-parameter" class="section level3">
<h3><span class="header-section-number">12.3.4</span> Dispersion Parameter</h3>
<p>The dispersion parameter is taken to be 1 for <code>glm</code> fit using either the <code>binomial</code> or <code>Poisson</code> families. For other sorts of generalized linear models, the dispersion parameter will be of some importance in estimating standard errors sensibly.</p>
</div>
<div id="fisher-scoring-iterations" class="section level3">
<h3><span class="header-section-number">12.3.5</span> Fisher Scoring iterations</h3>
<p>The solution of a logistic regression model involves maximizing a likelihood function. Fisher’s scoring algorithm in our <code>res_modA</code> needed five iterations to perform the logistic regression fit. All that this tells you is that the model converged, and didn’t require a lot of time to do so.</p>
</div>
</div>
<div id="plotting-a-simple-logistic-regression-model" class="section level2">
<h2><span class="header-section-number">12.4</span> Plotting a Simple Logistic Regression Model</h2>
<p>Let’s plot the logistic regression model <code>res_modA</code> for <code>died</code> using the extent of the resection in terms of probabilities. We can use either of two different approaches:</p>
<ul>
<li>we can plot the fitted values from our specific model against the original data, using the <code>augment</code> function from the <code>broom</code> package, or</li>
<li>we can create a smooth function called <code>binomial_smooth</code> that plots a simple logistic model in an analogous way to <code>geom_smooth(method = &quot;lm&quot;)</code> for a simple linear regression.</li>
</ul>
<div id="using-augment-to-capture-the-fitted-probabilities" class="section level3">
<h3><span class="header-section-number">12.4.1</span> Using <code>augment</code> to capture the fitted probabilities</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res_A_aug &lt;-<span class="st"> </span><span class="kw">augment</span>(res_modA, resect, 
                     <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>)
<span class="kw">head</span>(res_A_aug)</code></pre></div>
<pre><code>  id age prior resection intubated died    .fitted    .se.fit     .resid
1  1  34     1       2.5         0    0 0.07046791 0.02562381 -0.3822929
2  2  57     0       5.0         0    0 0.32622637 0.08605551 -0.8886631
3  3  60     1       4.0         1    1 0.18740037 0.04269795  1.8300317
4  4  62     1       4.2         0    0 0.21104240 0.04871389 -0.6885386
5  5  28     0       6.0         1    1 0.50409637 0.14302982  1.1704596
6  6  52     0       3.0         0    0 0.09897375 0.02867196 -0.4565542
         .hat    .sigma      .cooksd .std.resid
1 0.010024061 0.8258481 0.0003876961 -0.3842235
2 0.033691765 0.8227475 0.0087350915 -0.9040227
3 0.011972088 0.8107264 0.0265893468  1.8410857
4 0.014252277 0.8243062 0.0019617278 -0.6934983
5 0.081835623 0.8196110 0.0477480056  1.2215077
6 0.009218619 0.8255581 0.0005157780 -0.4586733</code></pre>
<p>This approach augments the <code>resect</code> data set with fitted, residual and other summaries of each observation’s impact on the fit, using the “response” type of prediction, which yields the fitted probabilities in the <code>.fitted</code> column.</p>
</div>
<div id="plotting-a-logistic-regression-models-fitted-values" class="section level3">
<h3><span class="header-section-number">12.4.2</span> Plotting a Logistic Regression Model’s Fitted Values</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(res_A_aug, <span class="kw">aes</span>(<span class="dt">x =</span> resection, <span class="dt">y =</span> died)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_jitter</span>(<span class="dt">height =</span> <span class="fl">0.05</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x =</span> resection, <span class="dt">y =</span> .fitted), 
              <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Logistic Regression from Model res_modA&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-153-1.png" width="672" /></p>
</div>
<div id="plotting-a-simple-logistic-model-using-binomial_smooth" class="section level3">
<h3><span class="header-section-number">12.4.3</span> Plotting a Simple Logistic Model using <code>binomial_smooth</code></h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">binomial_smooth &lt;-<span class="st"> </span><span class="cf">function</span>(...) {
  <span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>, 
              <span class="dt">method.args =</span> <span class="kw">list</span>(<span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>), ...)
}

<span class="kw">ggplot</span>(resect, <span class="kw">aes</span>(<span class="dt">x =</span> resection, <span class="dt">y =</span> died)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">height =</span> <span class="fl">0.05</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">binomial_smooth</span>() <span class="op">+</span><span class="st"> </span>## ...smooth(se=FALSE) to leave out interval
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Logistic Regression from Model A&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-154-1.png" width="672" /></p>
<p>As expected, we see an increase in the model probability of death as the extent of the resection grows larger.</p>
</div>
</div>
<div id="receiver-operating-characteristic-curve-analysis" class="section level2">
<h2><span class="header-section-number">12.5</span> Receiver Operating Characteristic Curve Analysis</h2>
<p>One way to assess the predictive accuracy within the model development sample in a logistic regression is to consider an analyses based on the receiver operating characteristic (ROC) curve. ROC curves are commonly used in assessing diagnoses in medical settings, and in signal detection applications.</p>
<p>The accuracy of a “test” can be evaluated by considering two types of errors: false positives and false negatives.</p>
<p>In our <code>res_modA</code> model, we use <code>resection</code> size to predict whether the patient <code>died</code>. Suppopse we established a value R, so that if the resection size was larger than R cm, we would predict that the patient <code>died</code>, and otherwise we would predict that the patient did not die.</p>
<p>A good outcome of our model’s “test”, then, would be when the resection size is larger than R for a patient who actually died. Another good outcome would be when the resection size is smaller than R for a patient who survived.</p>
<p>But we can make errors, too.</p>
<ul>
<li>A false positive error in this setting would occur when the resection size is larger than R (so we predict the patient dies) but in fact the patient does not die.</li>
<li>A false negative error in this case would occur when the resection size is smaller than R (so we predict the patient survives) but in fact the patient dies.</li>
</ul>
<p>Formally, the true positive fraction (TPF) for a specific resection cutoff <span class="math inline">\(R\)</span>, is the probability of a positive test (a prediction that the patient will die) among the people who have the outcome died = 1 (those who actually die).</p>
<p><span class="math display">\[
TPF(R) = Pr(resection &gt; R | subject died)
\]</span></p>
<p>Similarly, the false positive fraction (FPF) for a specific cutoff <span class="math inline">\(R\)</span> is the probability of a positive test (prediction that the patient will die) among the people with died = 0 (those who don’t actually die)</p>
<p><span class="math display">\[
FPF(R) = Pr(resection &gt; R | subject did not die)
\]</span></p>
<p>The True Positive Rate is referred to as the sensitivity of a diagnostic test, and the True Negative rate (1 - the False Positive rate) is referred to as the specificity of a diagnostic test.</p>
<p>Since the cutoff <span class="math inline">\(R\)</span> is not fixed in advanced, we can plot the value of TPF (on the y axis) against FPF (on the x axis) for all possible values of <span class="math inline">\(R\)</span>, and this is what the ROC curve is. Others refer to the Sensitivity on the Y axis, and 1-Specificity on the X axis, and this is the same idea.</p>
<p>Before we get too far into the weeds, let me show you some simple situations so you can understand what you might learn from the ROC curve. The web page <a href="http://blog.yhat.com/posts/roc-curves.html" class="uri">http://blog.yhat.com/posts/roc-curves.html</a> provides source materials.</p>
<div id="interpreting-the-area-under-the-roc-curve" class="section level3">
<h3><span class="header-section-number">12.5.1</span> Interpreting the Area under the ROC curve</h3>
<p>The AUC or Area under the ROC curve is the amount of space underneath the ROC curve. Often referred to as the c statistic, the AUC represents the quality of your TPR and FPR overall in a single number. The C statistic ranges from 0 to 1, with C = 0.5 for a prediction that is no better than random guessing, and C = 1 for a perfect prediction model.</p>
<p>Next, I’ll build a simulation to demonstrate several possible ROC curves in the sections that follow.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">432999</span>)
sim.temp &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">200</span>), 
                       <span class="dt">prob =</span> <span class="kw">exp</span>(x)<span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x)), 
                       <span class="dt">y =</span> <span class="kw">as.numeric</span>(<span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="kw">runif</span>(<span class="dv">200</span>) <span class="op">&lt;</span><span class="st"> </span>prob))

sim.temp &lt;-<span class="st"> </span>sim.temp <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">p_guess =</span> <span class="dv">1</span>,
           <span class="dt">p_perfect =</span> y, 
           <span class="dt">p_bad =</span> <span class="kw">exp</span>(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>x) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>x)),
           <span class="dt">p_ok =</span> prob <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span><span class="kw">runif</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.05</span>),
           <span class="dt">p_good =</span> prob <span class="op">+</span><span class="st"> </span>y<span class="op">*</span><span class="kw">runif</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.27</span>))</code></pre></div>
<div id="what-if-we-are-guessing" class="section level4">
<h4><span class="header-section-number">12.5.1.1</span> What if we are guessing?</h4>
<p>If we’re guessing completely at random, then the model should correctly classify a subject (as died or not died) about 50% of the time, so the TPR and FPR will be equal. This yields a diagonal line in the ROC curve, and an area under the curve (C statistic) of 0.5.</p>
<p>There are several ways to do this on the web, but I’ll show this one, which has some bizarre code, but that’s a function of using a package called <code>ROCR</code> to do the work. It comes from <a href="http://blog.yhat.com/posts/roc-curves.html">this link</a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred_guess &lt;-<span class="st"> </span><span class="kw">prediction</span>(sim.temp<span class="op">$</span>p_guess, sim.temp<span class="op">$</span>y)
perf_guess &lt;-<span class="st"> </span><span class="kw">performance</span>(pred_guess, <span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)
auc_guess &lt;-<span class="st"> </span><span class="kw">performance</span>(pred_guess, <span class="dt">measure=</span><span class="st">&quot;auc&quot;</span>)

auc_guess &lt;-<span class="st"> </span><span class="kw">round</span>(auc_guess<span class="op">@</span>y.values[[<span class="dv">1</span>]],<span class="dv">3</span>)
roc_guess &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">fpr=</span><span class="kw">unlist</span>(perf_guess<span class="op">@</span>x.values),
                        <span class="dt">tpr=</span><span class="kw">unlist</span>(perf_guess<span class="op">@</span>y.values),
                        <span class="dt">model=</span><span class="st">&quot;GLM&quot;</span>)

<span class="kw">ggplot</span>(roc_guess, <span class="kw">aes</span>(<span class="dt">x=</span>fpr, <span class="dt">ymin=</span><span class="dv">0</span>, <span class="dt">ymax=</span>tpr)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_ribbon</span>(<span class="dt">alpha=</span><span class="fl">0.2</span>, <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>tpr), <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;Guessing: ROC Curve w/ AUC=&quot;</span>, auc_guess)) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-156-1.png" width="672" /></p>
</div>
<div id="what-if-we-classify-things-perfectly" class="section level4">
<h4><span class="header-section-number">12.5.1.2</span> What if we classify things perfectly?</h4>
<p>If we’re classifying subjects perfectly, then we have a TPR of 1 and an FPR of 0. That yields an ROC curve that looks like the upper and left edges of a box. If our model correctly classifies a subject (as died or not died) 100% of the time, the area under the curve (c statistic) will be 1.0. We’ll add in the diagonal line here (in a dashed black line) to show how this model compares to random guessing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred_perf &lt;-<span class="st"> </span><span class="kw">prediction</span>(sim.temp<span class="op">$</span>p_perfect, sim.temp<span class="op">$</span>y)
perf_perf &lt;-<span class="st"> </span><span class="kw">performance</span>(pred_perf, <span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)
auc_perf &lt;-<span class="st"> </span><span class="kw">performance</span>(pred_perf, <span class="dt">measure=</span><span class="st">&quot;auc&quot;</span>)

auc_perf &lt;-<span class="st"> </span><span class="kw">round</span>(auc_perf<span class="op">@</span>y.values[[<span class="dv">1</span>]],<span class="dv">3</span>)
roc_perf &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">fpr=</span><span class="kw">unlist</span>(perf_perf<span class="op">@</span>x.values),
                        <span class="dt">tpr=</span><span class="kw">unlist</span>(perf_perf<span class="op">@</span>y.values),
                        <span class="dt">model=</span><span class="st">&quot;GLM&quot;</span>)

<span class="kw">ggplot</span>(roc_perf, <span class="kw">aes</span>(<span class="dt">x=</span>fpr, <span class="dt">ymin=</span><span class="dv">0</span>, <span class="dt">ymax=</span>tpr)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_ribbon</span>(<span class="dt">alpha=</span><span class="fl">0.2</span>, <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>tpr), <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;Perfect Prediction: ROC Curve w/ AUC=&quot;</span>, auc_perf)) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-157-1.png" width="672" /></p>
</div>
<div id="what-does-worse-than-guessing-look-like" class="section level4">
<h4><span class="header-section-number">12.5.1.3</span> What does “worse than guessing” look like?</h4>
<p>A bad classifier will appear below and to the right of the diagonal line we’d see if we were completely guessing. Such a model will have a c statistic below 0.5, and will be valueless.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred_bad &lt;-<span class="st"> </span><span class="kw">prediction</span>(sim.temp<span class="op">$</span>p_bad, sim.temp<span class="op">$</span>y)
perf_bad &lt;-<span class="st"> </span><span class="kw">performance</span>(pred_bad, <span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)
auc_bad &lt;-<span class="st"> </span><span class="kw">performance</span>(pred_bad, <span class="dt">measure=</span><span class="st">&quot;auc&quot;</span>)

auc_bad &lt;-<span class="st"> </span><span class="kw">round</span>(auc_bad<span class="op">@</span>y.values[[<span class="dv">1</span>]],<span class="dv">3</span>)
roc_bad &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">fpr=</span><span class="kw">unlist</span>(perf_bad<span class="op">@</span>x.values),
                        <span class="dt">tpr=</span><span class="kw">unlist</span>(perf_bad<span class="op">@</span>y.values),
                        <span class="dt">model=</span><span class="st">&quot;GLM&quot;</span>)

<span class="kw">ggplot</span>(roc_bad, <span class="kw">aes</span>(<span class="dt">x=</span>fpr, <span class="dt">ymin=</span><span class="dv">0</span>, <span class="dt">ymax=</span>tpr)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_ribbon</span>(<span class="dt">alpha=</span><span class="fl">0.2</span>, <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>tpr), <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;A Bad Model: ROC Curve w/ AUC=&quot;</span>, auc_bad)) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-158-1.png" width="672" /></p>
</div>
<div id="what-does-better-than-guessing-look-like" class="section level4">
<h4><span class="header-section-number">12.5.1.4</span> What does “better than guessing” look like?</h4>
<p>An “OK” classifier will appear above and to the left of the diagonal line we’d see if we were completely guessing. Such a model will have a c statistic above 0.5, and might have some value. The plot below shows a very fairly poor model, but at least it’s better than guessing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred_ok &lt;-<span class="st"> </span><span class="kw">prediction</span>(sim.temp<span class="op">$</span>p_ok, sim.temp<span class="op">$</span>y)
perf_ok &lt;-<span class="st"> </span><span class="kw">performance</span>(pred_ok, <span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)
auc_ok &lt;-<span class="st"> </span><span class="kw">performance</span>(pred_ok, <span class="dt">measure=</span><span class="st">&quot;auc&quot;</span>)

auc_ok &lt;-<span class="st"> </span><span class="kw">round</span>(auc_ok<span class="op">@</span>y.values[[<span class="dv">1</span>]],<span class="dv">3</span>)
roc_ok &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">fpr=</span><span class="kw">unlist</span>(perf_ok<span class="op">@</span>x.values),
                        <span class="dt">tpr=</span><span class="kw">unlist</span>(perf_ok<span class="op">@</span>y.values),
                        <span class="dt">model=</span><span class="st">&quot;GLM&quot;</span>)

<span class="kw">ggplot</span>(roc_ok, <span class="kw">aes</span>(<span class="dt">x=</span>fpr, <span class="dt">ymin=</span><span class="dv">0</span>, <span class="dt">ymax=</span>tpr)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_ribbon</span>(<span class="dt">alpha=</span><span class="fl">0.2</span>, <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>tpr), <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;A Mediocre Model: ROC Curve w/ AUC=&quot;</span>, auc_ok)) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-159-1.png" width="672" /></p>
<p>Sometimes people grasp for a rough guide as to the accuracy of a model’s predictions based on the area under the ROC curve. A common thought is to assess the C statistic much like you would a class grade.</p>
<table style="width:81%;">
<colgroup>
<col width="16%" />
<col width="63%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">C statistic</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.90 to 1.00</td>
<td>model does an excellent job at discriminating “yes” from “no” (A)</td>
</tr>
<tr class="even">
<td align="right">0.80 to 0.90</td>
<td>model does a good job (B)</td>
</tr>
<tr class="odd">
<td align="right">0.70 to 0.80</td>
<td>model does a fair job (C)</td>
</tr>
<tr class="even">
<td align="right">0.60 to 0.70</td>
<td>model does a poor job (D)</td>
</tr>
<tr class="odd">
<td align="right">0.50 to 0.60</td>
<td>model fails (F)</td>
</tr>
<tr class="even">
<td align="right">below 0.50</td>
<td>model is worse than random guessing</td>
</tr>
</tbody>
</table>
</div>
<div id="what-does-pretty-good-look-like" class="section level4">
<h4><span class="header-section-number">12.5.1.5</span> What does “pretty good” look like?</h4>
<p>A strong and good classifier will appear above and to the left of the diagonal line we’d see if we were completely guessing, often with a nice curve that is continually increasing and appears to be pulled up towards the top left. Such a model will have a c statistic well above 0.5, but not as large as 1. The plot below shows a stronger model, which appears substantially better than guessing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred_good &lt;-<span class="st"> </span><span class="kw">prediction</span>(sim.temp<span class="op">$</span>p_good, sim.temp<span class="op">$</span>y)
perf_good &lt;-<span class="st"> </span><span class="kw">performance</span>(pred_good, <span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)
auc_good &lt;-<span class="st"> </span><span class="kw">performance</span>(pred_good, <span class="dt">measure=</span><span class="st">&quot;auc&quot;</span>)

auc_good &lt;-<span class="st"> </span><span class="kw">round</span>(auc_good<span class="op">@</span>y.values[[<span class="dv">1</span>]],<span class="dv">3</span>)
roc_good &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">fpr=</span><span class="kw">unlist</span>(perf_good<span class="op">@</span>x.values),
                        <span class="dt">tpr=</span><span class="kw">unlist</span>(perf_good<span class="op">@</span>y.values),
                        <span class="dt">model=</span><span class="st">&quot;GLM&quot;</span>)

<span class="kw">ggplot</span>(roc_good, <span class="kw">aes</span>(<span class="dt">x=</span>fpr, <span class="dt">ymin=</span><span class="dv">0</span>, <span class="dt">ymax=</span>tpr)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_ribbon</span>(<span class="dt">alpha=</span><span class="fl">0.2</span>, <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>tpr), <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;A Pretty Good Model: ROC Curve w/ AUC=&quot;</span>, auc_good)) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-160-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="the-roc-plot-for-res_moda" class="section level2">
<h2><span class="header-section-number">12.6</span> The ROC Plot for <code>res_modA</code></h2>
<p>Let me show you the ROC curve for our <code>res_modA</code> model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## requires ROCR package
prob &lt;-<span class="st"> </span><span class="kw">predict</span>(res_modA, resect, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)
pred &lt;-<span class="st"> </span><span class="kw">prediction</span>(prob, resect<span class="op">$</span>died)
perf &lt;-<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)
auc &lt;-<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure=</span><span class="st">&quot;auc&quot;</span>)
## the rest of this code is a little strange
auc &lt;-<span class="st"> </span><span class="kw">round</span>(auc<span class="op">@</span>y.values[[<span class="dv">1</span>]],<span class="dv">3</span>)
roc.data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">fpr=</span><span class="kw">unlist</span>(perf<span class="op">@</span>x.values),
                       <span class="dt">tpr=</span><span class="kw">unlist</span>(perf<span class="op">@</span>y.values),
                       <span class="dt">model=</span><span class="st">&quot;GLM&quot;</span>)

<span class="kw">ggplot</span>(roc.data, <span class="kw">aes</span>(<span class="dt">x=</span>fpr, <span class="dt">ymin=</span><span class="dv">0</span>, <span class="dt">ymax=</span>tpr)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_ribbon</span>(<span class="dt">alpha=</span><span class="fl">0.2</span>, <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>tpr), <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;ROC Curve w/ AUC=&quot;</span>, auc)) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-161-1.png" width="672" /></p>
<p>Based on the C statistic (AUC = 0.771) this would rank somewhere near the high end of a “fair” predictive model by this standard, not quite to the level of a “good” model.</p>
</div>
<div id="assessing-residual-plots-from-model-a" class="section level2">
<h2><span class="header-section-number">12.7</span> Assessing Residual Plots from Model A</h2>
<blockquote>
<p>Residuals are certainly less informative for logistic regression than they are for linear regression: not only do yes/no outcomes inherently contain less information than continuous ones, but the fact that the adjusted response depends on the fit hampers our ability to use residuals as external checks on the model.</p>
</blockquote>
<blockquote>
<p>This is mitigated to some extent, however, by the fact that we are also making fewer distributional assumptions in logistic regression, so there is no need to inspect residuals for, say, skewness or heteroskedasticity.</p>
</blockquote>
<ul>
<li>Patrick Breheny, University of Kentucky, <a href="https://web.as.uky.edu/statistics/users/pbreheny/760/S13/notes/3-26.pdf">Slides on GLM Residuals and Diagnostics</a></li>
</ul>
<p>The usual residual plots are available in R for a logistic regression model, but most of them are irrelevant in the logistic regression setting. The residuals shouldn’t follow a standard Normal distribution, and they will not show constant variance over the range of the predictor variables, so plots looking into those issues aren’t helpful.</p>
<p>The only plot from the standard set that we’ll look at in many settings is plot 5, which helps us assess influence (via Cook’s distance contours), and a measure related to leverage (how unusual an observation is in terms of the predictors) and standardized Pearson residuals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(res_modA, <span class="dt">which =</span> <span class="dv">5</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-162-1.png" width="672" /></p>
<p>In this case, I don’t see any highly influential points, as no points fall outside of the Cook’s distance (0.5 or 1) contours.</p>
</div>
<div id="model-b-a-kitchen-sink-logistic-regression-model" class="section level2">
<h2><span class="header-section-number">12.8</span> Model B: A “Kitchen Sink” Logistic Regression Model</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res_modB &lt;-<span class="st"> </span><span class="kw">glm</span>(died <span class="op">~</span><span class="st"> </span>resection <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>prior <span class="op">+</span><span class="st"> </span>intubated,
               <span class="dt">data =</span> resect, <span class="dt">family =</span> binomial)

res_modB</code></pre></div>
<pre><code>
Call:  glm(formula = died ~ resection + age + prior + intubated, family = binomial, 
    data = resect)

Coefficients:
(Intercept)    resection          age        prior    intubated  
  -5.152886     0.612211     0.001173     0.814691     2.810797  

Degrees of Freedom: 133 Total (i.e. Null);  129 Residual
Null Deviance:      101.9 
Residual Deviance: 67.36    AIC: 77.36</code></pre>
<div id="comparing-model-a-to-model-b" class="section level3">
<h3><span class="header-section-number">12.8.1</span> Comparing Model A to Model B</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(res_modA, res_modB)</code></pre></div>
<pre><code>Analysis of Deviance Table

Model 1: died ~ resection
Model 2: died ~ resection + age + prior + intubated
  Resid. Df Resid. Dev Df Deviance
1       132     89.493            
2       129     67.359  3   22.134</code></pre>
<p>The addition of <code>age</code>, <code>prior</code> and <code>intubated</code> reduces the lack of fit by 22.134 points, at a cost of 3 degrees of freedom.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glance</span>(res_modA)</code></pre></div>
<pre><code>  null.deviance df.null    logLik      AIC     BIC deviance df.residual
1      101.9431     133 -44.74646 93.49292 99.2886 89.49292         132</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glance</span>(res_modB)</code></pre></div>
<pre><code>  null.deviance df.null   logLik     AIC     BIC deviance df.residual
1      101.9431     133 -33.6793 77.3586 91.8478  67.3586         129</code></pre>
<p>By either AIC or BIC, the larger model (<code>res_modB</code>) looks more effective.</p>
</div>
<div id="interpreting-model-b" class="section level3">
<h3><span class="header-section-number">12.8.2</span> Interpreting Model B</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(res_modB)</code></pre></div>
<pre><code>
Call:
glm(formula = died ~ resection + age + prior + intubated, family = binomial, 
    data = resect)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7831  -0.3741  -0.2386  -0.2014   2.5228  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.152886   1.469453  -3.507 0.000454 ***
resection    0.612211   0.282807   2.165 0.030406 *  
age          0.001173   0.020646   0.057 0.954700    
prior        0.814691   0.704785   1.156 0.247705    
intubated    2.810797   0.658395   4.269 1.96e-05 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 101.943  on 133  degrees of freedom
Residual deviance:  67.359  on 129  degrees of freedom
AIC: 77.359

Number of Fisher Scoring iterations: 6</code></pre>
<p>It appears that the <code>intubated</code> predictor adds significant value to the model, by the Wald test.</p>
<p>Let’s focus on the impact of these variables through odds ratios.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">coef</span>(res_modB))</code></pre></div>
<pre><code> (Intercept)    resection          age        prior    intubated 
 0.005782692  1.844504859  1.001173503  2.258476846 16.623153519 </code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">confint</span>(res_modB))</code></pre></div>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>                   2.5 %     97.5 %
(Intercept) 0.0002408626  0.0837263
resection   1.0804548590  3.3495636
age         0.9618416869  1.0442885
prior       0.5485116610  9.1679931
intubated   4.7473282453 64.6456919</code></pre>
<p>At a 5% significance level, we might conclude that:</p>
<ul>
<li>larger sized <code>resection</code>s are associated with a meaningful rise (est OR: 1.84, 95% CI 1.08, 3.35) in the odds of death, holding all other predictors constant,</li>
<li>the need for <code>intubation</code> at the end of surgery is associated with a substantial rise (est OR: 16.6, 95% CI 4.7, 64.7) in the odds of death, holding all other predictors constant, but that</li>
<li>older <code>age</code> as well as having a <code>prior</code> tracheal surgery appears to be associated with an increase in death risk, but not to an extent that we can declare statistically significant.</li>
</ul>
</div>
</div>
<div id="plotting-model-b" class="section level2">
<h2><span class="header-section-number">12.9</span> Plotting Model B</h2>
<p>Let’s think about plotting the fitted values from our model, in terms of probabilities.</p>
<div id="using-augment-to-capture-the-fitted-probabilities-1" class="section level3">
<h3><span class="header-section-number">12.9.1</span> Using <code>augment</code> to capture the fitted probabilities</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res_B_aug &lt;-<span class="st"> </span><span class="kw">augment</span>(res_modB, resect, 
                     <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>)
<span class="kw">head</span>(res_B_aug)</code></pre></div>
<pre><code>  id age prior resection intubated died    .fitted    .se.fit     .resid
1  1  34     1       2.5         0    0 0.05908963 0.03851118 -0.3490198
2  2  57     0       5.0         0    0 0.11660492 0.06253774 -0.4979613
3  3  60     1       4.0         1    1 0.72944600 0.15010423  0.7943172
4  4  62     1       4.2         0    0 0.15522494 0.09607978 -0.5808354
5  5  28     0       6.0         1    1 0.79641141 0.14588554  0.6747435
6  6  52     0       3.0         0    0 0.03713809 0.01933270 -0.2751191
        .hat    .sigma      .cooksd .std.resid
1 0.02667562 0.7247491 0.0003536652 -0.3537702
2 0.03796756 0.7240341 0.0010829917 -0.5076925
3 0.11416656 0.7215778 0.0107925872  0.8439524
4 0.07039819 0.7234665 0.0029937671 -0.6024273
5 0.13126049 0.7225958 0.0088920280  0.7239256
6 0.01045207 0.7250114 0.0000823406 -0.2765683</code></pre>
</div>
<div id="plotting-model-b-fits-by-observed-mortality" class="section level3">
<h3><span class="header-section-number">12.9.2</span> Plotting Model B Fits by Observed Mortality</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(res_B_aug, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">factor</span>(died), <span class="dt">y =</span> .fitted, <span class="dt">col =</span> <span class="kw">factor</span>(died))) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_boxplot</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_jitter</span>(<span class="dt">width =</span> <span class="fl">0.1</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">guides</span>(<span class="dt">col =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-169-1.png" width="672" /></p>
<p>Certainly it appears as though most of our predicted probabilities (of death) for the subjects who actually survived are quite small, but not all of them. We also have at least 6 big “misses” among the 17 subjects who actually died.</p>
</div>
<div id="the-roc-curve-for-model-b" class="section level3">
<h3><span class="header-section-number">12.9.3</span> The ROC curve for Model B</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## requires ROCR package
prob &lt;-<span class="st"> </span><span class="kw">predict</span>(res_modB, resect, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)
pred &lt;-<span class="st"> </span><span class="kw">prediction</span>(prob, resect<span class="op">$</span>died)
perf &lt;-<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)
auc &lt;-<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure=</span><span class="st">&quot;auc&quot;</span>)
## the rest of this code is a little strange
auc &lt;-<span class="st"> </span><span class="kw">round</span>(auc<span class="op">@</span>y.values[[<span class="dv">1</span>]],<span class="dv">3</span>)
roc.data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">fpr=</span><span class="kw">unlist</span>(perf<span class="op">@</span>x.values),
                       <span class="dt">tpr=</span><span class="kw">unlist</span>(perf<span class="op">@</span>y.values),
                       <span class="dt">model=</span><span class="st">&quot;GLM&quot;</span>)

<span class="kw">ggplot</span>(roc.data, <span class="kw">aes</span>(<span class="dt">x=</span>fpr, <span class="dt">ymin=</span><span class="dv">0</span>, <span class="dt">ymax=</span>tpr)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_ribbon</span>(<span class="dt">alpha=</span><span class="fl">0.2</span>, <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>tpr), <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;Model B: ROC Curve w/ AUC=&quot;</span>, auc)) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-170-1.png" width="672" /></p>
<p>The area under the curve (C-statistic) is 0.86, which certainly looks like a more discriminating fit than model A with resection alone.</p>
</div>
<div id="residuals-leverage-and-influence" class="section level3">
<h3><span class="header-section-number">12.9.4</span> Residuals, Leverage and Influence</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(res_modB, <span class="dt">which =</span> <span class="dv">5</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-171-1.png" width="672" /></p>
<p>Again, we see no signs of deeply influential points in this model.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Riffenburgh2006">
<p>Riffenburgh, Robert H. 2006. <em>Statistics in Medicine</em>. Second Edition. Burlington, MA: Elsevier Academic Press.</p>
</div>
<div id="ref-Peduzzi1996">
<p>Peduzzi, Peter, John Concato, Elizabeth Kemper, Theodore R. Holford, and Alvan R. Feinstein. 1996. “A Simulation Study of the Number of Events Per Variable in Logistic Regression Analysis.” <em>Journal of Clinical Epidemiology</em> 49 (12): 1373–9.</p>
</div>
<div id="ref-Long1997">
<p>Long, J. Scott. 1997. <em>Regression Models for Categorical and Limited Dependent Variables</em>. Thousand Oaks, CA: Sage Publications.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="other-variable-selection-strategies.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/12_logistic1.Rmd",
"text": "Edit"
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
