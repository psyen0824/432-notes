<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Science for Biological, Medical and Health Research: Notes for 432</title>
  <meta name="description" content="These are the Course Notes for 432.">
  <meta name="generator" content="bookdown 0.6 and GitBook 2.6.7">

  <meta property="og:title" content="Data Science for Biological, Medical and Health Research: Notes for 432" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the Course Notes for 432." />
  <meta name="github-repo" content="thomaselove/432-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Science for Biological, Medical and Health Research: Notes for 432" />
  
  <meta name="twitter:description" content="These are the Course Notes for 432." />
  

<meta name="author" content="Thomas E. Love, Ph.D.">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="stepwise-variable-selection.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">432 Course Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="r-packages-used-in-these-notes.html"><a href="r-packages-used-in-these-notes.html"><i class="fa fa-check"></i>R Packages used in these notes</a></li>
<li class="chapter" data-level="" data-path="data-used-in-these-notes.html"><a href="data-used-in-these-notes.html"><i class="fa fa-check"></i>Data used in these notes</a></li>
<li class="chapter" data-level="" data-path="special-functions-used-in-these-notes.html"><a href="special-functions-used-in-these-notes.html"><i class="fa fa-check"></i>Special Functions used in these notes</a></li>
<li class="chapter" data-level="1" data-path="building-table-1.html"><a href="building-table-1.html"><i class="fa fa-check"></i><b>1</b> Building Table 1</a><ul>
<li class="chapter" data-level="1.1" data-path="building-table-1.html"><a href="building-table-1.html#two-examples-from-the-new-england-journal-of-medicine"><i class="fa fa-check"></i><b>1.1</b> Two examples from the <em>New England Journal of Medicine</em></a><ul>
<li class="chapter" data-level="1.1.1" data-path="building-table-1.html"><a href="building-table-1.html#a-simple-table-1"><i class="fa fa-check"></i><b>1.1.1</b> A simple Table 1</a></li>
<li class="chapter" data-level="1.1.2" data-path="building-table-1.html"><a href="building-table-1.html#a-group-comparison"><i class="fa fa-check"></i><b>1.1.2</b> A group comparison</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="building-table-1.html"><a href="building-table-1.html#the-mr-clean-trial"><i class="fa fa-check"></i><b>1.2</b> The MR CLEAN trial</a></li>
<li class="chapter" data-level="1.3" data-path="building-table-1.html"><a href="building-table-1.html#simulated-fakestroke-data"><i class="fa fa-check"></i><b>1.3</b> Simulated <code>fakestroke</code> data</a></li>
<li class="chapter" data-level="1.4" data-path="building-table-1.html"><a href="building-table-1.html#building-table-1-for-fakestroke-attempt-1"><i class="fa fa-check"></i><b>1.4</b> Building Table 1 for <code>fakestroke</code>: Attempt 1</a><ul>
<li class="chapter" data-level="1.4.1" data-path="building-table-1.html"><a href="building-table-1.html#some-of-this-is-very-useful-and-other-parts-need-to-be-fixed."><i class="fa fa-check"></i><b>1.4.1</b> Some of this is very useful, and other parts need to be fixed.</a></li>
<li class="chapter" data-level="1.4.2" data-path="building-table-1.html"><a href="building-table-1.html#fakestroke-cleaning-up-categorical-variables"><i class="fa fa-check"></i><b>1.4.2</b> <code>fakestroke</code> Cleaning Up Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="building-table-1.html"><a href="building-table-1.html#fakestroke-table-1-attempt-2"><i class="fa fa-check"></i><b>1.5</b> <code>fakestroke</code> Table 1: Attempt 2</a><ul>
<li class="chapter" data-level="1.5.1" data-path="building-table-1.html"><a href="building-table-1.html#what-summaries-should-we-show"><i class="fa fa-check"></i><b>1.5.1</b> What summaries should we show?</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="building-table-1.html"><a href="building-table-1.html#obtaining-a-more-detailed-summary"><i class="fa fa-check"></i><b>1.6</b> Obtaining a more detailed Summary</a></li>
<li class="chapter" data-level="1.7" data-path="building-table-1.html"><a href="building-table-1.html#exporting-the-completed-table-1-from-r-to-excel-or-word"><i class="fa fa-check"></i><b>1.7</b> Exporting the Completed Table 1 from R to Excel or Word</a><ul>
<li class="chapter" data-level="1.7.1" data-path="building-table-1.html"><a href="building-table-1.html#approach-a-save-and-open-in-excel"><i class="fa fa-check"></i><b>1.7.1</b> Approach A: Save and open in Excel</a></li>
<li class="chapter" data-level="1.7.2" data-path="building-table-1.html"><a href="building-table-1.html#approach-b-produce-the-table-so-you-can-cut-and-paste-it"><i class="fa fa-check"></i><b>1.7.2</b> Approach B: Produce the Table so you can cut and paste it</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="building-table-1.html"><a href="building-table-1.html#a-controlled-biological-experiment---the-blood-brain-barrier"><i class="fa fa-check"></i><b>1.8</b> A Controlled Biological Experiment - The Blood-Brain Barrier</a></li>
<li class="chapter" data-level="1.9" data-path="building-table-1.html"><a href="building-table-1.html#the-bloodbrain.csv-file"><i class="fa fa-check"></i><b>1.9</b> The <code>bloodbrain.csv</code> file</a></li>
<li class="chapter" data-level="1.10" data-path="building-table-1.html"><a href="building-table-1.html#a-table-1-for-bloodbrain"><i class="fa fa-check"></i><b>1.10</b> A Table 1 for <code>bloodbrain</code></a><ul>
<li class="chapter" data-level="1.10.1" data-path="building-table-1.html"><a href="building-table-1.html#generate-final-table-1-for-bloodbrain"><i class="fa fa-check"></i><b>1.10.1</b> Generate final Table 1 for <code>bloodbrain</code></a></li>
<li class="chapter" data-level="1.10.2" data-path="building-table-1.html"><a href="building-table-1.html#a-more-finished-version-after-cleanup-in-word"><i class="fa fa-check"></i><b>1.10.2</b> A More Finished Version (after Cleanup in Word)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html"><i class="fa fa-check"></i><b>2</b> Linear Regression on a small SMART data set</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#brfss-and-smart"><i class="fa fa-check"></i><b>2.1</b> BRFSS and SMART</a><ul>
<li class="chapter" data-level="2.1.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#key-resources"><i class="fa fa-check"></i><b>2.1.1</b> Key resources</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-smartcle1-data-cookbook"><i class="fa fa-check"></i><b>2.2</b> The <code>smartcle1</code> data: Cookbook</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#smartcle2-omitting-missing-observations-complete-case-analyses"><i class="fa fa-check"></i><b>2.3</b> <code>smartcle2</code>: Omitting Missing Observations: Complete-Case Analyses</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#summarizing-the-smartcle2-data-numerically"><i class="fa fa-check"></i><b>2.4</b> Summarizing the <code>smartcle2</code> data numerically</a><ul>
<li class="chapter" data-level="2.4.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-new-toy-the-skim-function"><i class="fa fa-check"></i><b>2.4.1</b> The New Toy: The <code>skim</code> function</a></li>
<li class="chapter" data-level="2.4.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-usual-summary-for-a-data-frame"><i class="fa fa-check"></i><b>2.4.2</b> The usual <code>summary</code> for a data frame</a></li>
<li class="chapter" data-level="2.4.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-describe-function-in-hmisc"><i class="fa fa-check"></i><b>2.4.3</b> The <code>describe</code> function in <code>Hmisc</code></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#counting-as-exploratory-data-analysis"><i class="fa fa-check"></i><b>2.5</b> Counting as exploratory data analysis</a><ul>
<li class="chapter" data-level="2.5.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-many-respondents-had-exercised-in-the-past-30-days-did-this-vary-by-sex"><i class="fa fa-check"></i><b>2.5.1</b> How many respondents had exercised in the past 30 days? Did this vary by sex?</a></li>
<li class="chapter" data-level="2.5.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#whats-the-distribution-of-sleephrs"><i class="fa fa-check"></i><b>2.5.2</b> What’s the distribution of <code>sleephrs</code>?</a></li>
<li class="chapter" data-level="2.5.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#whats-the-distribution-of-bmi"><i class="fa fa-check"></i><b>2.5.3</b> What’s the distribution of <code>BMI</code>?</a></li>
<li class="chapter" data-level="2.5.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-many-of-the-respondents-have-a-bmi-below-30"><i class="fa fa-check"></i><b>2.5.4</b> How many of the respondents have a BMI below 30?</a></li>
<li class="chapter" data-level="2.5.5" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-many-of-the-respondents-who-have-a-bmi-30-exercised"><i class="fa fa-check"></i><b>2.5.5</b> How many of the respondents who have a BMI &lt; 30 exercised?</a></li>
<li class="chapter" data-level="2.5.6" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#is-obesity-associated-with-sex-in-these-data"><i class="fa fa-check"></i><b>2.5.6</b> Is obesity associated with sex, in these data?</a></li>
<li class="chapter" data-level="2.5.7" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#comparing-sleephrs-summaries-by-obesity-status"><i class="fa fa-check"></i><b>2.5.7</b> Comparing <code>sleephrs</code> summaries by obesity status</a></li>
<li class="chapter" data-level="2.5.8" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-skim-function-within-a-pipe"><i class="fa fa-check"></i><b>2.5.8</b> The <code>skim</code> function within a pipe</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#first-modeling-attempt-can-bmi-predict-physhealth"><i class="fa fa-check"></i><b>2.6</b> First Modeling Attempt: Can <code>bmi</code> predict <code>physhealth</code>?</a><ul>
<li class="chapter" data-level="2.6.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#fitting-a-simple-regression-model"><i class="fa fa-check"></i><b>2.6.1</b> Fitting a Simple Regression Model</a></li>
<li class="chapter" data-level="2.6.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#model-summary-for-a-simple-one-predictor-regression"><i class="fa fa-check"></i><b>2.6.2</b> Model Summary for a Simple (One-Predictor) Regression</a></li>
<li class="chapter" data-level="2.6.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#using-the-broom-package"><i class="fa fa-check"></i><b>2.6.3</b> Using the <code>broom</code> package</a></li>
<li class="chapter" data-level="2.6.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-does-the-model-do-residuals-vs.fitted-values"><i class="fa fa-check"></i><b>2.6.4</b> How does the model do? (Residuals vs. Fitted Values)</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#a-new-small-study-predicting-bmi"><i class="fa fa-check"></i><b>2.7</b> A New Small Study: Predicting BMI</a><ul>
<li class="chapter" data-level="2.7.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#does-female-predict-bmi-well"><i class="fa fa-check"></i><b>2.7.1</b> Does <code>female</code> predict <code>bmi</code> well?</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m1-a-simple-t-test-model"><i class="fa fa-check"></i><b>2.8</b> <code>c2_m1</code>: A simple t-test model</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m2-adding-another-predictor-two-way-anova-without-interaction"><i class="fa fa-check"></i><b>2.9</b> <code>c2_m2</code>: Adding another predictor (two-way ANOVA without interaction)</a></li>
<li class="chapter" data-level="2.10" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m3-adding-the-interaction-term-two-way-anova-with-interaction"><i class="fa fa-check"></i><b>2.10</b> <code>c2_m3</code>: Adding the interaction term (Two-way ANOVA with interaction)</a></li>
<li class="chapter" data-level="2.11" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m4-using-female-and-sleephrs-in-a-model-for-bmi"><i class="fa fa-check"></i><b>2.11</b> <code>c2_m4</code>: Using <code>female</code> and <code>sleephrs</code> in a model for <code>bmi</code></a></li>
<li class="chapter" data-level="2.12" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#making-predictions-with-a-linear-regression-model"><i class="fa fa-check"></i><b>2.12</b> Making Predictions with a Linear Regression Model</a><ul>
<li class="chapter" data-level="2.12.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#fitting-an-individual-prediction-and-95-prediction-interval"><i class="fa fa-check"></i><b>2.12.1</b> Fitting an Individual Prediction and 95% Prediction Interval</a></li>
<li class="chapter" data-level="2.12.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#confidence-interval-for-an-average-prediction"><i class="fa fa-check"></i><b>2.12.2</b> Confidence Interval for an Average Prediction</a></li>
<li class="chapter" data-level="2.12.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#fitting-multiple-individual-predictions-to-new-data"><i class="fa fa-check"></i><b>2.12.3</b> Fitting Multiple Individual Predictions to New Data</a></li>
<li class="chapter" data-level="2.12.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#simulation-to-represent-predictive-uncertainty-in-model-4"><i class="fa fa-check"></i><b>2.12.4</b> Simulation to represent predictive uncertainty in Model 4</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#centering-the-model"><i class="fa fa-check"></i><b>2.13</b> Centering the model</a><ul>
<li class="chapter" data-level="2.13.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#plot-of-model-4-on-centered-sleephrs-c2_m4_c"><i class="fa fa-check"></i><b>2.13.1</b> Plot of Model 4 on Centered <code>sleephrs</code>: <code>c2_m4_c</code></a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#rescaling-an-input-by-subtracting-the-mean-and-dividing-by-2-standard-deviations"><i class="fa fa-check"></i><b>2.14</b> Rescaling an input by subtracting the mean and dividing by 2 standard deviations</a><ul>
<li class="chapter" data-level="2.14.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#refitting-model-c2_m4-to-the-rescaled-data"><i class="fa fa-check"></i><b>2.14.1</b> Refitting model <code>c2_m4</code> to the rescaled data</a></li>
<li class="chapter" data-level="2.14.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#interpreting-the-model-on-rescaled-data"><i class="fa fa-check"></i><b>2.14.2</b> Interpreting the model on rescaled data</a></li>
<li class="chapter" data-level="2.14.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#plot-of-model-on-rescaled-data"><i class="fa fa-check"></i><b>2.14.3</b> Plot of model on rescaled data</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m5-what-if-we-add-more-variables"><i class="fa fa-check"></i><b>2.15</b> <code>c2_m5</code>: What if we add more variables?</a></li>
<li class="chapter" data-level="2.16" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m6-would-adding-self-reported-health-help"><i class="fa fa-check"></i><b>2.16</b> <code>c2_m6</code>: Would adding self-reported health help?</a></li>
<li class="chapter" data-level="2.17" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m7-what-if-we-added-the-menthealth-variable"><i class="fa fa-check"></i><b>2.17</b> <code>c2_m7</code>: What if we added the <code>menthealth</code> variable?</a></li>
<li class="chapter" data-level="2.18" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#key-regression-assumptions-for-building-effective-prediction-models"><i class="fa fa-check"></i><b>2.18</b> Key Regression Assumptions for Building Effective Prediction Models</a><ul>
<li class="chapter" data-level="2.18.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#checking-assumptions-in-model-c2_m7"><i class="fa fa-check"></i><b>2.18.1</b> Checking Assumptions in model <code>c2_m7</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html"><i class="fa fa-check"></i><b>3</b> Analysis of Variance</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-bonding-data-a-designed-dental-experiment"><i class="fa fa-check"></i><b>3.1</b> The <code>bonding</code> data: A Designed Dental Experiment</a></li>
<li class="chapter" data-level="3.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-one-factor-analysis-of-variance"><i class="fa fa-check"></i><b>3.2</b> A One-Factor Analysis of Variance</a><ul>
<li class="chapter" data-level="3.2.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#look-at-the-data"><i class="fa fa-check"></i><b>3.2.1</b> Look at the Data!</a></li>
<li class="chapter" data-level="3.2.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#table-of-summary-statistics"><i class="fa fa-check"></i><b>3.2.2</b> Table of Summary Statistics</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-two-way-anova-looking-at-two-factors"><i class="fa fa-check"></i><b>3.3</b> A Two-Way ANOVA: Looking at Two Factors</a></li>
<li class="chapter" data-level="3.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-means-plot-with-standard-deviations-to-check-for-interaction"><i class="fa fa-check"></i><b>3.4</b> A Means Plot (with standard deviations) to check for interaction</a><ul>
<li class="chapter" data-level="3.4.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#skimming-the-data-after-grouping-by-resin-and-light"><i class="fa fa-check"></i><b>3.4.1</b> Skimming the data after grouping by <code>resin</code> and <code>light</code></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#fitting-the-two-way-anova-model-with-interaction"><i class="fa fa-check"></i><b>3.5</b> Fitting the Two-Way ANOVA model with Interaction</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-anova-table-for-our-model"><i class="fa fa-check"></i><b>3.5.1</b> The ANOVA table for our model</a></li>
<li class="chapter" data-level="3.5.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#is-the-interaction-important"><i class="fa fa-check"></i><b>3.5.2</b> Is the interaction important?</a></li>
<li class="chapter" data-level="3.5.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#interpreting-the-interaction"><i class="fa fa-check"></i><b>3.5.3</b> Interpreting the Interaction</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#comparing-individual-combinations-of-resin-and-light"><i class="fa fa-check"></i><b>3.6</b> Comparing Individual Combinations of <code>resin</code> and <code>light</code></a></li>
<li class="chapter" data-level="3.7" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-bonding-model-without-interaction"><i class="fa fa-check"></i><b>3.7</b> The <code>bonding</code> model without Interaction</a></li>
<li class="chapter" data-level="3.8" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#cortisol-a-hypothetical-clinical-trial"><i class="fa fa-check"></i><b>3.8</b> <code>cortisol</code>: A Hypothetical Clinical Trial</a><ul>
<li class="chapter" data-level="3.8.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#codebook-and-raw-data-for-cortisol"><i class="fa fa-check"></i><b>3.8.1</b> Codebook and Raw Data for <code>cortisol</code></a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#creating-a-factor-combining-sex-and-waist"><i class="fa fa-check"></i><b>3.9</b> Creating a factor combining sex and waist</a></li>
<li class="chapter" data-level="3.10" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-means-plot-for-the-cortisol-trial-with-standard-errors"><i class="fa fa-check"></i><b>3.10</b> A Means Plot for the <code>cortisol</code> trial (with standard errors)</a></li>
<li class="chapter" data-level="3.11" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-two-way-anova-model-for-cortisol-with-interaction"><i class="fa fa-check"></i><b>3.11</b> A Two-Way ANOVA model for <code>cortisol</code> with Interaction</a></li>
<li class="chapter" data-level="3.12" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-two-way-anova-model-for-cortisol-without-interaction"><i class="fa fa-check"></i><b>3.12</b> A Two-Way ANOVA model for <code>cortisol</code> without Interaction</a><ul>
<li class="chapter" data-level="3.12.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-graph"><i class="fa fa-check"></i><b>3.12.1</b> The Graph</a></li>
<li class="chapter" data-level="3.12.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-anova-model"><i class="fa fa-check"></i><b>3.12.2</b> The ANOVA Model</a></li>
<li class="chapter" data-level="3.12.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-regression-summary"><i class="fa fa-check"></i><b>3.12.3</b> The Regression Summary</a></li>
<li class="chapter" data-level="3.12.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#tukey-hsd-comparisons"><i class="fa fa-check"></i><b>3.12.4</b> Tukey HSD Comparisons</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html"><i class="fa fa-check"></i><b>4</b> Analysis of Covariance</a><ul>
<li class="chapter" data-level="4.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#an-emphysema-study"><i class="fa fa-check"></i><b>4.1</b> An Emphysema Study</a><ul>
<li class="chapter" data-level="4.1.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#codebook"><i class="fa fa-check"></i><b>4.1.1</b> Codebook</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#does-sex-affect-the-mean-change-in-theophylline"><i class="fa fa-check"></i><b>4.2</b> Does <code>sex</code> affect the mean change in theophylline?</a></li>
<li class="chapter" data-level="4.3" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#is-there-an-association-between-age-and-sex-in-this-study"><i class="fa fa-check"></i><b>4.3</b> Is there an association between <code>age</code> and <code>sex</code> in this study?</a></li>
<li class="chapter" data-level="4.4" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#adding-a-quantitative-covariate-age-to-the-model"><i class="fa fa-check"></i><b>4.4</b> Adding a quantitative covariate, <code>age</code>, to the model</a><ul>
<li class="chapter" data-level="4.4.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#the-ancova-model"><i class="fa fa-check"></i><b>4.4.1</b> The ANCOVA model</a></li>
<li class="chapter" data-level="4.4.2" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#the-ancova-table"><i class="fa fa-check"></i><b>4.4.2</b> The ANCOVA Table</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#rerunning-the-ancova-model-after-simple-imputation"><i class="fa fa-check"></i><b>4.5</b> Rerunning the ANCOVA model after simple imputation</a></li>
<li class="chapter" data-level="4.6" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#looking-at-a-factor-covariate-interaction"><i class="fa fa-check"></i><b>4.6</b> Looking at a factor-covariate interaction</a></li>
<li class="chapter" data-level="4.7" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#centering-the-covariate-to-facilitate-ancova-interpretation"><i class="fa fa-check"></i><b>4.7</b> Centering the Covariate to Facilitate ANCOVA Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html"><i class="fa fa-check"></i><b>5</b> Missing Data Mechanisms and Single Imputation</a><ul>
<li class="chapter" data-level="5.1" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#a-toy-example"><i class="fa fa-check"></i><b>5.1</b> A Toy Example</a><ul>
<li class="chapter" data-level="5.1.1" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#how-many-missing-values-do-we-have-in-each-column"><i class="fa fa-check"></i><b>5.1.1</b> How many missing values do we have in each column?</a></li>
<li class="chapter" data-level="5.1.2" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#what-is-the-pattern-of-missing-data"><i class="fa fa-check"></i><b>5.1.2</b> What is the pattern of missing data?</a></li>
<li class="chapter" data-level="5.1.3" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#how-can-we-identify-the-subjects-with-missing-data"><i class="fa fa-check"></i><b>5.1.3</b> How can we identify the subjects with missing data?</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#missing-data-mechanisms"><i class="fa fa-check"></i><b>5.2</b> Missing-data mechanisms</a></li>
<li class="chapter" data-level="5.3" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#options-for-dealing-with-missingness"><i class="fa fa-check"></i><b>5.3</b> Options for Dealing with Missingness</a></li>
<li class="chapter" data-level="5.4" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#complete-case-and-available-case-analyses"><i class="fa fa-check"></i><b>5.4</b> Complete Case (and Available Case) analyses</a></li>
<li class="chapter" data-level="5.5" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#single-imputation"><i class="fa fa-check"></i><b>5.5</b> Single Imputation</a></li>
<li class="chapter" data-level="5.6" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#multiple-imputation"><i class="fa fa-check"></i><b>5.6</b> Multiple Imputation</a></li>
<li class="chapter" data-level="5.7" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#building-a-complete-case-analysis"><i class="fa fa-check"></i><b>5.7</b> Building a Complete Case Analysis</a></li>
<li class="chapter" data-level="5.8" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#single-imputation-with-the-mean-or-mode"><i class="fa fa-check"></i><b>5.8</b> Single Imputation with the Mean or Mode</a></li>
<li class="chapter" data-level="5.9" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#doing-single-imputation-with-simputation"><i class="fa fa-check"></i><b>5.9</b> Doing Single Imputation with <code>simputation</code></a><ul>
<li class="chapter" data-level="5.9.1" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#mirroring-our-prior-approach-imputing-meansmediansmodes"><i class="fa fa-check"></i><b>5.9.1</b> Mirroring Our Prior Approach (imputing means/medians/modes)</a></li>
<li class="chapter" data-level="5.9.2" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#using-a-model-to-impute-sbp.before-and-diabetes"><i class="fa fa-check"></i><b>5.9.2</b> Using a model to impute <code>sbp.before</code> and <code>diabetes</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html"><i class="fa fa-check"></i><b>6</b> A Study of Prostate Cancer</a><ul>
<li class="chapter" data-level="6.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#data-load-and-background"><i class="fa fa-check"></i><b>6.1</b> Data Load and Background</a></li>
<li class="chapter" data-level="6.2" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#code-book"><i class="fa fa-check"></i><b>6.2</b> Code Book</a></li>
<li class="chapter" data-level="6.3" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#additions-for-later-use"><i class="fa fa-check"></i><b>6.3</b> Additions for Later Use</a></li>
<li class="chapter" data-level="6.4" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#fitting-and-evaluating-a-two-predictor-model"><i class="fa fa-check"></i><b>6.4</b> Fitting and Evaluating a Two-Predictor Model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#using-tidy"><i class="fa fa-check"></i><b>6.4.1</b> Using <code>tidy</code></a></li>
<li class="chapter" data-level="6.4.2" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#interpretation"><i class="fa fa-check"></i><b>6.4.2</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#exploring-model-c5_prost_a"><i class="fa fa-check"></i><b>6.5</b> Exploring Model <code>c5_prost_A</code></a><ul>
<li class="chapter" data-level="6.5.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#summary-for-model-c5_prost_a"><i class="fa fa-check"></i><b>6.5.1</b> <code>summary</code> for Model <code>c5_prost_A</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#adjusted-r2"><i class="fa fa-check"></i><b>6.5.2</b> Adjusted R<sup>2</sup></a></li>
<li class="chapter" data-level="6.5.3" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#coefficient-confidence-intervals"><i class="fa fa-check"></i><b>6.5.3</b> Coefficient Confidence Intervals</a></li>
<li class="chapter" data-level="6.5.4" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#anova-for-model-c5_prost_a"><i class="fa fa-check"></i><b>6.5.4</b> ANOVA for Model <code>c5_prost_A</code></a></li>
<li class="chapter" data-level="6.5.5" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#residuals-fitted-values-and-standard-errors-with-augment"><i class="fa fa-check"></i><b>6.5.5</b> Residuals, Fitted Values and Standard Errors with <code>augment</code></a></li>
<li class="chapter" data-level="6.5.6" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#making-predictions-with-c5_prost_a"><i class="fa fa-check"></i><b>6.5.6</b> Making Predictions with <code>c5_prost_A</code></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#plotting-model-c5_prost_a"><i class="fa fa-check"></i><b>6.6</b> Plotting Model <code>c5_prost_A</code></a><ul>
<li class="chapter" data-level="6.6.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#residual-plots-of-c5_prost_a"><i class="fa fa-check"></i><b>6.6.1</b> Residual Plots of <code>c5_prost_A</code></a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#cross-validation-of-model-c5_prost_a"><i class="fa fa-check"></i><b>6.7</b> Cross-Validation of Model <code>c5_prost_A</code></a><ul>
<li class="chapter" data-level="6.7.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#cross-validated-summaries-of-prediction-quality"><i class="fa fa-check"></i><b>6.7.1</b> Cross-Validated Summaries of Prediction Quality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html"><i class="fa fa-check"></i><b>7</b> Stepwise Variable Selection</a><ul>
<li class="chapter" data-level="7.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#strategy-for-model-selection"><i class="fa fa-check"></i><b>7.1</b> Strategy for Model Selection</a><ul>
<li class="chapter" data-level="7.1.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#how-do-we-choose-potential-subsets-of-predictors"><i class="fa fa-check"></i><b>7.1.1</b> How Do We Choose Potential Subsets of Predictors?</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#a-kitchen-sink-model-model-c5_prost_ks"><i class="fa fa-check"></i><b>7.2</b> A “Kitchen Sink” Model (Model <code>c5_prost_ks</code>)</a></li>
<li class="chapter" data-level="7.3" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#sequential-variable-selection-stepwise-approaches"><i class="fa fa-check"></i><b>7.3</b> Sequential Variable Selection: Stepwise Approaches</a><ul>
<li class="chapter" data-level="7.3.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#the-big-problems-with-stepwise-regression"><i class="fa fa-check"></i><b>7.3.1</b> The Big Problems with Stepwise Regression</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#forward-selection-with-the-step-function"><i class="fa fa-check"></i><b>7.4</b> Forward Selection with the <code>step</code> function</a></li>
<li class="chapter" data-level="7.5" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#backward-elimination-using-the-step-function"><i class="fa fa-check"></i><b>7.5</b> Backward Elimination using the <code>step</code> function</a></li>
<li class="chapter" data-level="7.6" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#allen-cady-modified-backward-elimination"><i class="fa fa-check"></i><b>7.6</b> Allen-Cady Modified Backward Elimination</a><ul>
<li class="chapter" data-level="7.6.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#demonstration-of-the-allen-cady-approach"><i class="fa fa-check"></i><b>7.6.1</b> Demonstration of the Allen-Cady approach</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#summarizing-the-results"><i class="fa fa-check"></i><b>7.7</b> Summarizing the Results</a><ul>
<li class="chapter" data-level="7.7.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#in-sample-testing-and-summaries"><i class="fa fa-check"></i><b>7.7.1</b> In-Sample Testing and Summaries</a></li>
<li class="chapter" data-level="7.7.2" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#validating-the-results-of-the-various-models"><i class="fa fa-check"></i><b>7.7.2</b> Validating the Results of the Various Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><i class="fa fa-check"></i><b>8</b> “Best Subsets” Variable Selection in our Prostate Cancer Study</a><ul>
<li class="chapter" data-level="8.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#four-key-summaries-well-use-to-evaluate-potential-models"><i class="fa fa-check"></i><b>8.1</b> Four Key Summaries We’ll Use to Evaluate Potential Models</a></li>
<li class="chapter" data-level="8.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#using-regsubsets-in-the-leaps-package"><i class="fa fa-check"></i><b>8.2</b> Using <code>regsubsets</code> in the <code>leaps</code> package</a><ul>
<li class="chapter" data-level="8.2.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#using-which-and-obj"><i class="fa fa-check"></i><b>8.2.1</b> Using <code>which</code> and <code>obj</code></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#calculating-bias-corrected-aic"><i class="fa fa-check"></i><b>8.3</b> Calculating bias-corrected AIC</a><ul>
<li class="chapter" data-level="8.3.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#calculation-of-aic.c-in-our-setting"><i class="fa fa-check"></i><b>8.3.1</b> Calculation of aic.c in our setting</a></li>
<li class="chapter" data-level="8.3.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-uncorrected-aic-provides-no-more-useful-information-here"><i class="fa fa-check"></i><b>8.3.2</b> The Uncorrected AIC provides no more useful information here</a></li>
<li class="chapter" data-level="8.3.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#building-a-tibble-containing-the-necessary-information"><i class="fa fa-check"></i><b>8.3.3</b> Building a Tibble containing the necessary information</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#plotting-the-best-subsets-results-using-ggplot2"><i class="fa fa-check"></i><b>8.4</b> Plotting the Best Subsets Results using <code>ggplot2</code></a><ul>
<li class="chapter" data-level="8.4.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-adjusted-r2-plot"><i class="fa fa-check"></i><b>8.4.1</b> The Adjusted R<sup>2</sup> Plot</a></li>
<li class="chapter" data-level="8.4.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#mallows-c_p"><i class="fa fa-check"></i><b>8.4.2</b> Mallows’ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="8.4.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-c_p-plot"><i class="fa fa-check"></i><b>8.4.3</b> The <span class="math inline">\(C_p\)</span> Plot</a></li>
<li class="chapter" data-level="8.4.4" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#all-subsets-regression-and-information-criteria"><i class="fa fa-check"></i><b>8.4.4</b> “All Subsets” Regression and Information Criteria</a></li>
<li class="chapter" data-level="8.4.5" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-bias-corrected-aic-plot"><i class="fa fa-check"></i><b>8.4.5</b> The bias-corrected AIC plot</a></li>
<li class="chapter" data-level="8.4.6" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-bic-plot"><i class="fa fa-check"></i><b>8.4.6</b> The BIC plot</a></li>
<li class="chapter" data-level="8.4.7" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#all-four-plots-in-one-figure-via-ggplot2"><i class="fa fa-check"></i><b>8.4.7</b> All Four Plots in One Figure (via ggplot2)</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#plotting-the-best-subsets-results-using-base-r-plots"><i class="fa fa-check"></i><b>8.5</b> Plotting the Best Subsets Results using base R plots</a><ul>
<li class="chapter" data-level="8.5.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-adjusted-r2-plot-1"><i class="fa fa-check"></i><b>8.5.1</b> The Adjusted R<sup>2</sup> Plot</a></li>
<li class="chapter" data-level="8.5.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#a-fancier-version-identifying-the-largest-adjusted-r2"><i class="fa fa-check"></i><b>8.5.2</b> A Fancier Version (identifying the largest adjusted R<sup>2</sup>)</a></li>
<li class="chapter" data-level="8.5.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-c_p-plot-1"><i class="fa fa-check"></i><b>8.5.3</b> The <span class="math inline">\(C_p\)</span> Plot</a></li>
<li class="chapter" data-level="8.5.4" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-bic-plot-1"><i class="fa fa-check"></i><b>8.5.4</b> The BIC Plot</a></li>
<li class="chapter" data-level="8.5.5" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-bias-corrected-aic-hurwitz-tsai"><i class="fa fa-check"></i><b>8.5.5</b> The Bias-Corrected AIC (Hurwitz &amp; Tsai)</a></li>
<li class="chapter" data-level="8.5.6" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#all-four-plots-together-from-base-r"><i class="fa fa-check"></i><b>8.5.6</b> All Four Plots, Together (from Base R)</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#table-of-key-results"><i class="fa fa-check"></i><b>8.6</b> Table of Key Results</a></li>
<li class="chapter" data-level="8.7" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#models-worth-considering"><i class="fa fa-check"></i><b>8.7</b> Models Worth Considering?</a></li>
<li class="chapter" data-level="8.8" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#compare-these-candidate-models-in-sample"><i class="fa fa-check"></i><b>8.8</b> Compare these candidate models in-sample?</a><ul>
<li class="chapter" data-level="8.8.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#using-anova-to-compare-nested-models"><i class="fa fa-check"></i><b>8.8.1</b> Using <code>anova</code> to compare nested models</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#aic-and-bic-comparisons-within-the-training-sample"><i class="fa fa-check"></i><b>8.9</b> AIC and BIC comparisons, within the training sample</a></li>
<li class="chapter" data-level="8.10" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#cross-validation-of-candidate-models-out-of-sample"><i class="fa fa-check"></i><b>8.10</b> Cross-Validation of Candidate Models out of Sample</a><ul>
<li class="chapter" data-level="8.10.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#fold-cross-validation-of-model-m04"><i class="fa fa-check"></i><b>8.10.1</b> 20-fold Cross-Validation of model <code>m04</code></a></li>
<li class="chapter" data-level="8.10.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#fold-cross-validation-of-model-m07"><i class="fa fa-check"></i><b>8.10.2</b> 20-fold Cross-Validation of model <code>m07</code></a></li>
<li class="chapter" data-level="8.10.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#fold-cross-validation-of-model-m08"><i class="fa fa-check"></i><b>8.10.3</b> 20-fold Cross-Validation of model <code>m08</code></a></li>
<li class="chapter" data-level="8.10.4" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#comparing-the-results-of-the-cross-validations"><i class="fa fa-check"></i><b>8.10.4</b> Comparing the Results of the Cross-Validations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science for Biological, Medical and Health Research: Notes for 432</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="best-subsets-variable-selection-in-our-prostate-cancer-study" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> “Best Subsets” Variable Selection in our Prostate Cancer Study</h1>
<p>A second approach to model selection involved fitting all possible subset models and identifying the ones that look best according to some meaningful criterion and ideally one that includes enough variables to model the response appropriately without including lots of redundant or unnecessary terms.</p>
<div id="four-key-summaries-well-use-to-evaluate-potential-models" class="section level2">
<h2><span class="header-section-number">8.1</span> Four Key Summaries We’ll Use to Evaluate Potential Models</h2>
<ol style="list-style-type: decimal">
<li>Adjusted R<sup>2</sup>, which we try to maximize.</li>
<li>Akaike’s Information Criterion (AIC), which we try to minimize, and a Bias-Corrected version of AIC due to Hurwitz and Tsai, which we use when the sample size is small, specifically when the sample size <span class="math inline">\(n\)</span> and the number of predictors being studied <span class="math inline">\(k\)</span> are such that <span class="math inline">\(n/k \leq 40\)</span>. We also try to minimize this bias-corrected AIC.</li>
<li>Bayesian Information Criterion (BIC), which we also try to minimize.</li>
<li>Mallows’ C<sub>p</sub> statistic, which we (essentially) try to minimize.</li>
</ol>
<p>Choosing between AIC and BIC can be challenging.</p>
<blockquote>
<p>For model selection purposes, there is no clear choice between AIC and BIC. Given a family of models, including the true model, the probability that BIC will select the correct model approaches one as the sample size n approaches infinity - thus BIC is asymptotically consistent, which AIC is not. [But, for practical purposes,] BIC often chooses models that are too simple [relative to AIC] because of its heavy penalty on complexity.</p>
</blockquote>
<ul>
<li>Source: <span class="citation">Hastie, Tibshriani, and Frideman (<a href="#ref-Hastie2001">2001</a>)</span>, page 208.</li>
</ul>
<p>Several useful tools for running “all subsets” or “best subsets” regression comparisons are developed in R’s <code>leaps</code> package.</p>
</div>
<div id="using-regsubsets-in-the-leaps-package" class="section level2">
<h2><span class="header-section-number">8.2</span> Using <code>regsubsets</code> in the <code>leaps</code> package</h2>
<p>We can use the <code>leaps</code> package to obtain results in the <code>prost</code> study from looking at all possible subsets of the candidate predictors.</p>
<p>The <code>leaps</code> package isn’t particularly friendly to the tidyverse, and will require us first to identify a set of candidate predictors using <code>with</code> and <code>cbind</code>, then apply those to a <code>regsubsets</code> function, which identifies the set of models.</p>
<p>To start, we’ll ask R to find the one best subset (with 1 predictor variable [in addition to the intercept], then with 2 predictors, and then with each of 3, 4, … 8 predictor variables) according to an exhaustive search without forcing any of the variables to be in or out. We’d use the <code>nvmax</code> command within the <code>regsubsets</code> function to limit the number of regression inputs to a maximum.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">preds &lt;-<span class="st"> </span><span class="kw">with</span>(prost, 
   <span class="kw">cbind</span>(lcavol, lweight, age, bph_f, svi_f, lcp, gleason_f, pgg45))

x1 &lt;-<span class="st"> </span><span class="kw">regsubsets</span>(preds, <span class="dt">y=</span>prost<span class="op">$</span>lpsa)
rs &lt;-<span class="st"> </span><span class="kw">summary</span>(x1)
rs</code></pre></div>
<pre><code>Subset selection object
8 Variables  (and intercept)
          Forced in Forced out
lcavol        FALSE      FALSE
lweight       FALSE      FALSE
age           FALSE      FALSE
bph_f         FALSE      FALSE
svi_f         FALSE      FALSE
lcp           FALSE      FALSE
gleason_f     FALSE      FALSE
pgg45         FALSE      FALSE
1 subsets of each size up to 8
Selection Algorithm: exhaustive
         lcavol lweight age bph_f svi_f lcp gleason_f pgg45
1  ( 1 ) &quot;*&quot;    &quot; &quot;     &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot; &quot; &quot;       &quot; &quot;  
2  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot; &quot; &quot;       &quot; &quot;  
3  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot; &quot; &quot; &quot;   &quot;*&quot;   &quot; &quot; &quot; &quot;       &quot; &quot;  
4  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot; &quot; &quot;*&quot;   &quot;*&quot;   &quot; &quot; &quot; &quot;       &quot; &quot;  
5  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot; &quot; &quot; &quot;       &quot; &quot;  
6  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot; &quot; &quot;*&quot;       &quot; &quot;  
7  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot; &quot;*&quot;       &quot; &quot;  
8  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot; &quot;*&quot;       &quot;*&quot;  </code></pre>
<p>So…</p>
<ul>
<li>the best one-predictor model used <code>lcavol</code></li>
<li>the best two-predictor model used <code>lcavol</code> and <code>lweight</code></li>
<li>the best three-predictor model used <code>lcavol</code>, <code>lweight</code> and <code>svi_f</code></li>
<li>the best four-predictor model added <code>bph_f</code>, and</li>
<li>the best five-predictor model added <code>age</code></li>
<li>the best six-input model added <code>gleason_f</code>,</li>
<li>the best seven-input model added <code>lcp</code>,</li>
<li>and the eight-input model adds <code>pgg45</code>.</li>
</ul>
<p>All of these “best subsets” are hierarchical, in that each model is a subset of the one below it. This isn’t inevitably true.</p>
<ul>
<li>To determine which model is best, we can plot key summaries of model fit (adjusted R<sup>2</sup>, Mallows’ <span class="math inline">\(C_p\)</span>, bias-corrected AIC, and BIC) using either base R plotting techniques (what I’ve done in the past) or <code>ggplot2</code> (what I use now.) I’ll show both types of plotting approaches in the next two sections.</li>
</ul>
<div id="using-which-and-obj" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Using <code>which</code> and <code>obj</code></h3>
<p>Let’s see what we’ve fit:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rs<span class="op">$</span>obj</code></pre></div>
<pre><code>Subset selection object
8 Variables  (and intercept)
          Forced in Forced out
lcavol        FALSE      FALSE
lweight       FALSE      FALSE
age           FALSE      FALSE
bph_f         FALSE      FALSE
svi_f         FALSE      FALSE
lcp           FALSE      FALSE
gleason_f     FALSE      FALSE
pgg45         FALSE      FALSE
1 subsets of each size up to 8
Selection Algorithm: exhaustive</code></pre>
<p>To see the eight models selected by the system, we use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rs<span class="op">$</span>which</code></pre></div>
<pre><code>  (Intercept) lcavol lweight   age bph_f svi_f   lcp gleason_f pgg45
1        TRUE   TRUE   FALSE FALSE FALSE FALSE FALSE     FALSE FALSE
2        TRUE   TRUE    TRUE FALSE FALSE FALSE FALSE     FALSE FALSE
3        TRUE   TRUE    TRUE FALSE FALSE  TRUE FALSE     FALSE FALSE
4        TRUE   TRUE    TRUE FALSE  TRUE  TRUE FALSE     FALSE FALSE
5        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE FALSE     FALSE FALSE
6        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE FALSE      TRUE FALSE
7        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE  TRUE      TRUE FALSE
8        TRUE   TRUE    TRUE  TRUE  TRUE  TRUE  TRUE      TRUE  TRUE</code></pre>
<p>We built one subset of each size up to eight predictors, not including the intercept. This means we have models of size k = 2, 3, 4, 5, 6, 7, 8 and 9.</p>
<p>The models are:</p>
<table>
<thead>
<tr class="header">
<th align="right">Size k</th>
<th>Predictors included (besides intercept)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
<td>lcavol</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td>lcavol and lweight</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td>add svi_f</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td>add bph_f</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td>add age</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td>add gleason_f</td>
</tr>
<tr class="odd">
<td align="right">8</td>
<td>add lcp</td>
</tr>
<tr class="even">
<td align="right">9</td>
<td>add pgg45</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="calculating-bias-corrected-aic" class="section level2">
<h2><span class="header-section-number">8.3</span> Calculating bias-corrected AIC</h2>
<p>The bias-corrected AIC formula due to Hurwitz and Tsai requires three inputs:</p>
<ul>
<li>the residual sum of squares for a model</li>
<li>the sample size (n) or number of observations used to fit the model</li>
<li>the number of regression inputs, k, including the intercept, used in the model</li>
</ul>
<p>So, for a particular model fit to <em>n</em> observations, on <em>k</em> predictors (including the intercept) and a residual sum of squares equal to RSS, we have:</p>
<p><span class="math display">\[
AIC_c = n log(\frac{RSS}{n}) + 2k + \frac{2k (k+1)}{n-k-1}
\]</span></p>
<p>Note that the corrected <span class="math inline">\(AIC_c\)</span> can be related to the original AIC via:</p>
<p><span class="math display">\[
AIC_c = AIC + \frac{2k (k+1)}{n - k - 1}
\]</span></p>
<div id="calculation-of-aic.c-in-our-setting" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Calculation of aic.c in our setting</h3>
<p>In our case, we have <span class="math inline">\(n\)</span> = 97 observations, and built a series of models with <span class="math inline">\(k\)</span> = <code>2:9</code> predictors (including the intercept in each case), so we will insert those values into the general formula for bias-corrected AIC which is:</p>
<pre><code>aic.c &lt;- n * log( rs$rss / n) + 2 * k + 
                      (2 * k * (k + 1) / (n - k - 1))</code></pre>
<p>We can obtain the residual sum of squares explained by each model by pulling <code>rss</code> from the <code>regsubsets</code> summary contained here in <code>rs</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">k =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">9</span>, <span class="dt">RSS =</span> rs<span class="op">$</span>rss)</code></pre></div>
<pre><code># A tibble: 8 x 2
      k   RSS
  &lt;int&gt; &lt;dbl&gt;
1     2  58.9
2     3  51.7
3     4  46.6
4     5  45.7
5     6  44.6
6     7  43.7
7     8  43.0
8     9  42.8</code></pre>
<p>In this case, we have:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rs<span class="op">$</span>aic.c &lt;-<span class="st"> </span><span class="dv">97</span><span class="op">*</span><span class="kw">log</span>(rs<span class="op">$</span>rss <span class="op">/</span><span class="st"> </span><span class="dv">97</span>) <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>) <span class="op">+</span>
<span class="st">               </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>) <span class="op">*</span><span class="st"> </span>((<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>)<span class="op">+</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>(<span class="dv">97</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))

<span class="kw">round</span>(rs<span class="op">$</span>aic.c,<span class="dv">2</span>) <span class="co"># bias-corrected</span></code></pre></div>
<pre><code>[1] -44.24 -54.70 -62.74 -62.29 -62.34 -62.11 -61.17 -59.36</code></pre>
<p>The impact of this bias correction can be modest but important. Here’s a little table looking closely at the results in this problem. The uncorrected AIC are obtained using <code>extractAIC</code>, as described in the next section.</p>
<table>
<thead>
<tr class="header">
<th align="right">Size</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
<th align="right">7</th>
<th align="right">8</th>
<th align="right">9</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Bias-corrected AIC</td>
<td align="right">-44.2</td>
<td align="right">-54.7</td>
<td align="right">-62.7</td>
<td align="right">-62.3</td>
<td align="right">-62.3</td>
<td align="right">-62.1</td>
<td align="right">-61.2</td>
<td align="right">-59.4</td>
</tr>
<tr class="even">
<td align="right">Uncorrected AIC</td>
<td align="right">-44.4</td>
<td align="right">-55.0</td>
<td align="right">-63.2</td>
<td align="right">-62.4</td>
<td align="right">-63.4</td>
<td align="right">-63.0</td>
<td align="right">-62.4</td>
<td align="right">-61.4</td>
</tr>
</tbody>
</table>
</div>
<div id="the-uncorrected-aic-provides-no-more-useful-information-here" class="section level3">
<h3><span class="header-section-number">8.3.2</span> The Uncorrected AIC provides no more useful information here</h3>
<p>We could, if necessary, also calculate the <em>uncorrected</em> <code>aic</code> value for each model, but we won’t make any direct use of that, because that will not provide any new information not already gathered by the <span class="math inline">\(C_p\)</span> statistic for a linear regression model. If you wanted to find the uncorrected AIC for a given model, you can use the <code>extractAIC</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">extractAIC</span>(<span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol, <span class="dt">data =</span> prost))</code></pre></div>
<pre><code>[1]   2.00000 -44.36603</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">extractAIC</span>(<span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol <span class="op">+</span><span class="st"> </span>lweight, <span class="dt">data =</span> prost))</code></pre></div>
<pre><code>[1]   3.00000 -54.95846</code></pre>
<p>Note that:</p>
<ul>
<li>these results are fairly comparable to the bias-corrected AIC we built above, and</li>
<li>the <code>extractAIC</code> and <code>AIC</code> functions look like they give very different results, but they really don’t.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">AIC</span>(<span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol, <span class="dt">data =</span> prost))</code></pre></div>
<pre><code>[1] 232.908</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">AIC</span>(<span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol <span class="op">+</span><span class="st"> </span>lweight, <span class="dt">data =</span> prost))</code></pre></div>
<pre><code>[1] 222.3156</code></pre>
<p>But notice that the differences in AIC are the same, either way, comparing these two models:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">extractAIC</span>(<span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol, <span class="dt">data =</span> prost)) <span class="op">-</span><span class="st"> </span><span class="kw">extractAIC</span>(<span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol <span class="op">+</span><span class="st"> </span>lweight, <span class="dt">data =</span> prost))</code></pre></div>
<pre><code>[1] -1.00000 10.59243</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">AIC</span>(<span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol, <span class="dt">data =</span> prost)) <span class="op">-</span><span class="st"> </span><span class="kw">AIC</span>(<span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol <span class="op">+</span><span class="st"> </span>lweight, <span class="dt">data =</span> prost))</code></pre></div>
<pre><code>[1] 10.59243</code></pre>
<ul>
<li>AIC is only defined up to an additive constant.</li>
<li>Since the difference between two models using either <code>AIC</code> or <code>extractAIC</code> is the same, this doesn’t actually matter which one we use, so long as we use the same one consistently.</li>
</ul>
</div>
<div id="building-a-tibble-containing-the-necessary-information" class="section level3">
<h3><span class="header-section-number">8.3.3</span> Building a Tibble containing the necessary information</h3>
<p>Again, note the use of 2:9 for the values of <span class="math inline">\(k\)</span>, because we’re fitting one model for each size from 2 through 9.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">best_mods_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">data_frame</span>(
    <span class="dt">k =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">9</span>,
    <span class="dt">r2 =</span> rs<span class="op">$</span>rsq,
    <span class="dt">adjr2 =</span> rs<span class="op">$</span>adjr2,
    <span class="dt">cp =</span> rs<span class="op">$</span>cp,
    <span class="dt">aic.c =</span> rs<span class="op">$</span>aic.c,
    <span class="dt">bic =</span> rs<span class="op">$</span>bic
)

best_mods &lt;-<span class="st"> </span><span class="kw">cbind</span>(best_mods_<span class="dv">1</span>, rs<span class="op">$</span>which)

best_mods</code></pre></div>
<pre><code>  k        r2     adjr2        cp     aic.c       bic (Intercept) lcavol
1 2 0.5394320 0.5345839 28.213914 -44.23838 -66.05416        TRUE   TRUE
2 3 0.5955040 0.5868977 15.456669 -54.70040 -74.07188        TRUE   TRUE
3 4 0.6359499 0.6242063  6.811986 -62.74265 -79.71614        TRUE   TRUE
4 5 0.6425479 0.6270065  7.075509 -62.29223 -76.91557        TRUE   TRUE
5 6 0.6509970 0.6318211  6.851826 -62.33858 -74.66120        TRUE   TRUE
6 7 0.6584484 0.6356783  6.890739 -62.10692 -72.17992        TRUE   TRUE
7 8 0.6634967 0.6370302  7.562119 -61.17338 -69.04961        TRUE   TRUE
8 9 0.6656326 0.6352355  9.000000 -59.35841 -65.09253        TRUE   TRUE
  lweight   age bph_f svi_f   lcp gleason_f pgg45
1   FALSE FALSE FALSE FALSE FALSE     FALSE FALSE
2    TRUE FALSE FALSE FALSE FALSE     FALSE FALSE
3    TRUE FALSE FALSE  TRUE FALSE     FALSE FALSE
4    TRUE FALSE  TRUE  TRUE FALSE     FALSE FALSE
5    TRUE  TRUE  TRUE  TRUE FALSE     FALSE FALSE
6    TRUE  TRUE  TRUE  TRUE FALSE      TRUE FALSE
7    TRUE  TRUE  TRUE  TRUE  TRUE      TRUE FALSE
8    TRUE  TRUE  TRUE  TRUE  TRUE      TRUE  TRUE</code></pre>
</div>
</div>
<div id="plotting-the-best-subsets-results-using-ggplot2" class="section level2">
<h2><span class="header-section-number">8.4</span> Plotting the Best Subsets Results using <code>ggplot2</code></h2>
<div id="the-adjusted-r2-plot" class="section level3">
<h3><span class="header-section-number">8.4.1</span> The Adjusted R<sup>2</sup> Plot</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(best_mods, <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> adjr2,
                            <span class="dt">label =</span> <span class="kw">round</span>(adjr2,<span class="dv">2</span>))) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_label</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_label</span>(<span class="dt">data =</span> <span class="kw">subset</span>(best_mods,
                             adjr2 <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(adjr2)),
               <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> adjr2, <span class="dt">label =</span> <span class="kw">round</span>(adjr2,<span class="dv">2</span>)),
               <span class="dt">fill =</span> <span class="st">&quot;yellow&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">9</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;# of predictors (including intercept)&quot;</span>,
         <span class="dt">y =</span> <span class="st">&quot;Adjusted R-squared&quot;</span>)

p1</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-77-1.png" width="672" /></p>
<p>Models 4-9 all look like reasonable choices here. The maximum adjusted R<sup>2</sup> is seen in the model of size 8.</p>
</div>
<div id="mallows-c_p" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Mallows’ <span class="math inline">\(C_p\)</span></h3>
<p>The <span class="math inline">\(C_p\)</span> statistic focuses directly on the tradeoff between <strong>bias</strong> (due to excluding important predictors from the model) and extra <strong>variance</strong> (due to including too many unimportant predictors in the model.)</p>
<p>If N is the sample size, and we select <span class="math inline">\(p\)</span> regression predictors from a set of <span class="math inline">\(K\)</span> (where <span class="math inline">\(p &lt; K\)</span>), then the <span class="math inline">\(C_p\)</span> statistic is</p>
<p><span class="math inline">\(C_p = \frac{SSE_p}{MSE_K} - N + 2p\)</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(SSE_p\)</span> is the sum of squares for error (residual) in the model with <span class="math inline">\(p\)</span> predictors</li>
<li><span class="math inline">\(MSE_K\)</span> is the residual mean square after regression in the model with all <span class="math inline">\(K\)</span> predictors</li>
</ul>
<p>As it turns out, this is just measuring the particular model’s lack of fit, and then adding a penalty for the number of terms in the model (specifically <span class="math inline">\(2p - N\)</span> is the penalty since the lack of fit is measured as <span class="math inline">\((N-p) \frac{SSE_p}{MSE_K}\)</span>.</p>
<ul>
<li>If a model has no meaningful lack of fit (i.e. no substantial bias) then the expected value of <span class="math inline">\(C_p\)</span> is roughly <span class="math inline">\(p\)</span>.</li>
<li>Otherwise, the expectation is <span class="math inline">\(p\)</span> plus a positive bias term.</li>
<li>In general, we want to see <em>smaller</em> values of <span class="math inline">\(C_p\)</span>.</li>
<li>We usually select a “winning model” by choosing a subset of predictors that have <span class="math inline">\(C_p\)</span> near the value of <span class="math inline">\(p\)</span>.</li>
</ul>
</div>
<div id="the-c_p-plot" class="section level3">
<h3><span class="header-section-number">8.4.3</span> The <span class="math inline">\(C_p\)</span> Plot</h3>
<p>The <span class="math inline">\(C_p\)</span> plot is just a scatterplot of <span class="math inline">\(C_p\)</span> on the Y-axis, and <span class="math inline">\(p\)</span> on the X-axis.</p>
<p>Each of the various predictor subsets we will study is represented in a single point. A model without bias should have <span class="math inline">\(C_p\)</span> roughly equal to <span class="math inline">\(p\)</span>, so we’ll frequently draw a line at <span class="math inline">\(C_p = p\)</span> to make that clear. We then select our model from among all models with small <span class="math inline">\(C_p\)</span> statistics.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(best_mods, <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> cp,
                            <span class="dt">label =</span> <span class="kw">round</span>(cp,<span class="dv">1</span>))) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_label</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>,
                <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">9</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;# of predictors (including intercept)&quot;</span>,
         <span class="dt">y =</span> <span class="st">&quot;Mallows&#39; Cp&quot;</span>)

p2</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-78-1.png" width="672" /></p>
<ul>
<li>Model 7 looks pretty good, with C<sub>p</sub> just barely smaller than the size (p = 7) of the model.</li>
<li>Model 6 is also a possibility, with the difference <span class="math inline">\(C_p - p\)</span> minimized among all models with <span class="math inline">\(C_p &gt;= p\)</span>.</li>
</ul>
</div>
<div id="all-subsets-regression-and-information-criteria" class="section level3">
<h3><span class="header-section-number">8.4.4</span> “All Subsets” Regression and Information Criteria</h3>
<p>We might consider any of three main information criteria:</p>
<ul>
<li>the Bayesian Information Criterion, called BIC</li>
<li>the Akaike Information Criterion (used by R’s default stepwise approaches,) called AIC</li>
<li>a corrected version of AIC due to Hurwitz and Tsai, called AIC<sub>c</sub></li>
</ul>
<p>Each of these indicates better models by getting smaller. I’ll focus on bias-corrected AIC and on BIC.</p>
</div>
<div id="the-bias-corrected-aic-plot" class="section level3">
<h3><span class="header-section-number">8.4.5</span> The bias-corrected AIC plot</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p3 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(best_mods, <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> aic.c,
                             <span class="dt">label =</span> <span class="kw">round</span>(aic.c,<span class="dv">1</span>))) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_label</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_label</span>(<span class="dt">data =</span> <span class="kw">subset</span>(best_mods, aic.c <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(aic.c)),
               <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> aic.c), <span class="dt">fill =</span> <span class="st">&quot;pink&quot;</span>, 
               <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">9</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;# of predictors (including intercept)&quot;</span>,
         <span class="dt">y =</span> <span class="st">&quot;Bias-Corrected AIC&quot;</span>)

p3</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-79-1.png" width="672" /></p>
<p>The smallest AIC<sub>c</sub> values occur in models 4 and later, especially model 4 itself.</p>
</div>
<div id="the-bic-plot" class="section level3">
<h3><span class="header-section-number">8.4.6</span> The BIC plot</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p4 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(best_mods, <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> bic,
                            <span class="dt">label =</span> <span class="kw">round</span>(bic,<span class="dv">1</span>))) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_label</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_label</span>(<span class="dt">data =</span> <span class="kw">subset</span>(best_mods, bic <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(bic)),
               <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> bic),
               <span class="dt">fill =</span> <span class="st">&quot;lightgreen&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">9</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;# of predictors (including intercept)&quot;</span>,
         <span class="dt">y =</span> <span class="st">&quot;BIC&quot;</span>)

p4</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-80-1.png" width="672" /></p>
</div>
<div id="all-four-plots-in-one-figure-via-ggplot2" class="section level3">
<h3><span class="header-section-number">8.4.7</span> All Four Plots in One Figure (via ggplot2)</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(p1, p2, p3, p4, <span class="dt">nrow =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-81-1.png" width="672" /></p>
</div>
</div>
<div id="plotting-the-best-subsets-results-using-base-r-plots" class="section level2">
<h2><span class="header-section-number">8.5</span> Plotting the Best Subsets Results using base R plots</h2>
<div id="the-adjusted-r2-plot-1" class="section level3">
<h3><span class="header-section-number">8.5.1</span> The Adjusted R<sup>2</sup> Plot</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(rs<span class="op">$</span>adjr2 <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>), <span class="dt">ylab=</span><span class="st">&quot;Adjusted R-squared&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;# of Inputs, including intercept&quot;</span>)
<span class="kw">lines</span>(<span class="kw">spline</span>(rs<span class="op">$</span>adjr2 <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>)))</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-82-1.png" width="672" /></p>
<p>Again, models 4-9 all look like reasonable choices here.</p>
</div>
<div id="a-fancier-version-identifying-the-largest-adjusted-r2" class="section level3">
<h3><span class="header-section-number">8.5.2</span> A Fancier Version (identifying the largest adjusted R<sup>2</sup>)</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2 &lt;-<span class="st"> </span><span class="kw">max</span>(rs<span class="op">$</span>adjr2) 
m1 &lt;-<span class="st"> </span><span class="kw">which.max</span>(rs<span class="op">$</span>adjr2) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
<span class="kw">plot</span>(rs<span class="op">$</span>adjr2 <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>), <span class="dt">ylab=</span><span class="st">&quot;Adjusted R-squared&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;# of Inputs, including intercept&quot;</span>)
<span class="kw">lines</span>(<span class="kw">spline</span>(rs<span class="op">$</span>adjr2 <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>)))
<span class="kw">arrows</span>(m1, m2<span class="op">-</span><span class="fl">0.02</span>, m1, m2)
<span class="kw">text</span>(m1, m2<span class="op">-</span><span class="fl">0.03</span>, <span class="kw">paste</span>(<span class="st">&quot;max =&quot;</span>, <span class="kw">format</span>(m2, <span class="dt">digits=</span><span class="dv">3</span>)))
<span class="kw">text</span>(m1, m2<span class="op">-</span><span class="fl">0.045</span>, <span class="kw">paste</span>(<span class="st">&quot;with&quot;</span>, <span class="kw">format</span>(m1, <span class="dt">digits=</span><span class="dv">1</span>),
                        <span class="st">&quot;inputs&quot;</span>), <span class="dt">pos=</span><span class="dv">3</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-83-1.png" width="672" /></p>
</div>
<div id="the-c_p-plot-1" class="section level3">
<h3><span class="header-section-number">8.5.3</span> The <span class="math inline">\(C_p\)</span> Plot</h3>
<p>Again, the <span class="math inline">\(C_p\)</span> plot is just a scatterplot of <span class="math inline">\(C_p\)</span> on the Y-axis, and <span class="math inline">\(p\)</span> on the X-axis.</p>
<p>Each of the various predictor subsets we will study is represented in a single point. A model without bias should have <span class="math inline">\(C_p\)</span> roughly equal to <span class="math inline">\(p\)</span>, so we’ll frequently draw a line at <span class="math inline">\(C_p = p\)</span> to make that clear. We then select our model from among all models with small <span class="math inline">\(C_p\)</span> statistics.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(rs<span class="op">$</span>cp <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>),
     <span class="dt">ylab=</span><span class="st">&quot;Cp Statistic&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;# of Regression Inputs, including Intercept&quot;</span>,
     <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">main=</span><span class="st">&quot;Cp Plot&quot;</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dt">col =</span> <span class="st">&quot;purple&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-84-1.png" width="672" /></p>
<p>Model 4 has the smallest value of <span class="math inline">\(C_p\)</span> (and is the leftmost of the largely comparable models 4-9) while 6 is close to and 7 is right on the <span class="math inline">\(C_p = p\)</span> line, so those are the likeliest candidates.</p>
</div>
<div id="the-bic-plot-1" class="section level3">
<h3><span class="header-section-number">8.5.4</span> The BIC Plot</h3>
<p>R provides the BIC directly as part of the result of running <code>regsubsets</code>, as we’ve seen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(rs<span class="op">$</span>bic <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>), <span class="dt">ylab=</span><span class="st">&quot;BIC&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;# of Fitted Inputs&quot;</span>,
     <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">1.5</span>, <span class="dt">col=</span><span class="st">&quot;slateblue&quot;</span>, <span class="dt">main=</span><span class="st">&quot;BIC Plot&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-85-1.png" width="672" /></p>
<p>We want to minimize BIC, which argues strongly for the model with 4 inputs, including the intercept.</p>
</div>
<div id="the-bias-corrected-aic-hurwitz-tsai" class="section level3">
<h3><span class="header-section-number">8.5.5</span> The Bias-Corrected AIC (Hurwitz &amp; Tsai)</h3>
<p>The bias-corrected AIC formula due to Hurwitz and Tsai is, as we’ve seen:</p>
<p><span class="math display">\[
AIC_c = n log(\frac{RSS}{n}) + 2k + \frac{2k (k+1)}{n-k-1}
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rs<span class="op">$</span>aic.c &lt;-<span class="st"> </span><span class="dv">97</span><span class="op">*</span><span class="kw">log</span>(rs<span class="op">$</span>rss <span class="op">/</span><span class="st"> </span><span class="dv">97</span>) <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>) <span class="op">+</span>
<span class="st">               </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>) <span class="op">*</span><span class="st"> </span>((<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>)<span class="op">+</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>(<span class="dv">97</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))

<span class="kw">round</span>(rs<span class="op">$</span>aic,<span class="dv">2</span>) <span class="co"># uncorrected </span></code></pre></div>
<pre><code>[1] -44.24 -54.70 -62.74 -62.29 -62.34 -62.11 -61.17 -59.36</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(rs<span class="op">$</span>aic.c,<span class="dv">2</span>) <span class="co"># bias-corrected</span></code></pre></div>
<pre><code>[1] -44.24 -54.70 -62.74 -62.29 -62.34 -62.11 -61.17 -59.36</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(rs<span class="op">$</span>aic.c <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>), <span class="dt">ylab=</span><span class="st">&quot;AIC, corrected&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;# of Fitted Inputs&quot;</span>,
     <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">1.5</span>, <span class="dt">col=</span><span class="st">&quot;tomato&quot;</span>, <span class="dt">main=</span><span class="st">&quot;AIC (corrected) Plot&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-87-1.png" width="672" /></p>
<p>The smallest AIC<sub>c</sub> values occur in models 4 and later, especially model 4 itself.</p>
</div>
<div id="all-four-plots-together-from-base-r" class="section level3">
<h3><span class="header-section-number">8.5.6</span> All Four Plots, Together (from Base R)</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
m2 &lt;-<span class="st"> </span><span class="kw">max</span>(rs<span class="op">$</span>adjr2) 
m1 &lt;-<span class="st"> </span><span class="kw">which.max</span>(rs<span class="op">$</span>adjr2) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
<span class="kw">plot</span>(rs<span class="op">$</span>adjr2 <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>), <span class="dt">ylab=</span><span class="st">&quot;Adjusted R-squared&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;# of Inputs, including intercept&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Adjusted R-squared&quot;</span>)
<span class="kw">lines</span>(<span class="kw">spline</span>(rs<span class="op">$</span>adjr2 <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>)))
<span class="kw">arrows</span>(m1, m2<span class="op">-</span><span class="fl">0.02</span>, m1, m2)
<span class="kw">text</span>(m1, m2<span class="op">-</span><span class="fl">0.03</span>, <span class="kw">paste</span>(<span class="st">&quot;max =&quot;</span>, <span class="kw">format</span>(m2, <span class="dt">digits=</span><span class="dv">3</span>)))
<span class="kw">text</span>(m1, m2<span class="op">-</span><span class="fl">0.045</span>, <span class="kw">paste</span>(<span class="st">&quot;with&quot;</span>, <span class="kw">format</span>(m1, <span class="dt">digits=</span><span class="dv">1</span>),
                        <span class="st">&quot;inputs&quot;</span>), <span class="dt">pos=</span><span class="dv">3</span>)

<span class="kw">plot</span>(rs<span class="op">$</span>cp <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>),
     <span class="dt">ylab=</span><span class="st">&quot;Cp Statistic&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;# of Regression Inputs, including Intercept&quot;</span>,
     <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">main=</span><span class="st">&quot;Cp Plot&quot;</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dt">col =</span> <span class="st">&quot;purple&quot;</span>)

rs<span class="op">$</span>aic.c &lt;-<span class="st"> </span><span class="dv">97</span><span class="op">*</span><span class="kw">log</span>(rs<span class="op">$</span>rss <span class="op">/</span><span class="st"> </span><span class="dv">97</span>) <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>) <span class="op">+</span>
<span class="st">               </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>) <span class="op">*</span><span class="st"> </span>((<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>)<span class="op">+</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>(<span class="dv">97</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))
<span class="kw">plot</span>(rs<span class="op">$</span>aic.c <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>), <span class="dt">ylab=</span><span class="st">&quot;AIC, corrected&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;# of Fitted Inputs&quot;</span>,
     <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">1.5</span>, <span class="dt">col=</span><span class="st">&quot;tomato&quot;</span>, <span class="dt">main=</span><span class="st">&quot;AIC (corrected) Plot&quot;</span>)

<span class="kw">plot</span>(rs<span class="op">$</span>bic <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">9</span>), <span class="dt">ylab=</span><span class="st">&quot;BIC&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;# of Fitted Inputs&quot;</span>,
     <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">1.5</span>, <span class="dt">col=</span><span class="st">&quot;slateblue&quot;</span>, <span class="dt">main=</span><span class="st">&quot;BIC Plot&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-88-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
</div>
</div>
<div id="table-of-key-results" class="section level2">
<h2><span class="header-section-number">8.6</span> Table of Key Results</h2>
<p>We can build a big table, like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">best_mods</code></pre></div>
<pre><code>  k        r2     adjr2        cp     aic.c       bic (Intercept) lcavol
1 2 0.5394320 0.5345839 28.213914 -44.23838 -66.05416        TRUE   TRUE
2 3 0.5955040 0.5868977 15.456669 -54.70040 -74.07188        TRUE   TRUE
3 4 0.6359499 0.6242063  6.811986 -62.74265 -79.71614        TRUE   TRUE
4 5 0.6425479 0.6270065  7.075509 -62.29223 -76.91557        TRUE   TRUE
5 6 0.6509970 0.6318211  6.851826 -62.33858 -74.66120        TRUE   TRUE
6 7 0.6584484 0.6356783  6.890739 -62.10692 -72.17992        TRUE   TRUE
7 8 0.6634967 0.6370302  7.562119 -61.17338 -69.04961        TRUE   TRUE
8 9 0.6656326 0.6352355  9.000000 -59.35841 -65.09253        TRUE   TRUE
  lweight   age bph_f svi_f   lcp gleason_f pgg45
1   FALSE FALSE FALSE FALSE FALSE     FALSE FALSE
2    TRUE FALSE FALSE FALSE FALSE     FALSE FALSE
3    TRUE FALSE FALSE  TRUE FALSE     FALSE FALSE
4    TRUE FALSE  TRUE  TRUE FALSE     FALSE FALSE
5    TRUE  TRUE  TRUE  TRUE FALSE     FALSE FALSE
6    TRUE  TRUE  TRUE  TRUE FALSE      TRUE FALSE
7    TRUE  TRUE  TRUE  TRUE  TRUE      TRUE FALSE
8    TRUE  TRUE  TRUE  TRUE  TRUE      TRUE  TRUE</code></pre>
</div>
<div id="models-worth-considering" class="section level2">
<h2><span class="header-section-number">8.7</span> Models Worth Considering?</h2>
<table>
<thead>
<tr class="header">
<th align="right"><span class="math inline">\(k\)</span></th>
<th align="right">Predictors</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">4</td>
<td align="right"><code>lcavol lweight svi_f</code></td>
<td>minimizes BIC, AIC<sub>c</sub></td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right"><code>+ age bph_f gleason_f</code></td>
<td><span class="math inline">\(C_p\)</span> near <em>p</em></td>
</tr>
<tr class="odd">
<td align="right">8</td>
<td align="right"><code>+ lcp</code></td>
<td>max <span class="math inline">\(R^2_{adj}\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="compare-these-candidate-models-in-sample" class="section level2">
<h2><span class="header-section-number">8.8</span> Compare these candidate models in-sample?</h2>
<div id="using-anova-to-compare-nested-models" class="section level3">
<h3><span class="header-section-number">8.8.1</span> Using <code>anova</code> to compare nested models</h3>
<p>Let’s run an ANOVA-based comparison of these nested models to each other and to the model with the intercept alone.</p>
<ul>
<li>The models are <strong>nested</strong> because <code>m04</code> is a subset of the predictors in <code>m07</code>, which includes a subset of the predictors in <code>m08</code>.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.int &lt;-<span class="st"> </span><span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> prost)
m04 &lt;-<span class="st"> </span><span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol <span class="op">+</span><span class="st"> </span>lweight <span class="op">+</span><span class="st"> </span>svi_f, <span class="dt">data =</span> prost)
m07 &lt;-<span class="st"> </span><span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol <span class="op">+</span><span class="st"> </span>lweight <span class="op">+</span><span class="st"> </span>svi_f <span class="op">+</span><span class="st"> </span>
<span class="st">              </span>age <span class="op">+</span><span class="st"> </span>bph_f <span class="op">+</span><span class="st"> </span>gleason_f, <span class="dt">data =</span> prost)
m08 &lt;-<span class="st"> </span><span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol <span class="op">+</span><span class="st"> </span>lweight <span class="op">+</span><span class="st"> </span>svi_f <span class="op">+</span><span class="st"> </span>
<span class="st">              </span>age <span class="op">+</span><span class="st"> </span>bph_f <span class="op">+</span><span class="st"> </span>gleason_f <span class="op">+</span><span class="st"> </span>lcp, <span class="dt">data =</span> prost)
m.full &lt;-<span class="st"> </span><span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol <span class="op">+</span><span class="st"> </span>lweight <span class="op">+</span><span class="st"> </span>svi_f <span class="op">+</span><span class="st"> </span>
<span class="st">              </span>age <span class="op">+</span><span class="st"> </span>bph_f <span class="op">+</span><span class="st"> </span>gleason_f <span class="op">+</span><span class="st"> </span>lcp <span class="op">+</span><span class="st"> </span>pgg45, <span class="dt">data =</span> prost)</code></pre></div>
<p>Next, we’ll run…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(m.full, m08, m07, m04, m.int)</code></pre></div>
<pre><code>Analysis of Variance Table

Model 1: lpsa ~ lcavol + lweight + svi_f + age + bph_f + gleason_f + lcp + 
    pgg45
Model 2: lpsa ~ lcavol + lweight + svi_f + age + bph_f + gleason_f + lcp
Model 3: lpsa ~ lcavol + lweight + svi_f + age + bph_f + gleason_f
Model 4: lpsa ~ lcavol + lweight + svi_f
Model 5: lpsa ~ 1
  Res.Df     RSS Df Sum of Sq       F Pr(&gt;F)    
1     86  41.057                                
2     87  41.498 -1    -0.441  0.9234 0.3393    
3     88  42.066 -1    -0.568  1.1891 0.2786    
4     93  46.568 -5    -4.503  1.8863 0.1050    
5     96 127.918 -3   -81.349 56.7991 &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>What conclusions can we draw here, on the basis of these ANOVA tests?</p>
<ul>
<li>The first <em>p</em> value, of 0.3393, compares what the <code>anova</code> called Model 1, and what we call <code>m.full</code> to what the <code>anova</code> called Model 2, and what we call <code>m08</code>. So there’s no significant decline in predictive value observed when we drop from the <code>m.full</code> model to the <code>m08</code> model. This suggests that the <code>m08</code> model may be a better choice.</li>
<li>The second <em>p</em> value, of 0.2786, compares <code>m08</code> to <code>m07</code>, and suggests that we lose no significant predictive value by dropping down to <code>m07</code>.</li>
<li>The third <em>p</em> value, of 0.1050, compares <code>m07</code> to <code>m04</code>, and suggests that we lose no significant predictive value by dropping down to <code>m04</code>.</li>
<li>But the fourth <em>p</em> value, of 2e-16 (or, functionally, zero), compares <code>m04</code> to <code>m.int</code> and suggests that we do gain significant predictive value by including the predictors in <code>m04</code> as compared to a model with an intercept alone.</li>
<li>So, by the significance tests, the model we’d select would be <code>m04</code>, but, of course, in-sample statistical significance alone isn’t a good enough reason to select a model if we want to do prediction well.</li>
</ul>
</div>
</div>
<div id="aic-and-bic-comparisons-within-the-training-sample" class="section level2">
<h2><span class="header-section-number">8.9</span> AIC and BIC comparisons, within the training sample</h2>
<p>Next, we’ll compare the three candidate models (ignoring the intercept-only and kitchen sink models) in terms of their AIC values and BIC values, again using the same sample we used to fit the models in the first place.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">AIC</span>(m04, m07, m08)</code></pre></div>
<pre><code>    df      AIC
m04  5 214.0966
m07 10 214.2327
m08 11 214.9148</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">BIC</span>(m04, m07, m08)</code></pre></div>
<pre><code>    df      BIC
m04  5 226.9702
m07 10 239.9798
m08 11 243.2366</code></pre>
<ul>
<li>The model with the smallest AIC value shows the best performance within the sample on that measure.</li>
<li>Similarly, smaller BIC values are associated with predictor sets that perform better in sample on that criterion.</li>
<li>BIC often suggests smaller models (with fewer regression inputs) than does AIC. Does that happen in this case?</li>
<li>Note that <code>AIC</code> and <code>BIC</code> can be calculated in a few different ways, so we may see some variation if we don’t compare apples to apples with regard to the R functions involved.</li>
</ul>
</div>
<div id="cross-validation-of-candidate-models-out-of-sample" class="section level2">
<h2><span class="header-section-number">8.10</span> Cross-Validation of Candidate Models out of Sample</h2>
<div id="fold-cross-validation-of-model-m04" class="section level3">
<h3><span class="header-section-number">8.10.1</span> 20-fold Cross-Validation of model <code>m04</code></h3>
<p>Model <code>m04</code> uses <code>lcavol</code>, <code>lweight</code> and <code>svi_f</code> to predict the <code>lpsa</code> outcome. Let’s do 20-fold cross-validation of this modeling approach, and calculate the root mean squared prediction error and the mean absolute prediction error for that modeling scheme.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">43201</span>)

cv_m04 &lt;-<span class="st"> </span>prost <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">crossv_kfold</span>(<span class="dt">k =</span> <span class="dv">20</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(train, 
                       <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol <span class="op">+</span><span class="st"> </span>lweight <span class="op">+</span><span class="st"> </span>svi_f,
                                   <span class="dt">data =</span> .)))

cv_m04_pred &lt;-<span class="st"> </span>cv_m04 <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">unnest</span>(<span class="kw">map2</span>(model, test, <span class="op">~</span><span class="st"> </span><span class="kw">augment</span>(.x, <span class="dt">newdata =</span> .y)))

cv_m04_results &lt;-<span class="st"> </span>cv_m04_pred <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">Model =</span> <span class="st">&quot;m04&quot;</span>, 
              <span class="dt">RMSE =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>((lpsa <span class="op">-</span><span class="st"> </span>.fitted) <span class="op">^</span><span class="dv">2</span>)),
              <span class="dt">MAE =</span> <span class="kw">mean</span>(<span class="kw">abs</span>(lpsa <span class="op">-</span><span class="st"> </span>.fitted)))

cv_m04_results</code></pre></div>
<pre><code># A tibble: 1 x 3
  Model  RMSE   MAE
  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;
1 m04   0.725 0.574</code></pre>
</div>
<div id="fold-cross-validation-of-model-m07" class="section level3">
<h3><span class="header-section-number">8.10.2</span> 20-fold Cross-Validation of model <code>m07</code></h3>
<p>Model <code>m07</code> uses <code>lcavol</code>, <code>lweight</code>, <code>svi_f</code>, <code>age</code>, <code>bph_f</code>, and <code>gleason_f</code> to predict the <code>lpsa</code> outcome. Let’s now do 20-fold cross-validation of this modeling approach, and calculate the root mean squared prediction error and the mean absolute prediction error for that modeling scheme. Note the small changes required, as compared to our cross-validation of model <code>m04</code> a moment ago.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">43202</span>)

cv_m07 &lt;-<span class="st"> </span>prost <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">crossv_kfold</span>(<span class="dt">k =</span> <span class="dv">20</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(train, 
                       <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol <span class="op">+</span><span class="st"> </span>lweight <span class="op">+</span><span class="st"> </span>
<span class="st">                                </span>svi_f <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>bph_f <span class="op">+</span><span class="st"> </span>
<span class="st">                                </span>gleason_f,
                                   <span class="dt">data =</span> .)))

cv_m07_pred &lt;-<span class="st"> </span>cv_m07 <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">unnest</span>(<span class="kw">map2</span>(model, test, <span class="op">~</span><span class="st"> </span><span class="kw">augment</span>(.x, <span class="dt">newdata =</span> .y)))

cv_m07_results &lt;-<span class="st"> </span>cv_m07_pred <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">Model =</span> <span class="st">&quot;m07&quot;</span>, 
              <span class="dt">RMSE =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>((lpsa <span class="op">-</span><span class="st"> </span>.fitted) <span class="op">^</span><span class="dv">2</span>)),
              <span class="dt">MAE =</span> <span class="kw">mean</span>(<span class="kw">abs</span>(lpsa <span class="op">-</span><span class="st"> </span>.fitted)))

cv_m07_results</code></pre></div>
<pre><code># A tibble: 1 x 3
  Model  RMSE   MAE
  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;
1 m07   0.730 0.556</code></pre>
</div>
<div id="fold-cross-validation-of-model-m08" class="section level3">
<h3><span class="header-section-number">8.10.3</span> 20-fold Cross-Validation of model <code>m08</code></h3>
<p>Model <code>m08</code> uses <code>lcavol</code>, <code>lweight</code>, <code>svi_f</code>, <code>age</code>, <code>bph_f</code>, <code>gleason_f</code> and <code>lcp</code> to predict the <code>lpsa</code> outcome. Let’s now do 20-fold cross-validation of this modeling approach.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">43202</span>)

cv_m08 &lt;-<span class="st"> </span>prost <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">crossv_kfold</span>(<span class="dt">k =</span> <span class="dv">20</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(train, 
                       <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>lcavol <span class="op">+</span><span class="st"> </span>lweight <span class="op">+</span><span class="st"> </span>
<span class="st">                                </span>svi_f <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>bph_f <span class="op">+</span><span class="st"> </span>
<span class="st">                                </span>gleason_f <span class="op">+</span><span class="st"> </span>lcp,
                                   <span class="dt">data =</span> .)))

cv_m08_pred &lt;-<span class="st"> </span>cv_m08 <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">unnest</span>(<span class="kw">map2</span>(model, test, <span class="op">~</span><span class="st"> </span><span class="kw">augment</span>(.x, <span class="dt">newdata =</span> .y)))

cv_m08_results &lt;-<span class="st"> </span>cv_m08_pred <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">Model =</span> <span class="st">&quot;m08&quot;</span>, 
              <span class="dt">RMSE =</span> <span class="kw">sqrt</span>(<span class="kw">mean</span>((lpsa <span class="op">-</span><span class="st"> </span>.fitted) <span class="op">^</span><span class="dv">2</span>)),
              <span class="dt">MAE =</span> <span class="kw">mean</span>(<span class="kw">abs</span>(lpsa <span class="op">-</span><span class="st"> </span>.fitted)))

cv_m08_results</code></pre></div>
<pre><code># A tibble: 1 x 3
  Model  RMSE   MAE
  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;
1 m08   0.729 0.557</code></pre>
</div>
<div id="comparing-the-results-of-the-cross-validations" class="section level3">
<h3><span class="header-section-number">8.10.4</span> Comparing the Results of the Cross-Validations</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">bind_rows</span>(cv_m04_results, cv_m07_results, cv_m08_results)</code></pre></div>
<pre><code># A tibble: 3 x 3
  Model  RMSE   MAE
  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;
1 m04   0.725 0.574
2 m07   0.730 0.556
3 m08   0.729 0.557</code></pre>
<p>It appears that model <code>m04</code> has the smallest RMSE and MAE in this case. So, that’s the model with the strongest cross-validated predictive accuracy, by these two standards.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Hastie2001">
<p>Hastie, Trevor, Robert Tibshriani, and Jerome H. Frideman. 2001. <em>The Elements of Statistical Learning</em>. New York: Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="stepwise-variable-selection.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08_bestsubsets.Rmd",
"text": "Edit"
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
