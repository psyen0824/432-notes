<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Science for Biological, Medical and Health Research: Notes for 432</title>
  <meta name="description" content="These are the Course Notes for 432.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Data Science for Biological, Medical and Health Research: Notes for 432" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the Course Notes for 432." />
  <meta name="github-repo" content="thomaselove/432-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Science for Biological, Medical and Health Research: Notes for 432" />
  
  <meta name="twitter:description" content="These are the Course Notes for 432." />
  

<meta name="author" content="Thomas E. Love, Ph.D.">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="cleaning-the-brfss-smart-data.html">
<link rel="next" href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">432 Course Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="r-packages-used-in-these-notes.html"><a href="r-packages-used-in-these-notes.html"><i class="fa fa-check"></i>R Packages used in these notes</a></li>
<li class="chapter" data-level="" data-path="data-used-in-these-notes.html"><a href="data-used-in-these-notes.html"><i class="fa fa-check"></i>Data used in these notes</a></li>
<li class="chapter" data-level="" data-path="special-functions-used-in-these-notes.html"><a href="special-functions-used-in-these-notes.html"><i class="fa fa-check"></i>Special Functions used in these notes</a></li>
<li class="chapter" data-level="1" data-path="building-table-1.html"><a href="building-table-1.html"><i class="fa fa-check"></i><b>1</b> Building Table 1</a><ul>
<li class="chapter" data-level="1.1" data-path="building-table-1.html"><a href="building-table-1.html#two-examples-from-the-new-england-journal-of-medicine"><i class="fa fa-check"></i><b>1.1</b> Two examples from the <em>New England Journal of Medicine</em></a><ul>
<li class="chapter" data-level="1.1.1" data-path="building-table-1.html"><a href="building-table-1.html#a-simple-table-1"><i class="fa fa-check"></i><b>1.1.1</b> A simple Table 1</a></li>
<li class="chapter" data-level="1.1.2" data-path="building-table-1.html"><a href="building-table-1.html#a-group-comparison"><i class="fa fa-check"></i><b>1.1.2</b> A group comparison</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="building-table-1.html"><a href="building-table-1.html#the-mr-clean-trial"><i class="fa fa-check"></i><b>1.2</b> The MR CLEAN trial</a></li>
<li class="chapter" data-level="1.3" data-path="building-table-1.html"><a href="building-table-1.html#simulated-fakestroke-data"><i class="fa fa-check"></i><b>1.3</b> Simulated <code>fakestroke</code> data</a></li>
<li class="chapter" data-level="1.4" data-path="building-table-1.html"><a href="building-table-1.html#building-table-1-for-fakestroke-attempt-1"><i class="fa fa-check"></i><b>1.4</b> Building Table 1 for <code>fakestroke</code>: Attempt 1</a><ul>
<li class="chapter" data-level="1.4.1" data-path="building-table-1.html"><a href="building-table-1.html#some-of-this-is-very-useful-and-other-parts-need-to-be-fixed."><i class="fa fa-check"></i><b>1.4.1</b> Some of this is very useful, and other parts need to be fixed.</a></li>
<li class="chapter" data-level="1.4.2" data-path="building-table-1.html"><a href="building-table-1.html#fakestroke-cleaning-up-categorical-variables"><i class="fa fa-check"></i><b>1.4.2</b> <code>fakestroke</code> Cleaning Up Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="building-table-1.html"><a href="building-table-1.html#fakestroke-table-1-attempt-2"><i class="fa fa-check"></i><b>1.5</b> <code>fakestroke</code> Table 1: Attempt 2</a><ul>
<li class="chapter" data-level="1.5.1" data-path="building-table-1.html"><a href="building-table-1.html#what-summaries-should-we-show"><i class="fa fa-check"></i><b>1.5.1</b> What summaries should we show?</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="building-table-1.html"><a href="building-table-1.html#obtaining-a-more-detailed-summary"><i class="fa fa-check"></i><b>1.6</b> Obtaining a more detailed Summary</a></li>
<li class="chapter" data-level="1.7" data-path="building-table-1.html"><a href="building-table-1.html#exporting-the-completed-table-1-from-r-to-excel-or-word"><i class="fa fa-check"></i><b>1.7</b> Exporting the Completed Table 1 from R to Excel or Word</a><ul>
<li class="chapter" data-level="1.7.1" data-path="building-table-1.html"><a href="building-table-1.html#approach-a-save-and-open-in-excel"><i class="fa fa-check"></i><b>1.7.1</b> Approach A: Save and open in Excel</a></li>
<li class="chapter" data-level="1.7.2" data-path="building-table-1.html"><a href="building-table-1.html#approach-b-produce-the-table-so-you-can-cut-and-paste-it"><i class="fa fa-check"></i><b>1.7.2</b> Approach B: Produce the Table so you can cut and paste it</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="building-table-1.html"><a href="building-table-1.html#a-controlled-biological-experiment---the-blood-brain-barrier"><i class="fa fa-check"></i><b>1.8</b> A Controlled Biological Experiment - The Blood-Brain Barrier</a></li>
<li class="chapter" data-level="1.9" data-path="building-table-1.html"><a href="building-table-1.html#the-bloodbrain.csv-file"><i class="fa fa-check"></i><b>1.9</b> The <code>bloodbrain.csv</code> file</a></li>
<li class="chapter" data-level="1.10" data-path="building-table-1.html"><a href="building-table-1.html#a-table-1-for-bloodbrain"><i class="fa fa-check"></i><b>1.10</b> A Table 1 for <code>bloodbrain</code></a><ul>
<li class="chapter" data-level="1.10.1" data-path="building-table-1.html"><a href="building-table-1.html#generate-final-table-1-for-bloodbrain"><i class="fa fa-check"></i><b>1.10.1</b> Generate final Table 1 for <code>bloodbrain</code></a></li>
<li class="chapter" data-level="1.10.2" data-path="building-table-1.html"><a href="building-table-1.html#a-more-finished-version-after-cleanup-in-word"><i class="fa fa-check"></i><b>1.10.2</b> A More Finished Version (after Cleanup in Word)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html"><i class="fa fa-check"></i><b>2</b> Linear Regression on a small SMART data set</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#brfss-and-smart"><i class="fa fa-check"></i><b>2.1</b> BRFSS and SMART</a><ul>
<li class="chapter" data-level="2.1.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#key-resources"><i class="fa fa-check"></i><b>2.1.1</b> Key resources</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-smartcle1-data-cookbook"><i class="fa fa-check"></i><b>2.2</b> The <code>smartcle1</code> data: Cookbook</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#smartcle2-omitting-missing-observations-complete-case-analyses"><i class="fa fa-check"></i><b>2.3</b> <code>smartcle2</code>: Omitting Missing Observations: Complete-Case Analyses</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#summarizing-the-smartcle2-data-numerically"><i class="fa fa-check"></i><b>2.4</b> Summarizing the <code>smartcle2</code> data numerically</a><ul>
<li class="chapter" data-level="2.4.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-new-toy-the-skim-function"><i class="fa fa-check"></i><b>2.4.1</b> The New Toy: The <code>skim</code> function</a></li>
<li class="chapter" data-level="2.4.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-usual-summary-for-a-data-frame"><i class="fa fa-check"></i><b>2.4.2</b> The usual <code>summary</code> for a data frame</a></li>
<li class="chapter" data-level="2.4.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-describe-function-in-hmisc"><i class="fa fa-check"></i><b>2.4.3</b> The <code>describe</code> function in <code>Hmisc</code></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#counting-as-exploratory-data-analysis"><i class="fa fa-check"></i><b>2.5</b> Counting as exploratory data analysis</a><ul>
<li class="chapter" data-level="2.5.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-many-respondents-had-exercised-in-the-past-30-days-did-this-vary-by-sex"><i class="fa fa-check"></i><b>2.5.1</b> How many respondents had exercised in the past 30 days? Did this vary by sex?</a></li>
<li class="chapter" data-level="2.5.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#whats-the-distribution-of-sleephrs"><i class="fa fa-check"></i><b>2.5.2</b> What’s the distribution of <code>sleephrs</code>?</a></li>
<li class="chapter" data-level="2.5.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#whats-the-distribution-of-bmi"><i class="fa fa-check"></i><b>2.5.3</b> What’s the distribution of <code>BMI</code>?</a></li>
<li class="chapter" data-level="2.5.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-many-of-the-respondents-have-a-bmi-below-30"><i class="fa fa-check"></i><b>2.5.4</b> How many of the respondents have a BMI below 30?</a></li>
<li class="chapter" data-level="2.5.5" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-many-of-the-respondents-who-have-a-bmi-30-exercised"><i class="fa fa-check"></i><b>2.5.5</b> How many of the respondents who have a BMI &lt; 30 exercised?</a></li>
<li class="chapter" data-level="2.5.6" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#is-obesity-associated-with-sex-in-these-data"><i class="fa fa-check"></i><b>2.5.6</b> Is obesity associated with sex, in these data?</a></li>
<li class="chapter" data-level="2.5.7" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#comparing-sleephrs-summaries-by-obesity-status"><i class="fa fa-check"></i><b>2.5.7</b> Comparing <code>sleephrs</code> summaries by obesity status</a></li>
<li class="chapter" data-level="2.5.8" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-skim-function-within-a-pipe"><i class="fa fa-check"></i><b>2.5.8</b> The <code>skim</code> function within a pipe</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#first-modeling-attempt-can-bmi-predict-physhealth"><i class="fa fa-check"></i><b>2.6</b> First Modeling Attempt: Can <code>bmi</code> predict <code>physhealth</code>?</a><ul>
<li class="chapter" data-level="2.6.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#fitting-a-simple-regression-model"><i class="fa fa-check"></i><b>2.6.1</b> Fitting a Simple Regression Model</a></li>
<li class="chapter" data-level="2.6.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#model-summary-for-a-simple-one-predictor-regression"><i class="fa fa-check"></i><b>2.6.2</b> Model Summary for a Simple (One-Predictor) Regression</a></li>
<li class="chapter" data-level="2.6.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#using-the-broom-package"><i class="fa fa-check"></i><b>2.6.3</b> Using the <code>broom</code> package</a></li>
<li class="chapter" data-level="2.6.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-does-the-model-do-residuals-vs.fitted-values"><i class="fa fa-check"></i><b>2.6.4</b> How does the model do? (Residuals vs. Fitted Values)</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#a-new-small-study-predicting-bmi"><i class="fa fa-check"></i><b>2.7</b> A New Small Study: Predicting BMI</a><ul>
<li class="chapter" data-level="2.7.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#does-female-predict-bmi-well"><i class="fa fa-check"></i><b>2.7.1</b> Does <code>female</code> predict <code>bmi</code> well?</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m1-a-simple-t-test-model"><i class="fa fa-check"></i><b>2.8</b> <code>c2_m1</code>: A simple t-test model</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m2-adding-another-predictor-two-way-anova-without-interaction"><i class="fa fa-check"></i><b>2.9</b> <code>c2_m2</code>: Adding another predictor (two-way ANOVA without interaction)</a></li>
<li class="chapter" data-level="2.10" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m3-adding-the-interaction-term-two-way-anova-with-interaction"><i class="fa fa-check"></i><b>2.10</b> <code>c2_m3</code>: Adding the interaction term (Two-way ANOVA with interaction)</a></li>
<li class="chapter" data-level="2.11" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m4-using-female-and-sleephrs-in-a-model-for-bmi"><i class="fa fa-check"></i><b>2.11</b> <code>c2_m4</code>: Using <code>female</code> and <code>sleephrs</code> in a model for <code>bmi</code></a></li>
<li class="chapter" data-level="2.12" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#making-predictions-with-a-linear-regression-model"><i class="fa fa-check"></i><b>2.12</b> Making Predictions with a Linear Regression Model</a><ul>
<li class="chapter" data-level="2.12.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#fitting-an-individual-prediction-and-95-prediction-interval"><i class="fa fa-check"></i><b>2.12.1</b> Fitting an Individual Prediction and 95% Prediction Interval</a></li>
<li class="chapter" data-level="2.12.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#confidence-interval-for-an-average-prediction"><i class="fa fa-check"></i><b>2.12.2</b> Confidence Interval for an Average Prediction</a></li>
<li class="chapter" data-level="2.12.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#fitting-multiple-individual-predictions-to-new-data"><i class="fa fa-check"></i><b>2.12.3</b> Fitting Multiple Individual Predictions to New Data</a></li>
<li class="chapter" data-level="2.12.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#simulation-to-represent-predictive-uncertainty-in-model-4"><i class="fa fa-check"></i><b>2.12.4</b> Simulation to represent predictive uncertainty in Model 4</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#centering-the-model"><i class="fa fa-check"></i><b>2.13</b> Centering the model</a><ul>
<li class="chapter" data-level="2.13.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#plot-of-model-4-on-centered-sleephrs-c2_m4_c"><i class="fa fa-check"></i><b>2.13.1</b> Plot of Model 4 on Centered <code>sleephrs</code>: <code>c2_m4_c</code></a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#rescaling-an-input-by-subtracting-the-mean-and-dividing-by-2-standard-deviations"><i class="fa fa-check"></i><b>2.14</b> Rescaling an input by subtracting the mean and dividing by 2 standard deviations</a><ul>
<li class="chapter" data-level="2.14.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#refitting-model-c2_m4-to-the-rescaled-data"><i class="fa fa-check"></i><b>2.14.1</b> Refitting model <code>c2_m4</code> to the rescaled data</a></li>
<li class="chapter" data-level="2.14.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#interpreting-the-model-on-rescaled-data"><i class="fa fa-check"></i><b>2.14.2</b> Interpreting the model on rescaled data</a></li>
<li class="chapter" data-level="2.14.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#plot-of-model-on-rescaled-data"><i class="fa fa-check"></i><b>2.14.3</b> Plot of model on rescaled data</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m5-what-if-we-add-more-variables"><i class="fa fa-check"></i><b>2.15</b> <code>c2_m5</code>: What if we add more variables?</a></li>
<li class="chapter" data-level="2.16" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m6-would-adding-self-reported-health-help"><i class="fa fa-check"></i><b>2.16</b> <code>c2_m6</code>: Would adding self-reported health help?</a></li>
<li class="chapter" data-level="2.17" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m7-what-if-we-added-the-menthealth-variable"><i class="fa fa-check"></i><b>2.17</b> <code>c2_m7</code>: What if we added the <code>menthealth</code> variable?</a></li>
<li class="chapter" data-level="2.18" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#key-regression-assumptions-for-building-effective-prediction-models"><i class="fa fa-check"></i><b>2.18</b> Key Regression Assumptions for Building Effective Prediction Models</a><ul>
<li class="chapter" data-level="2.18.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#checking-assumptions-in-model-c2_m7"><i class="fa fa-check"></i><b>2.18.1</b> Checking Assumptions in model <code>c2_m7</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html"><i class="fa fa-check"></i><b>3</b> Analysis of Variance</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-bonding-data-a-designed-dental-experiment"><i class="fa fa-check"></i><b>3.1</b> The <code>bonding</code> data: A Designed Dental Experiment</a></li>
<li class="chapter" data-level="3.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-one-factor-analysis-of-variance"><i class="fa fa-check"></i><b>3.2</b> A One-Factor Analysis of Variance</a><ul>
<li class="chapter" data-level="3.2.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#look-at-the-data"><i class="fa fa-check"></i><b>3.2.1</b> Look at the Data!</a></li>
<li class="chapter" data-level="3.2.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#table-of-summary-statistics"><i class="fa fa-check"></i><b>3.2.2</b> Table of Summary Statistics</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-two-way-anova-looking-at-two-factors"><i class="fa fa-check"></i><b>3.3</b> A Two-Way ANOVA: Looking at Two Factors</a></li>
<li class="chapter" data-level="3.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-means-plot-with-standard-deviations-to-check-for-interaction"><i class="fa fa-check"></i><b>3.4</b> A Means Plot (with standard deviations) to check for interaction</a><ul>
<li class="chapter" data-level="3.4.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#skimming-the-data-after-grouping-by-resin-and-light"><i class="fa fa-check"></i><b>3.4.1</b> Skimming the data after grouping by <code>resin</code> and <code>light</code></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#fitting-the-two-way-anova-model-with-interaction"><i class="fa fa-check"></i><b>3.5</b> Fitting the Two-Way ANOVA model with Interaction</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-anova-table-for-our-model"><i class="fa fa-check"></i><b>3.5.1</b> The ANOVA table for our model</a></li>
<li class="chapter" data-level="3.5.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#is-the-interaction-important"><i class="fa fa-check"></i><b>3.5.2</b> Is the interaction important?</a></li>
<li class="chapter" data-level="3.5.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#interpreting-the-interaction"><i class="fa fa-check"></i><b>3.5.3</b> Interpreting the Interaction</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#comparing-individual-combinations-of-resin-and-light"><i class="fa fa-check"></i><b>3.6</b> Comparing Individual Combinations of <code>resin</code> and <code>light</code></a></li>
<li class="chapter" data-level="3.7" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-bonding-model-without-interaction"><i class="fa fa-check"></i><b>3.7</b> The <code>bonding</code> model without Interaction</a></li>
<li class="chapter" data-level="3.8" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#cortisol-a-hypothetical-clinical-trial"><i class="fa fa-check"></i><b>3.8</b> <code>cortisol</code>: A Hypothetical Clinical Trial</a><ul>
<li class="chapter" data-level="3.8.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#codebook-and-raw-data-for-cortisol"><i class="fa fa-check"></i><b>3.8.1</b> Codebook and Raw Data for <code>cortisol</code></a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#creating-a-factor-combining-sex-and-waist"><i class="fa fa-check"></i><b>3.9</b> Creating a factor combining sex and waist</a></li>
<li class="chapter" data-level="3.10" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-means-plot-for-the-cortisol-trial-with-standard-errors"><i class="fa fa-check"></i><b>3.10</b> A Means Plot for the <code>cortisol</code> trial (with standard errors)</a></li>
<li class="chapter" data-level="3.11" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-two-way-anova-model-for-cortisol-with-interaction"><i class="fa fa-check"></i><b>3.11</b> A Two-Way ANOVA model for <code>cortisol</code> with Interaction</a></li>
<li class="chapter" data-level="3.12" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-two-way-anova-model-for-cortisol-without-interaction"><i class="fa fa-check"></i><b>3.12</b> A Two-Way ANOVA model for <code>cortisol</code> without Interaction</a><ul>
<li class="chapter" data-level="3.12.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-graph"><i class="fa fa-check"></i><b>3.12.1</b> The Graph</a></li>
<li class="chapter" data-level="3.12.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-anova-model"><i class="fa fa-check"></i><b>3.12.2</b> The ANOVA Model</a></li>
<li class="chapter" data-level="3.12.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-regression-summary"><i class="fa fa-check"></i><b>3.12.3</b> The Regression Summary</a></li>
<li class="chapter" data-level="3.12.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#tukey-hsd-comparisons"><i class="fa fa-check"></i><b>3.12.4</b> Tukey HSD Comparisons</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html"><i class="fa fa-check"></i><b>4</b> Analysis of Covariance</a><ul>
<li class="chapter" data-level="4.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#an-emphysema-study"><i class="fa fa-check"></i><b>4.1</b> An Emphysema Study</a><ul>
<li class="chapter" data-level="4.1.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#codebook"><i class="fa fa-check"></i><b>4.1.1</b> Codebook</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#does-sex-affect-the-mean-change-in-theophylline"><i class="fa fa-check"></i><b>4.2</b> Does <code>sex</code> affect the mean change in theophylline?</a></li>
<li class="chapter" data-level="4.3" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#is-there-an-association-between-age-and-sex-in-this-study"><i class="fa fa-check"></i><b>4.3</b> Is there an association between <code>age</code> and <code>sex</code> in this study?</a></li>
<li class="chapter" data-level="4.4" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#adding-a-quantitative-covariate-age-to-the-model"><i class="fa fa-check"></i><b>4.4</b> Adding a quantitative covariate, <code>age</code>, to the model</a><ul>
<li class="chapter" data-level="4.4.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#the-ancova-model"><i class="fa fa-check"></i><b>4.4.1</b> The ANCOVA model</a></li>
<li class="chapter" data-level="4.4.2" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#the-ancova-table"><i class="fa fa-check"></i><b>4.4.2</b> The ANCOVA Table</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#rerunning-the-ancova-model-after-simple-imputation"><i class="fa fa-check"></i><b>4.5</b> Rerunning the ANCOVA model after simple imputation</a></li>
<li class="chapter" data-level="4.6" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#looking-at-a-factor-covariate-interaction"><i class="fa fa-check"></i><b>4.6</b> Looking at a factor-covariate interaction</a></li>
<li class="chapter" data-level="4.7" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#centering-the-covariate-to-facilitate-ancova-interpretation"><i class="fa fa-check"></i><b>4.7</b> Centering the Covariate to Facilitate ANCOVA Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html"><i class="fa fa-check"></i><b>5</b> Missing Data Mechanisms and Single Imputation</a><ul>
<li class="chapter" data-level="5.1" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#a-toy-example"><i class="fa fa-check"></i><b>5.1</b> A Toy Example</a><ul>
<li class="chapter" data-level="5.1.1" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#how-many-missing-values-do-we-have-in-each-column"><i class="fa fa-check"></i><b>5.1.1</b> How many missing values do we have in each column?</a></li>
<li class="chapter" data-level="5.1.2" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#what-is-the-pattern-of-missing-data"><i class="fa fa-check"></i><b>5.1.2</b> What is the pattern of missing data?</a></li>
<li class="chapter" data-level="5.1.3" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#how-can-we-identify-the-subjects-with-missing-data"><i class="fa fa-check"></i><b>5.1.3</b> How can we identify the subjects with missing data?</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#missing-data-mechanisms"><i class="fa fa-check"></i><b>5.2</b> Missing-data mechanisms</a></li>
<li class="chapter" data-level="5.3" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#options-for-dealing-with-missingness"><i class="fa fa-check"></i><b>5.3</b> Options for Dealing with Missingness</a></li>
<li class="chapter" data-level="5.4" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#complete-case-and-available-case-analyses"><i class="fa fa-check"></i><b>5.4</b> Complete Case (and Available Case) analyses</a></li>
<li class="chapter" data-level="5.5" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#single-imputation"><i class="fa fa-check"></i><b>5.5</b> Single Imputation</a></li>
<li class="chapter" data-level="5.6" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#multiple-imputation"><i class="fa fa-check"></i><b>5.6</b> Multiple Imputation</a></li>
<li class="chapter" data-level="5.7" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#building-a-complete-case-analysis"><i class="fa fa-check"></i><b>5.7</b> Building a Complete Case Analysis</a></li>
<li class="chapter" data-level="5.8" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#single-imputation-with-the-mean-or-mode"><i class="fa fa-check"></i><b>5.8</b> Single Imputation with the Mean or Mode</a></li>
<li class="chapter" data-level="5.9" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#doing-single-imputation-with-simputation"><i class="fa fa-check"></i><b>5.9</b> Doing Single Imputation with <code>simputation</code></a><ul>
<li class="chapter" data-level="5.9.1" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#mirroring-our-prior-approach-imputing-meansmediansmodes"><i class="fa fa-check"></i><b>5.9.1</b> Mirroring Our Prior Approach (imputing means/medians/modes)</a></li>
<li class="chapter" data-level="5.9.2" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#using-a-model-to-impute-sbp.before-and-diabetes"><i class="fa fa-check"></i><b>5.9.2</b> Using a model to impute <code>sbp.before</code> and <code>diabetes</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html"><i class="fa fa-check"></i><b>6</b> A Study of Prostate Cancer</a><ul>
<li class="chapter" data-level="6.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#data-load-and-background"><i class="fa fa-check"></i><b>6.1</b> Data Load and Background</a></li>
<li class="chapter" data-level="6.2" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#code-book"><i class="fa fa-check"></i><b>6.2</b> Code Book</a></li>
<li class="chapter" data-level="6.3" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#additions-for-later-use"><i class="fa fa-check"></i><b>6.3</b> Additions for Later Use</a></li>
<li class="chapter" data-level="6.4" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#fitting-and-evaluating-a-two-predictor-model"><i class="fa fa-check"></i><b>6.4</b> Fitting and Evaluating a Two-Predictor Model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#using-tidy"><i class="fa fa-check"></i><b>6.4.1</b> Using <code>tidy</code></a></li>
<li class="chapter" data-level="6.4.2" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#interpretation"><i class="fa fa-check"></i><b>6.4.2</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#exploring-model-c5_prost_a"><i class="fa fa-check"></i><b>6.5</b> Exploring Model <code>c5_prost_A</code></a><ul>
<li class="chapter" data-level="6.5.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#summary-for-model-c5_prost_a"><i class="fa fa-check"></i><b>6.5.1</b> <code>summary</code> for Model <code>c5_prost_A</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#adjusted-r2"><i class="fa fa-check"></i><b>6.5.2</b> Adjusted R<sup>2</sup></a></li>
<li class="chapter" data-level="6.5.3" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#coefficient-confidence-intervals"><i class="fa fa-check"></i><b>6.5.3</b> Coefficient Confidence Intervals</a></li>
<li class="chapter" data-level="6.5.4" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#anova-for-model-c5_prost_a"><i class="fa fa-check"></i><b>6.5.4</b> ANOVA for Model <code>c5_prost_A</code></a></li>
<li class="chapter" data-level="6.5.5" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#residuals-fitted-values-and-standard-errors-with-augment"><i class="fa fa-check"></i><b>6.5.5</b> Residuals, Fitted Values and Standard Errors with <code>augment</code></a></li>
<li class="chapter" data-level="6.5.6" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#making-predictions-with-c5_prost_a"><i class="fa fa-check"></i><b>6.5.6</b> Making Predictions with <code>c5_prost_A</code></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#plotting-model-c5_prost_a"><i class="fa fa-check"></i><b>6.6</b> Plotting Model <code>c5_prost_A</code></a><ul>
<li class="chapter" data-level="6.6.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#residual-plots-of-c5_prost_a"><i class="fa fa-check"></i><b>6.6.1</b> Residual Plots of <code>c5_prost_A</code></a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#cross-validation-of-model-c5_prost_a"><i class="fa fa-check"></i><b>6.7</b> Cross-Validation of Model <code>c5_prost_A</code></a><ul>
<li class="chapter" data-level="6.7.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#cross-validated-summaries-of-prediction-quality"><i class="fa fa-check"></i><b>6.7.1</b> Cross-Validated Summaries of Prediction Quality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html"><i class="fa fa-check"></i><b>7</b> Stepwise Variable Selection</a><ul>
<li class="chapter" data-level="7.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#strategy-for-model-selection"><i class="fa fa-check"></i><b>7.1</b> Strategy for Model Selection</a><ul>
<li class="chapter" data-level="7.1.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#how-do-we-choose-potential-subsets-of-predictors"><i class="fa fa-check"></i><b>7.1.1</b> How Do We Choose Potential Subsets of Predictors?</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#a-kitchen-sink-model-model-c5_prost_ks"><i class="fa fa-check"></i><b>7.2</b> A “Kitchen Sink” Model (Model <code>c5_prost_ks</code>)</a></li>
<li class="chapter" data-level="7.3" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#sequential-variable-selection-stepwise-approaches"><i class="fa fa-check"></i><b>7.3</b> Sequential Variable Selection: Stepwise Approaches</a><ul>
<li class="chapter" data-level="7.3.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#the-big-problems-with-stepwise-regression"><i class="fa fa-check"></i><b>7.3.1</b> The Big Problems with Stepwise Regression</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#forward-selection-with-the-step-function"><i class="fa fa-check"></i><b>7.4</b> Forward Selection with the <code>step</code> function</a></li>
<li class="chapter" data-level="7.5" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#backward-elimination-using-the-step-function"><i class="fa fa-check"></i><b>7.5</b> Backward Elimination using the <code>step</code> function</a></li>
<li class="chapter" data-level="7.6" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#allen-cady-modified-backward-elimination"><i class="fa fa-check"></i><b>7.6</b> Allen-Cady Modified Backward Elimination</a><ul>
<li class="chapter" data-level="7.6.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#demonstration-of-the-allen-cady-approach"><i class="fa fa-check"></i><b>7.6.1</b> Demonstration of the Allen-Cady approach</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#summarizing-the-results"><i class="fa fa-check"></i><b>7.7</b> Summarizing the Results</a><ul>
<li class="chapter" data-level="7.7.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#in-sample-testing-and-summaries"><i class="fa fa-check"></i><b>7.7.1</b> In-Sample Testing and Summaries</a></li>
<li class="chapter" data-level="7.7.2" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#validating-the-results-of-the-various-models"><i class="fa fa-check"></i><b>7.7.2</b> Validating the Results of the Various Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><i class="fa fa-check"></i><b>8</b> “Best Subsets” Variable Selection in our Prostate Cancer Study</a><ul>
<li class="chapter" data-level="8.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#four-key-summaries-well-use-to-evaluate-potential-models"><i class="fa fa-check"></i><b>8.1</b> Four Key Summaries We’ll Use to Evaluate Potential Models</a></li>
<li class="chapter" data-level="8.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#using-regsubsets-in-the-leaps-package"><i class="fa fa-check"></i><b>8.2</b> Using <code>regsubsets</code> in the <code>leaps</code> package</a><ul>
<li class="chapter" data-level="8.2.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#identifying-the-models-with-which-and-outmat"><i class="fa fa-check"></i><b>8.2.1</b> Identifying the models with <code>which</code> and <code>outmat</code></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#calculating-bias-corrected-aic"><i class="fa fa-check"></i><b>8.3</b> Calculating bias-corrected AIC</a><ul>
<li class="chapter" data-level="8.3.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#calculation-of-aic.c-in-our-setting"><i class="fa fa-check"></i><b>8.3.1</b> Calculation of aic.c in our setting</a></li>
<li class="chapter" data-level="8.3.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-uncorrected-aic-provides-no-more-useful-information-here"><i class="fa fa-check"></i><b>8.3.2</b> The Uncorrected AIC provides no more useful information here</a></li>
<li class="chapter" data-level="8.3.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#building-a-tibble-containing-the-necessary-information"><i class="fa fa-check"></i><b>8.3.3</b> Building a Tibble containing the necessary information</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#plotting-the-best-subsets-results-using-ggplot2"><i class="fa fa-check"></i><b>8.4</b> Plotting the Best Subsets Results using <code>ggplot2</code></a><ul>
<li class="chapter" data-level="8.4.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-adjusted-r2-plot"><i class="fa fa-check"></i><b>8.4.1</b> The Adjusted R<sup>2</sup> Plot</a></li>
<li class="chapter" data-level="8.4.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#mallows-c_p"><i class="fa fa-check"></i><b>8.4.2</b> Mallows’ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="8.4.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-c_p-plot"><i class="fa fa-check"></i><b>8.4.3</b> The <span class="math inline">\(C_p\)</span> Plot</a></li>
<li class="chapter" data-level="8.4.4" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#all-subsets-regression-and-information-criteria"><i class="fa fa-check"></i><b>8.4.4</b> “All Subsets” Regression and Information Criteria</a></li>
<li class="chapter" data-level="8.4.5" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-bias-corrected-aic-plot"><i class="fa fa-check"></i><b>8.4.5</b> The bias-corrected AIC plot</a></li>
<li class="chapter" data-level="8.4.6" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-bic-plot"><i class="fa fa-check"></i><b>8.4.6</b> The BIC plot</a></li>
<li class="chapter" data-level="8.4.7" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#all-four-plots-in-one-figure-via-ggplot2"><i class="fa fa-check"></i><b>8.4.7</b> All Four Plots in One Figure (via ggplot2)</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#table-of-key-results"><i class="fa fa-check"></i><b>8.5</b> Table of Key Results</a></li>
<li class="chapter" data-level="8.6" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#models-worth-considering"><i class="fa fa-check"></i><b>8.6</b> Models Worth Considering?</a></li>
<li class="chapter" data-level="8.7" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#compare-these-candidate-models-in-sample"><i class="fa fa-check"></i><b>8.7</b> Compare these candidate models in-sample?</a><ul>
<li class="chapter" data-level="8.7.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#using-anova-to-compare-nested-models"><i class="fa fa-check"></i><b>8.7.1</b> Using <code>anova</code> to compare nested models</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#aic-and-bic-comparisons-within-the-training-sample"><i class="fa fa-check"></i><b>8.8</b> AIC and BIC comparisons, within the training sample</a></li>
<li class="chapter" data-level="8.9" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#cross-validation-of-candidate-models-out-of-sample"><i class="fa fa-check"></i><b>8.9</b> Cross-Validation of Candidate Models out of Sample</a><ul>
<li class="chapter" data-level="8.9.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#fold-cross-validation-of-model-m04"><i class="fa fa-check"></i><b>8.9.1</b> 20-fold Cross-Validation of model <code>m04</code></a></li>
<li class="chapter" data-level="8.9.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#fold-cross-validation-of-model-m07"><i class="fa fa-check"></i><b>8.9.2</b> 20-fold Cross-Validation of model <code>m07</code></a></li>
<li class="chapter" data-level="8.9.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#fold-cross-validation-of-model-m08"><i class="fa fa-check"></i><b>8.9.3</b> 20-fold Cross-Validation of model <code>m08</code></a></li>
<li class="chapter" data-level="8.9.4" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#comparing-the-results-of-the-cross-validations"><i class="fa fa-check"></i><b>8.9.4</b> Comparing the Results of the Cross-Validations</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#what-about-interaction-terms"><i class="fa fa-check"></i><b>8.10</b> What about Interaction Terms?</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html"><i class="fa fa-check"></i><b>9</b> Adding Non-linear Terms to a Linear Regression Model</a><ul>
<li class="chapter" data-level="9.1" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#the-pollution-data"><i class="fa fa-check"></i><b>9.1</b> The <code>pollution</code> data</a></li>
<li class="chapter" data-level="9.2" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#fitting-a-straight-line-model-to-predict-y-from-x2"><i class="fa fa-check"></i><b>9.2</b> Fitting a straight line model to predict <code>y</code> from <code>x2</code></a></li>
<li class="chapter" data-level="9.3" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#quadratic-polynomial-model-to-predict-y-using-x2"><i class="fa fa-check"></i><b>9.3</b> Quadratic polynomial model to predict <code>y</code> using <code>x2</code></a><ul>
<li class="chapter" data-level="9.3.1" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#the-raw-quadratic-model"><i class="fa fa-check"></i><b>9.3.1</b> The raw quadratic model</a></li>
<li class="chapter" data-level="9.3.2" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#raw-quadratic-fit-after-centering-x2"><i class="fa fa-check"></i><b>9.3.2</b> Raw quadratic fit after centering <code>x2</code></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#orthogonal-polynomials"><i class="fa fa-check"></i><b>9.4</b> Orthogonal Polynomials</a></li>
<li class="chapter" data-level="9.5" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#fit-a-cubic-polynomial-to-predict-y-from-x3"><i class="fa fa-check"></i><b>9.5</b> Fit a cubic polynomial to predict <code>y</code> from <code>x3</code></a></li>
<li class="chapter" data-level="9.6" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#fitting-a-restricted-cubic-spline-in-a-linear-regression"><i class="fa fa-check"></i><b>9.6</b> Fitting a restricted cubic spline in a linear regression</a></li>
<li class="chapter" data-level="9.7" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#spending-degrees-of-freedom"><i class="fa fa-check"></i><b>9.7</b> “Spending” Degrees of Freedom</a><ul>
<li class="chapter" data-level="9.7.1" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#overfitting-and-limits-on-the-of-predictors"><i class="fa fa-check"></i><b>9.7.1</b> Overfitting and Limits on the # of Predictors</a></li>
<li class="chapter" data-level="9.7.2" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#the-importance-of-collinearity"><i class="fa fa-check"></i><b>9.7.2</b> The Importance of Collinearity</a></li>
<li class="chapter" data-level="9.7.3" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#collinearity-in-an-explanatory-model"><i class="fa fa-check"></i><b>9.7.3</b> Collinearity in an Explanatory Model</a></li>
<li class="chapter" data-level="9.7.4" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#collinearity-in-a-prediction-model"><i class="fa fa-check"></i><b>9.7.4</b> Collinearity in a Prediction Model</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#spending-df-on-non-linearity-the-spearman-rho2-plot"><i class="fa fa-check"></i><b>9.8</b> Spending DF on Non-Linearity: The Spearman <span class="math inline">\(\rho^2\)</span> Plot</a><ul>
<li class="chapter" data-level="9.8.1" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#fitting-a-big-model-to-the-pollution-data"><i class="fa fa-check"></i><b>9.8.1</b> Fitting a Big Model to the <code>pollution</code> data</a></li>
<li class="chapter" data-level="9.8.2" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#limitations-of-lm-for-fitting-complex-linear-regression-models"><i class="fa fa-check"></i><b>9.8.2</b> Limitations of <code>lm</code> for fitting complex linear regression models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html"><i class="fa fa-check"></i><b>10</b> Using <code>ols</code> from the <code>rms</code> package to fit linear models</a><ul>
<li class="chapter" data-level="10.1" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#fitting-a-model-with-ols"><i class="fa fa-check"></i><b>10.1</b> Fitting a model with <code>ols</code></a><ul>
<li class="chapter" data-level="10.1.1" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#the-model-likelihood-ratio-test"><i class="fa fa-check"></i><b>10.1.1</b> The Model Likelihood Ratio Test</a></li>
<li class="chapter" data-level="10.1.2" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#the-g-statistic"><i class="fa fa-check"></i><b>10.1.2</b> The g statistic</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#anova-for-an-ols-model"><i class="fa fa-check"></i><b>10.2</b> ANOVA for an <code>ols</code> model</a></li>
<li class="chapter" data-level="10.3" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#effect-estimates"><i class="fa fa-check"></i><b>10.3</b> Effect Estimates</a><ul>
<li class="chapter" data-level="10.3.1" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#simultaneous-confidence-intervals"><i class="fa fa-check"></i><b>10.3.1</b> Simultaneous Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#the-predict-function-for-an-ols-model"><i class="fa fa-check"></i><b>10.4</b> The <code>Predict</code> function for an <code>ols</code> model</a></li>
<li class="chapter" data-level="10.5" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#checking-influence-via-dfbeta"><i class="fa fa-check"></i><b>10.5</b> Checking Influence via <code>dfbeta</code></a><ul>
<li class="chapter" data-level="10.5.1" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#using-the-residuals-command-for-dfbetas"><i class="fa fa-check"></i><b>10.5.1</b> Using the <code>residuals</code> command for <code>dfbetas</code></a></li>
<li class="chapter" data-level="10.5.2" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#using-the-residuals-command-for-other-summaries"><i class="fa fa-check"></i><b>10.5.2</b> Using the <code>residuals</code> command for other summaries</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#model-validation-and-correcting-for-optimism"><i class="fa fa-check"></i><b>10.6</b> Model Validation and Correcting for Optimism</a></li>
<li class="chapter" data-level="10.7" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#building-a-nomogram-for-our-model"><i class="fa fa-check"></i><b>10.7</b> Building a Nomogram for Our Model</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html"><i class="fa fa-check"></i><b>11</b> Other Variable Selection Strategies</a><ul>
<li class="chapter" data-level="11.1" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#why-not-use-stepwise-procedures"><i class="fa fa-check"></i><b>11.1</b> Why not use stepwise procedures?</a></li>
<li class="chapter" data-level="11.2" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#ridge-regression"><i class="fa fa-check"></i><b>11.2</b> Ridge Regression</a><ul>
<li class="chapter" data-level="11.2.1" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#assessing-a-ridge-regression-approach"><i class="fa fa-check"></i><b>11.2.1</b> Assessing a Ridge Regression Approach</a></li>
<li class="chapter" data-level="11.2.2" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#the-lm.ridge-plot---where-do-coefficients-stabilize"><i class="fa fa-check"></i><b>11.2.2</b> The <code>lm.ridge</code> plot - where do coefficients stabilize?</a></li>
<li class="chapter" data-level="11.2.3" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#ridge-regression-the-bottom-line"><i class="fa fa-check"></i><b>11.2.3</b> Ridge Regression: The Bottom Line</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#the-lasso"><i class="fa fa-check"></i><b>11.3</b> The Lasso</a><ul>
<li class="chapter" data-level="11.3.1" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#consequences-of-the-lasso-approach"><i class="fa fa-check"></i><b>11.3.1</b> Consequences of the Lasso Approach</a></li>
<li class="chapter" data-level="11.3.2" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#how-the-lasso-works"><i class="fa fa-check"></i><b>11.3.2</b> How The Lasso Works</a></li>
<li class="chapter" data-level="11.3.3" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#cross-validation-with-the-lasso"><i class="fa fa-check"></i><b>11.3.3</b> Cross-Validation with the Lasso</a></li>
<li class="chapter" data-level="11.3.4" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#what-value-of-the-key-fraction-minimizes-cross-validated-mse"><i class="fa fa-check"></i><b>11.3.4</b> What value of the key fraction minimizes cross-validated MSE?</a></li>
<li class="chapter" data-level="11.3.5" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#coefficients-for-the-model-identified-by-the-cross-validation"><i class="fa fa-check"></i><b>11.3.5</b> Coefficients for the Model Identified by the Cross-Validation</a></li>
<li class="chapter" data-level="11.3.6" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#obtaining-fitted-values-from-lasso"><i class="fa fa-check"></i><b>11.3.6</b> Obtaining Fitted Values from Lasso</a></li>
<li class="chapter" data-level="11.3.7" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#complete-set-of-fitted-values-from-the-lasso"><i class="fa fa-check"></i><b>11.3.7</b> Complete Set of Fitted Values from the Lasso</a></li>
<li class="chapter" data-level="11.3.8" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#when-is-the-lasso-most-useful"><i class="fa fa-check"></i><b>11.3.8</b> When is the Lasso Most Useful?</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#applying-the-lasso-to-the-pollution-data"><i class="fa fa-check"></i><b>11.4</b> Applying the Lasso to the <code>pollution</code> data</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html"><i class="fa fa-check"></i><b>12</b> Logistic Regression: The Foundations</a><ul>
<li class="chapter" data-level="12.1" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#a-first-attempt-a-linear-probability-model"><i class="fa fa-check"></i><b>12.1</b> A First Attempt: A Linear Probability Model</a></li>
<li class="chapter" data-level="12.2" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#logistic-regression"><i class="fa fa-check"></i><b>12.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="12.3" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#the-logistic-regression-model"><i class="fa fa-check"></i><b>12.3</b> The Logistic Regression Model</a></li>
<li class="chapter" data-level="12.4" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#the-link-function"><i class="fa fa-check"></i><b>12.4</b> The Link Function</a></li>
<li class="chapter" data-level="12.5" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#the-logit-or-log-odds"><i class="fa fa-check"></i><b>12.5</b> The logit or log odds</a></li>
<li class="chapter" data-level="12.6" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#interpreting-the-coefficients-of-a-logistic-regression-model"><i class="fa fa-check"></i><b>12.6</b> Interpreting the Coefficients of a Logistic Regression Model</a></li>
<li class="chapter" data-level="12.7" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#the-logistic-regression-has-non-constant-variance"><i class="fa fa-check"></i><b>12.7</b> The Logistic Regression has non-constant variance</a></li>
<li class="chapter" data-level="12.8" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#fitting-a-logistic-regression-model-to-our-simulated-data"><i class="fa fa-check"></i><b>12.8</b> Fitting a Logistic Regression Model to our Simulated Data</a></li>
<li class="chapter" data-level="12.9" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#plotting-the-logistic-regression-model"><i class="fa fa-check"></i><b>12.9</b> Plotting the Logistic Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html"><i class="fa fa-check"></i><b>13</b> Logistic Regression and the <code>resect</code> data</a><ul>
<li class="chapter" data-level="13.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#the-resect-data"><i class="fa fa-check"></i><b>13.1</b> The <code>resect</code> data</a></li>
<li class="chapter" data-level="13.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#running-a-simple-logistic-regression-model"><i class="fa fa-check"></i><b>13.2</b> Running A Simple Logistic Regression Model</a><ul>
<li class="chapter" data-level="13.2.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#logistic-regression-can-be-harder-than-linear-regression"><i class="fa fa-check"></i><b>13.2.1</b> Logistic Regression Can Be Harder than Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#logistic-regression-using-glm"><i class="fa fa-check"></i><b>13.3</b> Logistic Regression using <code>glm</code></a><ul>
<li class="chapter" data-level="13.3.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-coefficients-of-a-logistic-regression-model-1"><i class="fa fa-check"></i><b>13.3.1</b> Interpreting the Coefficients of a Logistic Regression Model</a></li>
<li class="chapter" data-level="13.3.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#using-predict-to-describe-the-models-fits"><i class="fa fa-check"></i><b>13.3.2</b> Using <code>predict</code> to describe the model’s fits</a></li>
<li class="chapter" data-level="13.3.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#odds-ratio-interpretation-of-coefficients"><i class="fa fa-check"></i><b>13.3.3</b> Odds Ratio interpretation of Coefficients</a></li>
<li class="chapter" data-level="13.3.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-rest-of-the-model-output-from-glm"><i class="fa fa-check"></i><b>13.3.4</b> Interpreting the rest of the model output from <code>glm</code></a></li>
<li class="chapter" data-level="13.3.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#deviance-and-comparing-our-model-to-the-null-model"><i class="fa fa-check"></i><b>13.3.5</b> Deviance and Comparing Our Model to the Null Model</a></li>
<li class="chapter" data-level="13.3.6" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#using-glance-with-a-logistic-regression-model"><i class="fa fa-check"></i><b>13.3.6</b> Using <code>glance</code> with a logistic regression model</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-model-summary"><i class="fa fa-check"></i><b>13.4</b> Interpreting the Model Summary</a><ul>
<li class="chapter" data-level="13.4.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#wald-z-tests-for-coefficients-in-a-logistic-regression"><i class="fa fa-check"></i><b>13.4.1</b> Wald Z tests for Coefficients in a Logistic Regression</a></li>
<li class="chapter" data-level="13.4.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#confidence-intervals-for-the-coefficients"><i class="fa fa-check"></i><b>13.4.2</b> Confidence Intervals for the Coefficients</a></li>
<li class="chapter" data-level="13.4.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#deviance-residuals"><i class="fa fa-check"></i><b>13.4.3</b> Deviance Residuals</a></li>
<li class="chapter" data-level="13.4.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#dispersion-parameter"><i class="fa fa-check"></i><b>13.4.4</b> Dispersion Parameter</a></li>
<li class="chapter" data-level="13.4.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#fisher-scoring-iterations"><i class="fa fa-check"></i><b>13.4.5</b> Fisher Scoring iterations</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-a-simple-logistic-regression-model"><i class="fa fa-check"></i><b>13.5</b> Plotting a Simple Logistic Regression Model</a><ul>
<li class="chapter" data-level="13.5.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#using-augment-to-capture-the-fitted-probabilities"><i class="fa fa-check"></i><b>13.5.1</b> Using <code>augment</code> to capture the fitted probabilities</a></li>
<li class="chapter" data-level="13.5.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-a-logistic-regression-models-fitted-values"><i class="fa fa-check"></i><b>13.5.2</b> Plotting a Logistic Regression Model’s Fitted Values</a></li>
<li class="chapter" data-level="13.5.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-a-simple-logistic-model-using-binomial_smooth"><i class="fa fa-check"></i><b>13.5.3</b> Plotting a Simple Logistic Model using <code>binomial_smooth</code></a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#how-well-does-model-a-classify-subjects"><i class="fa fa-check"></i><b>13.6</b> How well does Model A classify subjects?</a></li>
<li class="chapter" data-level="13.7" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#receiver-operating-characteristic-curve-analysis"><i class="fa fa-check"></i><b>13.7</b> Receiver Operating Characteristic Curve Analysis</a><ul>
<li class="chapter" data-level="13.7.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-area-under-the-roc-curve"><i class="fa fa-check"></i><b>13.7.1</b> Interpreting the Area under the ROC curve</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#the-roc-plot-for-res_moda"><i class="fa fa-check"></i><b>13.8</b> The ROC Plot for <code>res_modA</code></a><ul>
<li class="chapter" data-level="13.8.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#another-way-to-plot-the-roc-curve"><i class="fa fa-check"></i><b>13.8.1</b> Another way to plot the ROC Curve</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#assessing-residual-plots-from-model-a"><i class="fa fa-check"></i><b>13.9</b> Assessing Residual Plots from Model A</a></li>
<li class="chapter" data-level="13.10" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#model-b-a-kitchen-sink-logistic-regression-model"><i class="fa fa-check"></i><b>13.10</b> Model B: A “Kitchen Sink” Logistic Regression Model</a><ul>
<li class="chapter" data-level="13.10.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#comparing-model-a-to-model-b"><i class="fa fa-check"></i><b>13.10.1</b> Comparing Model A to Model B</a></li>
<li class="chapter" data-level="13.10.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-model-b"><i class="fa fa-check"></i><b>13.10.2</b> Interpreting Model B</a></li>
</ul></li>
<li class="chapter" data-level="13.11" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-model-b"><i class="fa fa-check"></i><b>13.11</b> Plotting Model B</a><ul>
<li class="chapter" data-level="13.11.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#using-augment-to-capture-the-fitted-probabilities-1"><i class="fa fa-check"></i><b>13.11.1</b> Using <code>augment</code> to capture the fitted probabilities</a></li>
<li class="chapter" data-level="13.11.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-model-b-fits-by-observed-mortality"><i class="fa fa-check"></i><b>13.11.2</b> Plotting Model B Fits by Observed Mortality</a></li>
<li class="chapter" data-level="13.11.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#the-roc-curve-for-model-b"><i class="fa fa-check"></i><b>13.11.3</b> The ROC curve for Model B</a></li>
<li class="chapter" data-level="13.11.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#residuals-leverage-and-influence"><i class="fa fa-check"></i><b>13.11.4</b> Residuals, Leverage and Influence</a></li>
</ul></li>
<li class="chapter" data-level="13.12" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#logistic-regression-using-lrm"><i class="fa fa-check"></i><b>13.12</b> Logistic Regression using <code>lrm</code></a><ul>
<li class="chapter" data-level="13.12.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-nagelkerke-r2"><i class="fa fa-check"></i><b>13.12.1</b> Interpreting Nagelkerke R<sup>2</sup></a></li>
<li class="chapter" data-level="13.12.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-c-statistic-and-plotting-the-roc-curve"><i class="fa fa-check"></i><b>13.12.2</b> Interpreting the C statistic and Plotting the ROC Curve</a></li>
<li class="chapter" data-level="13.12.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#the-c-statistic-and-somers-d"><i class="fa fa-check"></i><b>13.12.3</b> The C statistic and Somers’ D</a></li>
<li class="chapter" data-level="13.12.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#validating-the-logistic-regression-model-summary-statistics"><i class="fa fa-check"></i><b>13.12.4</b> Validating the Logistic Regression Model Summary Statistics</a></li>
<li class="chapter" data-level="13.12.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-the-summary-of-the-lrm-approach"><i class="fa fa-check"></i><b>13.12.5</b> Plotting the Summary of the <code>lrm</code> approach</a></li>
<li class="chapter" data-level="13.12.6" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plot-in-sample-predictions-for-model-c"><i class="fa fa-check"></i><b>13.12.6</b> Plot In-Sample Predictions for Model C</a></li>
<li class="chapter" data-level="13.12.7" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#anova-from-the-lrm-approach"><i class="fa fa-check"></i><b>13.12.7</b> ANOVA from the <code>lrm</code> approach</a></li>
<li class="chapter" data-level="13.12.8" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#are-any-points-particularly-influential"><i class="fa fa-check"></i><b>13.12.8</b> Are any points particularly influential?</a></li>
<li class="chapter" data-level="13.12.9" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#a-nomogram-for-model-c"><i class="fa fa-check"></i><b>13.12.9</b> A Nomogram for Model C</a></li>
</ul></li>
<li class="chapter" data-level="13.13" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#model-d-an-augmented-kitchen-sink-model"><i class="fa fa-check"></i><b>13.13</b> Model D: An Augmented Kitchen Sink Model</a><ul>
<li class="chapter" data-level="13.13.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#spearman-rho2-plot"><i class="fa fa-check"></i><b>13.13.1</b> Spearman <span class="math inline">\(\rho^2\)</span> Plot</a></li>
<li class="chapter" data-level="13.13.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#fitting-model-d-using-lrm"><i class="fa fa-check"></i><b>13.13.2</b> Fitting Model D using <code>lrm</code></a></li>
<li class="chapter" data-level="13.13.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#assessing-model-d-using-lrms-tools"><i class="fa fa-check"></i><b>13.13.3</b> Assessing Model D using <code>lrm</code>’s tools</a></li>
<li class="chapter" data-level="13.13.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#anova-and-wald-tests-for-model-d"><i class="fa fa-check"></i><b>13.13.4</b> ANOVA and Wald Tests for Model D</a></li>
<li class="chapter" data-level="13.13.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#effect-sizes-in-model-d"><i class="fa fa-check"></i><b>13.13.5</b> Effect Sizes in Model D</a></li>
<li class="chapter" data-level="13.13.6" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plot-in-sample-predictions-for-model-d"><i class="fa fa-check"></i><b>13.13.6</b> Plot In-Sample Predictions for Model D</a></li>
<li class="chapter" data-level="13.13.7" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-the-roc-curve-for-model-d"><i class="fa fa-check"></i><b>13.13.7</b> Plotting the ROC curve for Model D</a></li>
<li class="chapter" data-level="13.13.8" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#validation-of-model-d-summaries"><i class="fa fa-check"></i><b>13.13.8</b> Validation of Model D summaries</a></li>
</ul></li>
<li class="chapter" data-level="13.14" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#model-e-fitting-a-reduced-model-in-light-of-model-d"><i class="fa fa-check"></i><b>13.14</b> Model E: Fitting a Reduced Model in light of Model D</a><ul>
<li class="chapter" data-level="13.14.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#a-plot-comparing-the-two-intubation-groups"><i class="fa fa-check"></i><b>13.14.1</b> A Plot comparing the two intubation groups</a></li>
<li class="chapter" data-level="13.14.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#nomogram-for-model-e"><i class="fa fa-check"></i><b>13.14.2</b> Nomogram for Model E</a></li>
<li class="chapter" data-level="13.14.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#effect-sizes-from-model-e"><i class="fa fa-check"></i><b>13.14.3</b> Effect Sizes from Model E</a></li>
<li class="chapter" data-level="13.14.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plot-in-sample-predictions-for-model-e"><i class="fa fa-check"></i><b>13.14.4</b> Plot In-Sample Predictions for Model E</a></li>
<li class="chapter" data-level="13.14.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#anova-for-model-e"><i class="fa fa-check"></i><b>13.14.5</b> ANOVA for Model E</a></li>
<li class="chapter" data-level="13.14.6" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#validation-of-model-e"><i class="fa fa-check"></i><b>13.14.6</b> Validation of Model E</a></li>
<li class="chapter" data-level="13.14.7" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#do-any-points-seem-particularly-influential"><i class="fa fa-check"></i><b>13.14.7</b> Do any points seem particularly influential?</a></li>
<li class="chapter" data-level="13.14.8" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#fitting-model-e-using-glm-to-get-plots-about-influence"><i class="fa fa-check"></i><b>13.14.8</b> Fitting Model E using <code>glm</code> to get plots about influence</a></li>
</ul></li>
<li class="chapter" data-level="13.15" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#concordance-comparing-model-c-d-and-es-predictions"><i class="fa fa-check"></i><b>13.15</b> Concordance: Comparing Model C, D and E’s predictions</a></li>
<li class="chapter" data-level="13.16" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#conclusions"><i class="fa fa-check"></i><b>13.16</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html"><i class="fa fa-check"></i><b>14</b> Logistic Regression and the <code>smartcle1</code> data</a><ul>
<li class="chapter" data-level="14.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#the-smartcle1-data"><i class="fa fa-check"></i><b>14.1</b> The <code>smartcle1</code> data</a></li>
<li class="chapter" data-level="14.2" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#thinking-about-non-linear-terms"><i class="fa fa-check"></i><b>14.2</b> Thinking About Non-Linear Terms</a></li>
<li class="chapter" data-level="14.3" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#a-first-model-for-exerany-complete-case-analysis"><i class="fa fa-check"></i><b>14.3</b> A First Model for <code>exerany</code> (Complete Case Analysis)</a></li>
<li class="chapter" data-level="14.4" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#building-a-larger-model-spearman-rho2-plot"><i class="fa fa-check"></i><b>14.4</b> Building a Larger Model: Spearman <span class="math inline">\(\rho^2\)</span> Plot</a></li>
<li class="chapter" data-level="14.5" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#a-second-model-for-exerany-complete-cases"><i class="fa fa-check"></i><b>14.5</b> A Second Model for <code>exerany</code> (Complete Cases)</a></li>
<li class="chapter" data-level="14.6" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#dealing-with-missing-data-via-simple-imputation"><i class="fa fa-check"></i><b>14.6</b> Dealing with Missing Data via Simple Imputation</a><ul>
<li class="chapter" data-level="14.6.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#omit-cases-where-the-outcome-is-missing"><i class="fa fa-check"></i><b>14.6.1</b> Omit cases where the outcome is missing</a></li>
<li class="chapter" data-level="14.6.2" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#plot-the-remaining-missingness"><i class="fa fa-check"></i><b>14.6.2</b> Plot the remaining missingness</a></li>
<li class="chapter" data-level="14.6.3" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#use-simple-imputation-build-a-new-data-set"><i class="fa fa-check"></i><b>14.6.3</b> Use simple imputation, build a new data set</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#refitting-model-1-with-simply-imputed-data"><i class="fa fa-check"></i><b>14.7</b> Refitting Model 1 with simply imputed data</a><ul>
<li class="chapter" data-level="14.7.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#validating-summary-statistics"><i class="fa fa-check"></i><b>14.7.1</b> Validating Summary Statistics</a></li>
<li class="chapter" data-level="14.7.2" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#anova-for-the-model"><i class="fa fa-check"></i><b>14.7.2</b> ANOVA for the model</a></li>
<li class="chapter" data-level="14.7.3" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#summarizing-effect-size"><i class="fa fa-check"></i><b>14.7.3</b> Summarizing Effect Size</a></li>
<li class="chapter" data-level="14.7.4" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#plotting-the-model-with-ggplot-and-predict"><i class="fa fa-check"></i><b>14.7.4</b> Plotting the Model with <code>ggplot</code> and <code>Predict</code></a></li>
<li class="chapter" data-level="14.7.5" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#plotting-the-model-with-a-nomogram"><i class="fa fa-check"></i><b>14.7.5</b> Plotting the model with a nomogram</a></li>
<li class="chapter" data-level="14.7.6" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#checking-the-goodness-of-fit-of-our-model"><i class="fa fa-check"></i><b>14.7.6</b> Checking the Goodness of Fit of our model</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#refitting-model-2-with-simply-imputed-data"><i class="fa fa-check"></i><b>14.8</b> Refitting Model 2 with simply imputed data</a><ul>
<li class="chapter" data-level="14.8.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#validating-summary-statistics-1"><i class="fa fa-check"></i><b>14.8.1</b> Validating Summary Statistics</a></li>
<li class="chapter" data-level="14.8.2" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#anova-for-the-model-1"><i class="fa fa-check"></i><b>14.8.2</b> ANOVA for the model</a></li>
<li class="chapter" data-level="14.8.3" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#summarizing-effect-size-1"><i class="fa fa-check"></i><b>14.8.3</b> Summarizing Effect Size</a></li>
<li class="chapter" data-level="14.8.4" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#plotting-the-model-with-ggplot-and-predict-1"><i class="fa fa-check"></i><b>14.8.4</b> Plotting the Model with <code>ggplot</code> and <code>Predict</code></a></li>
<li class="chapter" data-level="14.8.5" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#plotting-the-model-with-a-nomogram-1"><i class="fa fa-check"></i><b>14.8.5</b> Plotting the model with a nomogram</a></li>
<li class="chapter" data-level="14.8.6" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#checking-the-goodness-of-fit-of-our-model-1"><i class="fa fa-check"></i><b>14.8.6</b> Checking the Goodness of Fit of our model</a></li>
</ul></li>
<li class="chapter" data-level="14.9" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#comparing-model-2-to-model-1-after-simple-imputation"><i class="fa fa-check"></i><b>14.9</b> Comparing Model 2 to Model 1 after simple imputation</a><ul>
<li class="chapter" data-level="14.9.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#comparison-by-analysis-of-deviance"><i class="fa fa-check"></i><b>14.9.1</b> Comparison by Analysis of Deviance</a></li>
<li class="chapter" data-level="14.9.2" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#comparing-aic-and-bic"><i class="fa fa-check"></i><b>14.9.2</b> Comparing AIC and BIC</a></li>
</ul></li>
<li class="chapter" data-level="14.10" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#dealing-with-missing-data-via-multiple-imputation"><i class="fa fa-check"></i><b>14.10</b> Dealing with Missing Data via Multiple Imputation</a><ul>
<li class="chapter" data-level="14.10.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#using-aregimpute-to-fit-a-multiple-imputation-model"><i class="fa fa-check"></i><b>14.10.1</b> Using <code>aregImpute</code> to fit a multiple imputation model</a></li>
</ul></li>
<li class="chapter" data-level="14.11" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#combining-the-imputation-and-outcome-models"><i class="fa fa-check"></i><b>14.11</b> Combining the Imputation and Outcome Models</a><ul>
<li class="chapter" data-level="14.11.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#model-1-with-multiple-imputation"><i class="fa fa-check"></i><b>14.11.1</b> Model 1 with Multiple Imputation</a></li>
<li class="chapter" data-level="14.11.2" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#model-2-with-multiple-imputation"><i class="fa fa-check"></i><b>14.11.2</b> Model 2 with Multiple Imputation</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#models-with-and-without-imputation"><i class="fa fa-check"></i><b>14.12</b> Models with and without Imputation</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html"><i class="fa fa-check"></i><b>15</b> Linear Regression and the <code>smartcle1</code> data</a><ul>
<li class="chapter" data-level="15.1" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#the-smartcle1-data-1"><i class="fa fa-check"></i><b>15.1</b> The <code>smartcle1</code> data</a></li>
<li class="chapter" data-level="15.2" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#thinking-about-non-linear-terms-1"><i class="fa fa-check"></i><b>15.2</b> Thinking About Non-Linear Terms</a></li>
<li class="chapter" data-level="15.3" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#a-first-model-for-sleephrs-complete-case-analysis"><i class="fa fa-check"></i><b>15.3</b> A First Model for <code>sleephrs</code> (Complete Case Analysis)</a></li>
<li class="chapter" data-level="15.4" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#building-a-larger-model-spearman-rho2-plot-1"><i class="fa fa-check"></i><b>15.4</b> Building a Larger Model: Spearman <span class="math inline">\(\rho^2\)</span> Plot</a></li>
<li class="chapter" data-level="15.5" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#a-second-model-for-sleephrs-complete-cases"><i class="fa fa-check"></i><b>15.5</b> A Second Model for <code>sleephrs</code> (Complete Cases)</a></li>
<li class="chapter" data-level="15.6" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#dealing-with-missing-data-via-simple-imputation-1"><i class="fa fa-check"></i><b>15.6</b> Dealing with Missing Data via Simple Imputation</a><ul>
<li class="chapter" data-level="15.6.1" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#omit-cases-where-the-outcome-is-missing-1"><i class="fa fa-check"></i><b>15.6.1</b> Omit cases where the outcome is missing</a></li>
<li class="chapter" data-level="15.6.2" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#plot-the-remaining-missingness-1"><i class="fa fa-check"></i><b>15.6.2</b> Plot the remaining missingness</a></li>
<li class="chapter" data-level="15.6.3" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#use-simple-imputation-build-a-new-data-set-1"><i class="fa fa-check"></i><b>15.6.3</b> Use simple imputation, build a new data set</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#refitting-model-a-with-simply-imputed-data"><i class="fa fa-check"></i><b>15.7</b> Refitting Model A with simply imputed data</a><ul>
<li class="chapter" data-level="15.7.1" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#validating-summary-statistics-2"><i class="fa fa-check"></i><b>15.7.1</b> Validating Summary Statistics</a></li>
<li class="chapter" data-level="15.7.2" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#anova-for-the-model-2"><i class="fa fa-check"></i><b>15.7.2</b> ANOVA for the model</a></li>
<li class="chapter" data-level="15.7.3" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#summarizing-effect-size-2"><i class="fa fa-check"></i><b>15.7.3</b> Summarizing Effect Size</a></li>
<li class="chapter" data-level="15.7.4" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#plotting-the-model-with-ggplot-and-predict-2"><i class="fa fa-check"></i><b>15.7.4</b> Plotting the Model with <code>ggplot</code> and <code>Predict</code></a></li>
<li class="chapter" data-level="15.7.5" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#plotting-the-model-with-a-nomogram-2"><i class="fa fa-check"></i><b>15.7.5</b> Plotting the model with a nomogram</a></li>
<li class="chapter" data-level="15.7.6" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#residual-plots-for-mod.a2"><i class="fa fa-check"></i><b>15.7.6</b> Residual Plots for mod.A2</a></li>
</ul></li>
<li class="chapter" data-level="15.8" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#refitting-model-b-with-simply-imputed-data"><i class="fa fa-check"></i><b>15.8</b> Refitting Model B with simply imputed data</a><ul>
<li class="chapter" data-level="15.8.1" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#validating-summary-statistics-3"><i class="fa fa-check"></i><b>15.8.1</b> Validating Summary Statistics</a></li>
<li class="chapter" data-level="15.8.2" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#anova-for-the-model-3"><i class="fa fa-check"></i><b>15.8.2</b> ANOVA for the model</a></li>
<li class="chapter" data-level="15.8.3" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#summarizing-effect-size-3"><i class="fa fa-check"></i><b>15.8.3</b> Summarizing Effect Size</a></li>
<li class="chapter" data-level="15.8.4" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#plotting-the-model-with-ggplot-and-predict-3"><i class="fa fa-check"></i><b>15.8.4</b> Plotting the Model with <code>ggplot</code> and <code>Predict</code></a></li>
</ul></li>
<li class="chapter" data-level="15.9" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#comparing-model-b.2-to-model-a.2-after-simple-imputation"><i class="fa fa-check"></i><b>15.9</b> Comparing Model B.2 to Model A.2 after simple imputation</a><ul>
<li class="chapter" data-level="15.9.1" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#comparison-by-analysis-of-variance"><i class="fa fa-check"></i><b>15.9.1</b> Comparison by Analysis of Variance</a></li>
<li class="chapter" data-level="15.9.2" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#comparing-aic-and-bic-1"><i class="fa fa-check"></i><b>15.9.2</b> Comparing AIC and BIC</a></li>
</ul></li>
<li class="chapter" data-level="15.10" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#dealing-with-missing-data-via-multiple-imputation-1"><i class="fa fa-check"></i><b>15.10</b> Dealing with Missing Data via Multiple Imputation</a><ul>
<li class="chapter" data-level="15.10.1" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#using-aregimpute-to-fit-a-multiple-imputation-model-1"><i class="fa fa-check"></i><b>15.10.1</b> Using <code>aregImpute</code> to fit a multiple imputation model</a></li>
</ul></li>
<li class="chapter" data-level="15.11" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#combining-the-imputation-and-outcome-models-1"><i class="fa fa-check"></i><b>15.11</b> Combining the Imputation and Outcome Models</a><ul>
<li class="chapter" data-level="15.11.1" data-path="linear-regression-and-the-smartcle1-data.html"><a href="linear-regression-and-the-smartcle1-data.html#model-a-with-multiple-imputation"><i class="fa fa-check"></i><b>15.11.1</b> Model A with Multiple Imputation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="colorectal-cancer-screening-and-some-special-cases.html"><a href="colorectal-cancer-screening-and-some-special-cases.html"><i class="fa fa-check"></i><b>16</b> Colorectal Cancer Screening and Some Special Cases</a><ul>
<li class="chapter" data-level="16.1" data-path="colorectal-cancer-screening-and-some-special-cases.html"><a href="colorectal-cancer-screening-and-some-special-cases.html#logistic-regression-for-aggregated-data"><i class="fa fa-check"></i><b>16.1</b> Logistic Regression for Aggregated Data</a><ul>
<li class="chapter" data-level="16.1.1" data-path="colorectal-cancer-screening-and-some-special-cases.html"><a href="colorectal-cancer-screening-and-some-special-cases.html#colorectal-cancer-screening-data"><i class="fa fa-check"></i><b>16.1.1</b> Colorectal Cancer Screening Data</a></li>
<li class="chapter" data-level="16.1.2" data-path="colorectal-cancer-screening-and-some-special-cases.html"><a href="colorectal-cancer-screening-and-some-special-cases.html#fitting-a-logistic-regression-model-to-proportion-data"><i class="fa fa-check"></i><b>16.1.2</b> Fitting a Logistic Regression Model to Proportion Data</a></li>
<li class="chapter" data-level="16.1.3" data-path="colorectal-cancer-screening-and-some-special-cases.html"><a href="colorectal-cancer-screening-and-some-special-cases.html#fitting-a-logistic-regression-model-to-counts-of-successes-and-failures"><i class="fa fa-check"></i><b>16.1.3</b> Fitting a Logistic Regression Model to Counts of Successes and Failures</a></li>
<li class="chapter" data-level="16.1.4" data-path="colorectal-cancer-screening-and-some-special-cases.html"><a href="colorectal-cancer-screening-and-some-special-cases.html#how-does-one-address-this-problem-in-rms"><i class="fa fa-check"></i><b>16.1.4</b> How does one address this problem in <code>rms</code>?</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="colorectal-cancer-screening-and-some-special-cases.html"><a href="colorectal-cancer-screening-and-some-special-cases.html#probit-regression"><i class="fa fa-check"></i><b>16.2</b> Probit Regression</a><ul>
<li class="chapter" data-level="16.2.1" data-path="colorectal-cancer-screening-and-some-special-cases.html"><a href="colorectal-cancer-screening-and-some-special-cases.html#colorectal-cancer-screening-data-on-individuals"><i class="fa fa-check"></i><b>16.2.1</b> Colorectal Cancer Screening Data on Individuals</a></li>
<li class="chapter" data-level="16.2.2" data-path="colorectal-cancer-screening-and-some-special-cases.html"><a href="colorectal-cancer-screening-and-some-special-cases.html#a-logistic-regression-model"><i class="fa fa-check"></i><b>16.2.2</b> A logistic regression model</a></li>
<li class="chapter" data-level="16.2.3" data-path="colorectal-cancer-screening-and-some-special-cases.html"><a href="colorectal-cancer-screening-and-some-special-cases.html#predicting-status-for-harry-and-sally"><i class="fa fa-check"></i><b>16.2.3</b> Predicting status for Harry and Sally</a></li>
<li class="chapter" data-level="16.2.4" data-path="colorectal-cancer-screening-and-some-special-cases.html"><a href="colorectal-cancer-screening-and-some-special-cases.html#a-probit-regression-model"><i class="fa fa-check"></i><b>16.2.4</b> A probit regression model</a></li>
<li class="chapter" data-level="16.2.5" data-path="colorectal-cancer-screening-and-some-special-cases.html"><a href="colorectal-cancer-screening-and-some-special-cases.html#interpreting-the-probit-models-coefficients"><i class="fa fa-check"></i><b>16.2.5</b> Interpreting the Probit Model’s Coefficients</a></li>
<li class="chapter" data-level="16.2.6" data-path="colorectal-cancer-screening-and-some-special-cases.html"><a href="colorectal-cancer-screening-and-some-special-cases.html#what-about-harry-and-sally"><i class="fa fa-check"></i><b>16.2.6</b> What about Harry and Sally?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html"><i class="fa fa-check"></i><b>17</b> Cleaning the BRFSS SMART Data</a><ul>
<li class="chapter" data-level="17.1" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#key-resources-1"><i class="fa fa-check"></i><b>17.1</b> Key resources</a></li>
<li class="chapter" data-level="17.2" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#ingesting-the-raw-data"><i class="fa fa-check"></i><b>17.2</b> Ingesting The Raw Data</a></li>
<li class="chapter" data-level="17.3" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#the-national-data"><i class="fa fa-check"></i><b>17.3</b> The National Data</a></li>
<li class="chapter" data-level="17.4" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#cleaning-the-brfss-data"><i class="fa fa-check"></i><b>17.4</b> Cleaning the BRFSS Data</a><ul>
<li class="chapter" data-level="17.4.1" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#health-status-1-item"><i class="fa fa-check"></i><b>17.4.1</b> Health Status (1 item)</a></li>
<li class="chapter" data-level="17.4.2" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#healthy-days---health-related-quality-of-life-3-items"><i class="fa fa-check"></i><b>17.4.2</b> Healthy Days - Health-Related Quality of Life (3 items)</a></li>
<li class="chapter" data-level="17.4.3" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#health-care-access-4-items"><i class="fa fa-check"></i><b>17.4.3</b> Health Care Access (4 items)</a></li>
<li class="chapter" data-level="17.4.4" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#exercise-1-item"><i class="fa fa-check"></i><b>17.4.4</b> Exercise (1 item)</a></li>
<li class="chapter" data-level="17.4.5" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#inadequate-sleep-1-item"><i class="fa fa-check"></i><b>17.4.5</b> Inadequate Sleep (1 item)</a></li>
<li class="chapter" data-level="17.4.6" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#chronic-health-conditions-13-items"><i class="fa fa-check"></i><b>17.4.6</b> Chronic Health Conditions (13 items)</a></li>
<li class="chapter" data-level="17.4.7" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#oral-health-2-items"><i class="fa fa-check"></i><b>17.4.7</b> Oral Health (2 items)</a></li>
<li class="chapter" data-level="17.4.8" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#demographics-23-items"><i class="fa fa-check"></i><b>17.4.8</b> Demographics (23 items)</a></li>
<li class="chapter" data-level="17.4.9" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#tobacco-use-5-items"><i class="fa fa-check"></i><b>17.4.9</b> Tobacco Use (5 items)</a></li>
<li class="chapter" data-level="17.4.10" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#e-cigarettes-2-items"><i class="fa fa-check"></i><b>17.4.10</b> E-Cigarettes (2 items)</a></li>
<li class="chapter" data-level="17.4.11" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#alcohol-consumption-4-items"><i class="fa fa-check"></i><b>17.4.11</b> Alcohol Consumption (4 items)</a></li>
<li class="chapter" data-level="17.4.12" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#immunization-4-items"><i class="fa fa-check"></i><b>17.4.12</b> Immunization (4 items)</a></li>
<li class="chapter" data-level="17.4.13" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#falls-2-items"><i class="fa fa-check"></i><b>17.4.13</b> Falls (2 items)</a></li>
<li class="chapter" data-level="17.4.14" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#seatbelt-use-1-item"><i class="fa fa-check"></i><b>17.4.14</b> Seatbelt Use (1 item)</a></li>
<li class="chapter" data-level="17.4.15" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#drinking-and-driving-1-item"><i class="fa fa-check"></i><b>17.4.15</b> Drinking and Driving (1 item)</a></li>
<li class="chapter" data-level="17.4.16" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#breast-and-cervical-cancer-screening-7-items"><i class="fa fa-check"></i><b>17.4.16</b> Breast and Cervical Cancer Screening (7 items)</a></li>
<li class="chapter" data-level="17.4.17" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#prostate-cancer-screening-6-items"><i class="fa fa-check"></i><b>17.4.17</b> Prostate Cancer Screening (6 items)</a></li>
<li class="chapter" data-level="17.4.18" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#colorectal-cancer-screening-5-items"><i class="fa fa-check"></i><b>17.4.18</b> Colorectal Cancer Screening (5 items)</a></li>
<li class="chapter" data-level="17.4.19" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#hivaids-3-items"><i class="fa fa-check"></i><b>17.4.19</b> HIV/AIDS (3 items)</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#creating-some-quantitative-variables-from-thin-air"><i class="fa fa-check"></i><b>17.5</b> Creating Some Quantitative Variables from Thin Air</a><ul>
<li class="chapter" data-level="17.5.1" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#age_imp-creating-age-data-out-of-thin-air"><i class="fa fa-check"></i><b>17.5.1</b> <code>age_imp</code>: Creating Age Data out of Thin Air</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#clean-data-in-the-state-of-ohio"><i class="fa fa-check"></i><b>17.6</b> Clean Data in the State of Ohio</a></li>
<li class="chapter" data-level="17.7" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#clean-cleveland-elyria-data"><i class="fa fa-check"></i><b>17.7</b> Clean Cleveland-Elyria Data</a><ul>
<li class="chapter" data-level="17.7.1" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#cleveland---elyria-raw-data"><i class="fa fa-check"></i><b>17.7.1</b> Cleveland - Elyria Raw Data</a></li>
<li class="chapter" data-level="17.7.2" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#clean-data---larger"><i class="fa fa-check"></i><b>17.7.2</b> Clean Data - Larger</a></li>
<li class="chapter" data-level="17.7.3" data-path="cleaning-the-brfss-smart-data.html"><a href="cleaning-the-brfss-smart-data.html#creation-of-the-smartcle1.csv-data"><i class="fa fa-check"></i><b>17.7.3</b> Creation of the <code>smartcle1.csv</code> data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html"><i class="fa fa-check"></i><b>18</b> Modeling a Count Outcome in Ohio SMART</a><ul>
<li class="chapter" data-level="18.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#preliminaries"><i class="fa fa-check"></i><b>18.1</b> Preliminaries</a></li>
<li class="chapter" data-level="18.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#a-subset-of-the-ohio-smart-data"><i class="fa fa-check"></i><b>18.2</b> A subset of the Ohio SMART data</a><ul>
<li class="chapter" data-level="18.2.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#is-age-group-associated-with-physhealth"><i class="fa fa-check"></i><b>18.2.1</b> Is age group associated with <code>physhealth</code>?</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#exploratory-data-analysis-in-the-18-49-group"><i class="fa fa-check"></i><b>18.3</b> Exploratory Data Analysis (in the 18-49 group)</a><ul>
<li class="chapter" data-level="18.3.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#build-a-subset-of-those-ages-18-49"><i class="fa fa-check"></i><b>18.3.1</b> Build a subset of those ages 18-49</a></li>
<li class="chapter" data-level="18.3.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#distribution-of-the-outcome"><i class="fa fa-check"></i><b>18.3.2</b> Distribution of the Outcome</a></li>
<li class="chapter" data-level="18.3.3" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#initial-scatterplot-matrix"><i class="fa fa-check"></i><b>18.3.3</b> Initial Scatterplot Matrix</a></li>
<li class="chapter" data-level="18.3.4" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#dropping-the-subject-with-an-unreasonably-large-bmi"><i class="fa fa-check"></i><b>18.3.4</b> Dropping the subject with an unreasonably large <code>bmi</code></a></li>
<li class="chapter" data-level="18.3.5" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#revised-final-scatterplot-matrix"><i class="fa fa-check"></i><b>18.3.5</b> Revised (Final) Scatterplot Matrix</a></li>
<li class="chapter" data-level="18.3.6" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#summary-of-the-final-subset-of-data"><i class="fa fa-check"></i><b>18.3.6</b> Summary of the final subset of data</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#modeling-strategies-explored-here"><i class="fa fa-check"></i><b>18.4</b> Modeling Strategies Explored Here</a><ul>
<li class="chapter" data-level="18.4.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#what-will-we-demonstrate"><i class="fa fa-check"></i><b>18.4.1</b> What Will We Demonstrate?</a></li>
<li class="chapter" data-level="18.4.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#extra-data-file-for-harry-and-sally"><i class="fa fa-check"></i><b>18.4.2</b> Extra Data File for Harry and Sally</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-ols-approach"><i class="fa fa-check"></i><b>18.5</b> The OLS Approach</a><ul>
<li class="chapter" data-level="18.5.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-fitted-equation"><i class="fa fa-check"></i><b>18.5.1</b> The Fitted Equation</a></li>
<li class="chapter" data-level="18.5.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#interpreting-the-coefficients"><i class="fa fa-check"></i><b>18.5.2</b> Interpreting the Coefficients</a></li>
<li class="chapter" data-level="18.5.3" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#testing-the-predictors"><i class="fa fa-check"></i><b>18.5.3</b> Testing the Predictors</a></li>
<li class="chapter" data-level="18.5.4" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#store-fitted-values-and-residuals"><i class="fa fa-check"></i><b>18.5.4</b> Store fitted values and residuals</a></li>
<li class="chapter" data-level="18.5.5" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#specify-the-r2-and-loglikelihood-values"><i class="fa fa-check"></i><b>18.5.5</b> Specify the R<sup>2</sup> and log(likelihood) values</a></li>
<li class="chapter" data-level="18.5.6" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#check-model-assumptions"><i class="fa fa-check"></i><b>18.5.6</b> Check model assumptions</a></li>
<li class="chapter" data-level="18.5.7" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#predictions-for-harry-and-sally"><i class="fa fa-check"></i><b>18.5.7</b> Predictions for Harry and Sally</a></li>
<li class="chapter" data-level="18.5.8" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#notes"><i class="fa fa-check"></i><b>18.5.8</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#ols-model-on-logphyshealth-1-days"><i class="fa fa-check"></i><b>18.6</b> OLS model on log(<code>physhealth</code> + 1) days</a><ul>
<li class="chapter" data-level="18.6.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-fitted-equation-1"><i class="fa fa-check"></i><b>18.6.1</b> The Fitted Equation</a></li>
<li class="chapter" data-level="18.6.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#interpreting-the-coefficients-1"><i class="fa fa-check"></i><b>18.6.2</b> Interpreting the Coefficients</a></li>
<li class="chapter" data-level="18.6.3" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#testing-the-predictors-1"><i class="fa fa-check"></i><b>18.6.3</b> Testing the Predictors</a></li>
<li class="chapter" data-level="18.6.4" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#store-fitted-values-and-residuals-1"><i class="fa fa-check"></i><b>18.6.4</b> Store fitted values and residuals</a></li>
<li class="chapter" data-level="18.6.5" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#specify-the-r2-and-loglikelihood-values-1"><i class="fa fa-check"></i><b>18.6.5</b> Specify the R<sup>2</sup> and log(likelihood) values</a></li>
<li class="chapter" data-level="18.6.6" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#getting-r2-on-the-scale-of-physhealth"><i class="fa fa-check"></i><b>18.6.6</b> Getting R<sup>2</sup> on the scale of <code>physhealth</code></a></li>
<li class="chapter" data-level="18.6.7" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#check-model-assumptions-1"><i class="fa fa-check"></i><b>18.6.7</b> Check model assumptions</a></li>
<li class="chapter" data-level="18.6.8" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#predictions-for-harry-and-sally-1"><i class="fa fa-check"></i><b>18.6.8</b> Predictions for Harry and Sally</a></li>
</ul></li>
<li class="chapter" data-level="18.7" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#a-poisson-regression-model"><i class="fa fa-check"></i><b>18.7</b> A Poisson Regression Model</a><ul>
<li class="chapter" data-level="18.7.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-fitted-equation-2"><i class="fa fa-check"></i><b>18.7.1</b> The Fitted Equation</a></li>
<li class="chapter" data-level="18.7.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#interpreting-the-coefficients-2"><i class="fa fa-check"></i><b>18.7.2</b> Interpreting the Coefficients</a></li>
<li class="chapter" data-level="18.7.3" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#testing-the-predictors-2"><i class="fa fa-check"></i><b>18.7.3</b> Testing the Predictors</a></li>
<li class="chapter" data-level="18.7.4" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#correcting-for-overdispersion-with-coeftestcoefci"><i class="fa fa-check"></i><b>18.7.4</b> Correcting for Overdispersion with <code>coeftest</code>/<code>coefci</code></a></li>
<li class="chapter" data-level="18.7.5" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#store-fitted-values-and-residuals-2"><i class="fa fa-check"></i><b>18.7.5</b> Store fitted values and residuals</a></li>
<li class="chapter" data-level="18.7.6" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#rootogram-see-the-fit-of-a-count-regression-model"><i class="fa fa-check"></i><b>18.7.6</b> Rootogram: see the fit of a count regression model</a></li>
<li class="chapter" data-level="18.7.7" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#specify-the-r2-and-loglikelihood-values-2"><i class="fa fa-check"></i><b>18.7.7</b> Specify the R<sup>2</sup> and log(likelihood) values</a></li>
<li class="chapter" data-level="18.7.8" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#check-model-assumptions-2"><i class="fa fa-check"></i><b>18.7.8</b> Check model assumptions</a></li>
<li class="chapter" data-level="18.7.9" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#using-glm.diag.plots-from-the-boot-package"><i class="fa fa-check"></i><b>18.7.9</b> Using <code>glm.diag.plots</code> from the <code>boot</code> package</a></li>
<li class="chapter" data-level="18.7.10" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#predictions-for-harry-and-sally-2"><i class="fa fa-check"></i><b>18.7.10</b> Predictions for Harry and Sally</a></li>
</ul></li>
<li class="chapter" data-level="18.8" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#overdispersion-in-a-poisson-model"><i class="fa fa-check"></i><b>18.8</b> Overdispersion in a Poisson Model</a><ul>
<li class="chapter" data-level="18.8.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#testing-for-overdispersion"><i class="fa fa-check"></i><b>18.8.1</b> Testing for Overdispersion?</a></li>
</ul></li>
<li class="chapter" data-level="18.9" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#fitting-the-quasi-poisson-model"><i class="fa fa-check"></i><b>18.9</b> Fitting the Quasi-Poisson Model</a><ul>
<li class="chapter" data-level="18.9.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-fitted-equation-3"><i class="fa fa-check"></i><b>18.9.1</b> The Fitted Equation</a></li>
<li class="chapter" data-level="18.9.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#interpreting-the-coefficients-3"><i class="fa fa-check"></i><b>18.9.2</b> Interpreting the Coefficients</a></li>
<li class="chapter" data-level="18.9.3" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#testing-the-predictors-3"><i class="fa fa-check"></i><b>18.9.3</b> Testing the Predictors</a></li>
<li class="chapter" data-level="18.9.4" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#store-fitted-values-and-residuals-3"><i class="fa fa-check"></i><b>18.9.4</b> Store fitted values and residuals</a></li>
<li class="chapter" data-level="18.9.5" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#specify-the-r2-and-loglikelihood-values-3"><i class="fa fa-check"></i><b>18.9.5</b> Specify the R<sup>2</sup> and log(likelihood) values</a></li>
<li class="chapter" data-level="18.9.6" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#check-model-assumptions-3"><i class="fa fa-check"></i><b>18.9.6</b> Check model assumptions</a></li>
<li class="chapter" data-level="18.9.7" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#predictions-for-harry-and-sally-3"><i class="fa fa-check"></i><b>18.9.7</b> Predictions for Harry and Sally</a></li>
</ul></li>
<li class="chapter" data-level="18.10" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#poisson-and-quasi-poisson-models-using-glm-from-the-rms-package"><i class="fa fa-check"></i><b>18.10</b> Poisson and Quasi-Poisson models using <code>Glm</code> from the <code>rms</code> package</a><ul>
<li class="chapter" data-level="18.10.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#refitting-the-original-poisson-regression-with-glm"><i class="fa fa-check"></i><b>18.10.1</b> Refitting the original Poisson regression with <code>Glm</code></a></li>
<li class="chapter" data-level="18.10.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#refitting-the-overdispersed-poisson-regression-with-glm"><i class="fa fa-check"></i><b>18.10.2</b> Refitting the overdispersed Poisson regression with <code>Glm</code></a></li>
<li class="chapter" data-level="18.10.3" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#anova-on-a-glm-fit"><i class="fa fa-check"></i><b>18.10.3</b> ANOVA on a <code>Glm</code> fit</a></li>
<li class="chapter" data-level="18.10.4" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#ggplots-from-glm-fit"><i class="fa fa-check"></i><b>18.10.4</b> ggplots from <code>Glm</code> fit</a></li>
<li class="chapter" data-level="18.10.5" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#summary-of-a-glm-fit"><i class="fa fa-check"></i><b>18.10.5</b> Summary of a <code>Glm</code> fit</a></li>
<li class="chapter" data-level="18.10.6" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#plot-of-the-summary"><i class="fa fa-check"></i><b>18.10.6</b> Plot of the Summary</a></li>
<li class="chapter" data-level="18.10.7" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#nomogram-of-a-glm-fit"><i class="fa fa-check"></i><b>18.10.7</b> Nomogram of a <code>Glm</code> fit</a></li>
</ul></li>
<li class="chapter" data-level="18.11" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#negative-binomial-model"><i class="fa fa-check"></i><b>18.11</b> Negative Binomial Model</a><ul>
<li class="chapter" data-level="18.11.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-fitted-equation-4"><i class="fa fa-check"></i><b>18.11.1</b> The Fitted Equation</a></li>
<li class="chapter" data-level="18.11.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#comparison-with-the-raw-poisson-model"><i class="fa fa-check"></i><b>18.11.2</b> Comparison with the (raw) Poisson model</a></li>
<li class="chapter" data-level="18.11.3" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#interpreting-the-coefficients-4"><i class="fa fa-check"></i><b>18.11.3</b> Interpreting the Coefficients</a></li>
<li class="chapter" data-level="18.11.4" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#interpretation-of-coefficients-in-terms-of-irrs"><i class="fa fa-check"></i><b>18.11.4</b> Interpretation of Coefficients in terms of IRRs</a></li>
<li class="chapter" data-level="18.11.5" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#testing-the-predictors-4"><i class="fa fa-check"></i><b>18.11.5</b> Testing the Predictors</a></li>
<li class="chapter" data-level="18.11.6" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#store-fitted-values-and-residuals-4"><i class="fa fa-check"></i><b>18.11.6</b> Store fitted values and residuals</a></li>
<li class="chapter" data-level="18.11.7" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#rootogram-for-negative-binomial-model"><i class="fa fa-check"></i><b>18.11.7</b> Rootogram for Negative Binomial model</a></li>
<li class="chapter" data-level="18.11.8" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#simulating-what-the-negative-binomial-model-predicts"><i class="fa fa-check"></i><b>18.11.8</b> Simulating what the Negative Binomial model predicts</a></li>
<li class="chapter" data-level="18.11.9" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#specify-the-r2-and-loglikelihood-values-4"><i class="fa fa-check"></i><b>18.11.9</b> Specify the R<sup>2</sup> and log(likelihood) values</a></li>
<li class="chapter" data-level="18.11.10" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#check-model-assumptions-4"><i class="fa fa-check"></i><b>18.11.10</b> Check model assumptions</a></li>
<li class="chapter" data-level="18.11.11" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#predictions-for-harry-and-sally-4"><i class="fa fa-check"></i><b>18.11.11</b> Predictions for Harry and Sally</a></li>
</ul></li>
<li class="chapter" data-level="18.12" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-problem-too-few-zeros"><i class="fa fa-check"></i><b>18.12</b> The Problem: Too Few Zeros</a></li>
<li class="chapter" data-level="18.13" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-zero-inflated-poisson-regression-model"><i class="fa fa-check"></i><b>18.13</b> The Zero-Inflated Poisson Regression Model</a><ul>
<li class="chapter" data-level="18.13.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#comparison-to-a-null-model"><i class="fa fa-check"></i><b>18.13.1</b> Comparison to a null model</a></li>
<li class="chapter" data-level="18.13.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#comparison-to-a-poisson-model-with-the-vuong-test"><i class="fa fa-check"></i><b>18.13.2</b> Comparison to a Poisson Model with the Vuong test</a></li>
<li class="chapter" data-level="18.13.3" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-fitted-equation-5"><i class="fa fa-check"></i><b>18.13.3</b> The Fitted Equation</a></li>
<li class="chapter" data-level="18.13.4" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#interpreting-the-coefficients-5"><i class="fa fa-check"></i><b>18.13.4</b> Interpreting the Coefficients</a></li>
<li class="chapter" data-level="18.13.5" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#testing-the-predictors-5"><i class="fa fa-check"></i><b>18.13.5</b> Testing the Predictors</a></li>
<li class="chapter" data-level="18.13.6" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#store-fitted-values-and-residuals-5"><i class="fa fa-check"></i><b>18.13.6</b> Store fitted values and residuals</a></li>
<li class="chapter" data-level="18.13.7" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#modeled-number-of-zero-counts"><i class="fa fa-check"></i><b>18.13.7</b> Modeled Number of Zero Counts</a></li>
<li class="chapter" data-level="18.13.8" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#rootogram-for-zip-model"><i class="fa fa-check"></i><b>18.13.8</b> Rootogram for ZIP model</a></li>
<li class="chapter" data-level="18.13.9" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#specify-the-r2-and-log-likelihood-values"><i class="fa fa-check"></i><b>18.13.9</b> Specify the R<sup>2</sup> and log (likelihood) values</a></li>
<li class="chapter" data-level="18.13.10" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#check-model-assumptions-5"><i class="fa fa-check"></i><b>18.13.10</b> Check model assumptions</a></li>
<li class="chapter" data-level="18.13.11" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#predictions-for-harry-and-sally-5"><i class="fa fa-check"></i><b>18.13.11</b> Predictions for Harry and Sally</a></li>
</ul></li>
<li class="chapter" data-level="18.14" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-zero-inflated-negative-binomial-regression-model"><i class="fa fa-check"></i><b>18.14</b> The Zero-Inflated Negative Binomial Regression Model</a><ul>
<li class="chapter" data-level="18.14.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#comparison-to-a-null-model-1"><i class="fa fa-check"></i><b>18.14.1</b> Comparison to a null model</a></li>
<li class="chapter" data-level="18.14.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#comparison-to-a-negative-binomial-model-vuong-test"><i class="fa fa-check"></i><b>18.14.2</b> Comparison to a Negative Binomial Model: Vuong test</a></li>
<li class="chapter" data-level="18.14.3" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-fitted-equation-6"><i class="fa fa-check"></i><b>18.14.3</b> The Fitted Equation</a></li>
<li class="chapter" data-level="18.14.4" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#interpreting-the-coefficients-6"><i class="fa fa-check"></i><b>18.14.4</b> Interpreting the Coefficients</a></li>
<li class="chapter" data-level="18.14.5" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#testing-the-predictors-6"><i class="fa fa-check"></i><b>18.14.5</b> Testing the Predictors</a></li>
<li class="chapter" data-level="18.14.6" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#store-fitted-values-and-residuals-6"><i class="fa fa-check"></i><b>18.14.6</b> Store fitted values and residuals</a></li>
<li class="chapter" data-level="18.14.7" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#modeled-number-of-zero-counts-1"><i class="fa fa-check"></i><b>18.14.7</b> Modeled Number of Zero Counts</a></li>
<li class="chapter" data-level="18.14.8" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#rootogram-for-zero-inflated-negative-binomial-model"><i class="fa fa-check"></i><b>18.14.8</b> Rootogram for Zero-Inflated Negative Binomial model</a></li>
<li class="chapter" data-level="18.14.9" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#specify-the-r2-and-log-likelihood-values-1"><i class="fa fa-check"></i><b>18.14.9</b> Specify the R<sup>2</sup> and log (likelihood) values</a></li>
<li class="chapter" data-level="18.14.10" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#check-model-assumptions-6"><i class="fa fa-check"></i><b>18.14.10</b> Check model assumptions</a></li>
<li class="chapter" data-level="18.14.11" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#predictions-for-harry-and-sally-6"><i class="fa fa-check"></i><b>18.14.11</b> Predictions for Harry and Sally</a></li>
</ul></li>
<li class="chapter" data-level="18.15" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#a-hurdle-model-with-poisson"><i class="fa fa-check"></i><b>18.15</b> A “hurdle” model (with Poisson)</a><ul>
<li class="chapter" data-level="18.15.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#comparison-to-a-null-model-2"><i class="fa fa-check"></i><b>18.15.1</b> Comparison to a null model</a></li>
<li class="chapter" data-level="18.15.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#comparison-to-a-poisson-model-vuong-test"><i class="fa fa-check"></i><b>18.15.2</b> Comparison to a Poisson Model: Vuong test</a></li>
<li class="chapter" data-level="18.15.3" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#comparison-to-a-zero-inflated-poisson-model-vuong-test"><i class="fa fa-check"></i><b>18.15.3</b> Comparison to a Zero-Inflated Poisson Model: Vuong test</a></li>
<li class="chapter" data-level="18.15.4" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-fitted-equation-7"><i class="fa fa-check"></i><b>18.15.4</b> The Fitted Equation</a></li>
<li class="chapter" data-level="18.15.5" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#interpreting-the-coefficients-7"><i class="fa fa-check"></i><b>18.15.5</b> Interpreting the Coefficients</a></li>
<li class="chapter" data-level="18.15.6" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#testing-the-predictors-7"><i class="fa fa-check"></i><b>18.15.6</b> Testing the Predictors</a></li>
<li class="chapter" data-level="18.15.7" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#store-fitted-values-and-residuals-7"><i class="fa fa-check"></i><b>18.15.7</b> Store fitted values and residuals</a></li>
<li class="chapter" data-level="18.15.8" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#modeled-number-of-zero-counts-2"><i class="fa fa-check"></i><b>18.15.8</b> Modeled Number of Zero Counts</a></li>
<li class="chapter" data-level="18.15.9" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#rootogram-for-hurdle-model"><i class="fa fa-check"></i><b>18.15.9</b> Rootogram for Hurdle Model</a></li>
<li class="chapter" data-level="18.15.10" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#understanding-the-modeled-counts-in-detail"><i class="fa fa-check"></i><b>18.15.10</b> Understanding the Modeled Counts in Detail</a></li>
<li class="chapter" data-level="18.15.11" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#specify-the-r2-and-log-likelihood-values-2"><i class="fa fa-check"></i><b>18.15.11</b> Specify the R<sup>2</sup> and log (likelihood) values</a></li>
<li class="chapter" data-level="18.15.12" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#check-model-assumptions-7"><i class="fa fa-check"></i><b>18.15.12</b> Check model assumptions</a></li>
<li class="chapter" data-level="18.15.13" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#predictions-for-harry-and-sally-7"><i class="fa fa-check"></i><b>18.15.13</b> Predictions for Harry and Sally</a></li>
</ul></li>
<li class="chapter" data-level="18.16" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#a-hurdle-model-with-negative-binomial-for-overdispersion"><i class="fa fa-check"></i><b>18.16</b> A “hurdle” model (with negative binomial for overdispersion)</a><ul>
<li class="chapter" data-level="18.16.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#comparison-to-a-null-model-3"><i class="fa fa-check"></i><b>18.16.1</b> Comparison to a null model</a></li>
<li class="chapter" data-level="18.16.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#comparison-to-a-negative-binomial-model-vuong-test-1"><i class="fa fa-check"></i><b>18.16.2</b> Comparison to a Negative Binomial Model: Vuong test</a></li>
<li class="chapter" data-level="18.16.3" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#comparison-to-a-zero-inflated-nb-model-vuong-test"><i class="fa fa-check"></i><b>18.16.3</b> Comparison to a Zero-Inflated NB Model: Vuong test</a></li>
<li class="chapter" data-level="18.16.4" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#comparing-the-hurdle-models-with-aic-and-bic"><i class="fa fa-check"></i><b>18.16.4</b> Comparing the Hurdle Models with AIC and BIC</a></li>
<li class="chapter" data-level="18.16.5" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-fitted-equation-8"><i class="fa fa-check"></i><b>18.16.5</b> The Fitted Equation</a></li>
<li class="chapter" data-level="18.16.6" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#interpreting-the-coefficients-8"><i class="fa fa-check"></i><b>18.16.6</b> Interpreting the Coefficients</a></li>
<li class="chapter" data-level="18.16.7" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#testing-the-predictors-8"><i class="fa fa-check"></i><b>18.16.7</b> Testing the Predictors</a></li>
<li class="chapter" data-level="18.16.8" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#store-fitted-values-and-residuals-8"><i class="fa fa-check"></i><b>18.16.8</b> Store fitted values and residuals</a></li>
<li class="chapter" data-level="18.16.9" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#rootogram-for-nb-hurdle-model"><i class="fa fa-check"></i><b>18.16.9</b> Rootogram for NB Hurdle Model</a></li>
<li class="chapter" data-level="18.16.10" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#specify-the-r2-and-log-likelihood-values-3"><i class="fa fa-check"></i><b>18.16.10</b> Specify the R<sup>2</sup> and log (likelihood) values</a></li>
<li class="chapter" data-level="18.16.11" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#check-model-assumptions-8"><i class="fa fa-check"></i><b>18.16.11</b> Check model assumptions</a></li>
<li class="chapter" data-level="18.16.12" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#predictions-for-harry-and-sally-8"><i class="fa fa-check"></i><b>18.16.12</b> Predictions for Harry and Sally</a></li>
<li class="chapter" data-level="18.16.13" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#note-fitting-a-different-hurdle-model-for-counts-and-przero"><i class="fa fa-check"></i><b>18.16.13</b> Note: Fitting a Different Hurdle Model for Counts and Pr(zero)</a></li>
<li class="chapter" data-level="18.16.14" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#hanging-rootogram-for-this-new-hurdle-model"><i class="fa fa-check"></i><b>18.16.14</b> Hanging Rootogram for this new Hurdle Model</a></li>
</ul></li>
<li class="chapter" data-level="18.17" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#a-tobit-censored-regression-model"><i class="fa fa-check"></i><b>18.17</b> A Tobit (Censored) Regression Model</a><ul>
<li class="chapter" data-level="18.17.1" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#the-fitted-equation-9"><i class="fa fa-check"></i><b>18.17.1</b> The Fitted Equation</a></li>
<li class="chapter" data-level="18.17.2" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#interpreting-the-coefficients-9"><i class="fa fa-check"></i><b>18.17.2</b> Interpreting the Coefficients</a></li>
<li class="chapter" data-level="18.17.3" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#testing-the-predictors-9"><i class="fa fa-check"></i><b>18.17.3</b> Testing the Predictors</a></li>
<li class="chapter" data-level="18.17.4" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#store-fitted-values-and-residuals-9"><i class="fa fa-check"></i><b>18.17.4</b> Store fitted values and residuals</a></li>
<li class="chapter" data-level="18.17.5" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#building-something-like-a-rootogram"><i class="fa fa-check"></i><b>18.17.5</b> Building Something Like a Rootogram</a></li>
<li class="chapter" data-level="18.17.6" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#tables-of-the-observed-and-fitted-physhealth-from-tobit"><i class="fa fa-check"></i><b>18.17.6</b> Tables of the Observed and Fitted <code>physhealth</code> from Tobit</a></li>
<li class="chapter" data-level="18.17.7" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#specify-the-r2-and-log-likelihood-values-4"><i class="fa fa-check"></i><b>18.17.7</b> Specify the R<sup>2</sup> and log (likelihood) values</a></li>
<li class="chapter" data-level="18.17.8" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#check-model-assumptions-9"><i class="fa fa-check"></i><b>18.17.8</b> Check model assumptions</a></li>
<li class="chapter" data-level="18.17.9" data-path="modeling-a-count-outcome-in-ohio-smart.html"><a href="modeling-a-count-outcome-in-ohio-smart.html#predictions-for-harry-and-sally-9"><i class="fa fa-check"></i><b>18.17.9</b> Predictions for Harry and Sally</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><i class="fa fa-check"></i><b>19</b> Modeling an Ordinal Categorical Outcome in Ohio SMART</a><ul>
<li class="chapter" data-level="19.1" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#preliminaries-1"><i class="fa fa-check"></i><b>19.1</b> Preliminaries</a></li>
<li class="chapter" data-level="19.2" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#a-subset-of-the-ohio-smart-data-1"><i class="fa fa-check"></i><b>19.2</b> A subset of the Ohio SMART data</a><ul>
<li class="chapter" data-level="19.2.1" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#several-ways-of-storing-multi-categorical-data"><i class="fa fa-check"></i><b>19.2.1</b> Several Ways of Storing Multi-Categorical data</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#building-cross-tabulations"><i class="fa fa-check"></i><b>19.3</b> Building Cross-Tabulations</a><ul>
<li class="chapter" data-level="19.3.1" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#using-base-table-functions"><i class="fa fa-check"></i><b>19.3.1</b> Using base <code>table</code> functions</a></li>
<li class="chapter" data-level="19.3.2" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#using-xtabs"><i class="fa fa-check"></i><b>19.3.2</b> Using <code>xtabs</code></a></li>
<li class="chapter" data-level="19.3.3" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#storing-a-table-in-a-tibble"><i class="fa fa-check"></i><b>19.3.3</b> Storing a table in a tibble</a></li>
<li class="chapter" data-level="19.3.4" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#using-crosstable-from-the-gmodels-package"><i class="fa fa-check"></i><b>19.3.4</b> Using <code>CrossTable</code> from the <code>gmodels</code> package</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#graphing-categorical-data"><i class="fa fa-check"></i><b>19.4</b> Graphing Categorical Data</a><ul>
<li class="chapter" data-level="19.4.1" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#a-bar-chart-for-a-single-variable"><i class="fa fa-check"></i><b>19.4.1</b> A Bar Chart for a Single Variable</a></li>
<li class="chapter" data-level="19.4.2" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#a-counts-chart-for-a-2-way-cross-tabulation"><i class="fa fa-check"></i><b>19.4.2</b> A Counts Chart for a 2-Way Cross-Tabulation</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#building-a-model-for-genh-using-sleephrs"><i class="fa fa-check"></i><b>19.5</b> Building a Model for <code>genh</code> using <code>sleephrs</code></a><ul>
<li class="chapter" data-level="19.5.1" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#a-little-eda"><i class="fa fa-check"></i><b>19.5.1</b> A little EDA</a></li>
<li class="chapter" data-level="19.5.2" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#describing-the-proportional-odds-cumulative-logit-model"><i class="fa fa-check"></i><b>19.5.2</b> Describing the Proportional-Odds Cumulative Logit Model</a></li>
<li class="chapter" data-level="19.5.3" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#fitting-a-proportional-odds-logistic-regression-with-polr"><i class="fa fa-check"></i><b>19.5.3</b> Fitting a Proportional Odds Logistic Regression with <code>polr</code></a></li>
</ul></li>
<li class="chapter" data-level="19.6" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#interpreting-model-m1"><i class="fa fa-check"></i><b>19.6</b> Interpreting Model <code>m1</code></a><ul>
<li class="chapter" data-level="19.6.1" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#looking-at-predictions"><i class="fa fa-check"></i><b>19.6.1</b> Looking at Predictions</a></li>
<li class="chapter" data-level="19.6.2" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#making-predictions-for-harry-and-sally-with-predict"><i class="fa fa-check"></i><b>19.6.2</b> Making Predictions for Harry (and Sally) with <code>predict</code></a></li>
<li class="chapter" data-level="19.6.3" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#predicting-the-actual-classification-of-genh"><i class="fa fa-check"></i><b>19.6.3</b> Predicting the actual classification of <code>genh</code></a></li>
<li class="chapter" data-level="19.6.4" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#a-cross-tabuation-of-predictions"><i class="fa fa-check"></i><b>19.6.4</b> A Cross-Tabuation of Predictions?</a></li>
<li class="chapter" data-level="19.6.5" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#the-fitted-model-equations"><i class="fa fa-check"></i><b>19.6.5</b> The Fitted Model Equations</a></li>
<li class="chapter" data-level="19.6.6" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#interpreting-the-sleephrs-coefficient"><i class="fa fa-check"></i><b>19.6.6</b> Interpreting the <code>sleephrs</code> coefficient</a></li>
<li class="chapter" data-level="19.6.7" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#exponentiating-the-slope-coefficient-to-facilitate-interpretation"><i class="fa fa-check"></i><b>19.6.7</b> Exponentiating the Slope Coefficient to facilitate Interpretation</a></li>
<li class="chapter" data-level="19.6.8" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#comparison-to-a-null-model-4"><i class="fa fa-check"></i><b>19.6.8</b> Comparison to a Null Model</a></li>
</ul></li>
<li class="chapter" data-level="19.7" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#the-assumption-of-proportional-odds"><i class="fa fa-check"></i><b>19.7</b> The Assumption of Proportional Odds</a><ul>
<li class="chapter" data-level="19.7.1" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#testing-the-proportional-odds-assumption"><i class="fa fa-check"></i><b>19.7.1</b> Testing the Proportional Odds Assumption</a></li>
</ul></li>
<li class="chapter" data-level="19.8" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#can-model-m1-be-fit-using-rms-tools"><i class="fa fa-check"></i><b>19.8</b> Can model <code>m1</code> be fit using <code>rms</code> tools?</a></li>
<li class="chapter" data-level="19.9" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#building-a-three-predictor-model"><i class="fa fa-check"></i><b>19.9</b> Building a Three-Predictor Model</a><ul>
<li class="chapter" data-level="19.9.1" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#scatterplot-matrix"><i class="fa fa-check"></i><b>19.9.1</b> Scatterplot Matrix</a></li>
<li class="chapter" data-level="19.9.2" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#our-three-predictor-model-m2"><i class="fa fa-check"></i><b>19.9.2</b> Our Three-Predictor Model, <code>m2</code></a></li>
<li class="chapter" data-level="19.9.3" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#does-the-three-predictor-model-outperform-m1"><i class="fa fa-check"></i><b>19.9.3</b> Does the three-predictor model outperform <code>m1</code>?</a></li>
<li class="chapter" data-level="19.9.4" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#wald-tests-for-individual-predictors"><i class="fa fa-check"></i><b>19.9.4</b> Wald tests for individual predictors</a></li>
<li class="chapter" data-level="19.9.5" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#a-cross-tabuation-of-predictions-1"><i class="fa fa-check"></i><b>19.9.5</b> A Cross-Tabuation of Predictions?</a></li>
<li class="chapter" data-level="19.9.6" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#interpreting-the-effect-sizes"><i class="fa fa-check"></i><b>19.9.6</b> Interpreting the Effect Sizes</a></li>
<li class="chapter" data-level="19.9.7" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#quality-of-the-model-fit"><i class="fa fa-check"></i><b>19.9.7</b> Quality of the Model Fit</a></li>
<li class="chapter" data-level="19.9.8" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#validating-the-summary-statistics-in-m2_lrm"><i class="fa fa-check"></i><b>19.9.8</b> Validating the Summary Statistics in <code>m2_lrm</code></a></li>
<li class="chapter" data-level="19.9.9" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#testing-the-proportional-odds-assumption-1"><i class="fa fa-check"></i><b>19.9.9</b> Testing the Proportional Odds Assumption</a></li>
<li class="chapter" data-level="19.9.10" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#plotting-the-fitted-model"><i class="fa fa-check"></i><b>19.9.10</b> Plotting the Fitted Model</a></li>
</ul></li>
<li class="chapter" data-level="19.10" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#a-larger-model-including-income-group"><i class="fa fa-check"></i><b>19.10</b> A Larger Model, including income group</a><ul>
<li class="chapter" data-level="19.10.1" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#cross-tabulation-of-predictedobserved-classifications"><i class="fa fa-check"></i><b>19.10.1</b> Cross-Tabulation of Predicted/Observed Classifications</a></li>
<li class="chapter" data-level="19.10.2" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#nomogram-1"><i class="fa fa-check"></i><b>19.10.2</b> Nomogram</a></li>
<li class="chapter" data-level="19.10.3" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#using-predict-and-showing-mean-prediction-on-1-5-scale-1"><i class="fa fa-check"></i><b>19.10.3</b> Using Predict and showing mean prediction on 1-5 scale</a></li>
<li class="chapter" data-level="19.10.4" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#validating-the-summary-statistics-in-m3_lrm"><i class="fa fa-check"></i><b>19.10.4</b> Validating the Summary Statistics in <code>m3_lrm</code></a></li>
</ul></li>
<li class="chapter" data-level="19.11" data-path="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html"><a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html#references-for-this-chapter"><i class="fa fa-check"></i><b>19.11</b> References for this Chapter</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html"><i class="fa fa-check"></i><b>20</b> Analyzing Literary Styles with Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="20.1" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#the-authorship-example"><i class="fa fa-check"></i><b>20.1</b> The Authorship Example</a></li>
<li class="chapter" data-level="20.2" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#focus-on-11-key-words"><i class="fa fa-check"></i><b>20.2</b> Focus on 11 key words</a><ul>
<li class="chapter" data-level="20.2.1" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#side-by-side-boxplots"><i class="fa fa-check"></i><b>20.2.1</b> Side by Side Boxplots</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#a-multinomial-logistic-regression-model"><i class="fa fa-check"></i><b>20.3</b> A Multinomial Logistic Regression Model</a><ul>
<li class="chapter" data-level="20.3.1" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#testing-model-1"><i class="fa fa-check"></i><b>20.3.1</b> Testing Model 1</a></li>
</ul></li>
<li class="chapter" data-level="20.4" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#model-2"><i class="fa fa-check"></i><b>20.4</b> Model 2</a><ul>
<li class="chapter" data-level="20.4.1" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#comparing-model-2-to-model-1"><i class="fa fa-check"></i><b>20.4.1</b> Comparing Model 2 to Model 1</a></li>
<li class="chapter" data-level="20.4.2" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#testing-model-2"><i class="fa fa-check"></i><b>20.4.2</b> Testing Model 2</a></li>
<li class="chapter" data-level="20.4.3" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#a-little-history"><i class="fa fa-check"></i><b>20.4.3</b> A little history</a></li>
</ul></li>
<li class="chapter" data-level="20.5" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#classification-table"><i class="fa fa-check"></i><b>20.5</b> Classification Table</a></li>
<li class="chapter" data-level="20.6" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#probability-curves-based-on-a-single-predictor"><i class="fa fa-check"></i><b>20.6</b> Probability Curves based on a Single Predictor</a><ul>
<li class="chapter" data-level="20.6.1" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#produce-the-plot-of-estimated-probabilities-based-on-been-counts"><i class="fa fa-check"></i><b>20.6.1</b> Produce the Plot of Estimated Probabilities based on “been” counts</a></li>
<li class="chapter" data-level="20.6.2" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#boxplot-of-been-counts"><i class="fa fa-check"></i><b>20.6.2</b> Boxplot of “been” counts</a></li>
<li class="chapter" data-level="20.6.3" data-path="analyzing-literary-styles-with-multinomial-logistic-regression.html"><a href="analyzing-literary-styles-with-multinomial-logistic-regression.html#quote-sources"><i class="fa fa-check"></i><b>20.6.3</b> Quote Sources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science for Biological, Medical and Health Research: Notes for 432</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modeling-a-count-outcome-in-ohio-smart" class="section level1">
<h1><span class="header-section-number">Chapter 18</span> Modeling a Count Outcome in Ohio SMART</h1>
<p>In this chapter, I use a count outcome (# of poor physical health days out of the last 30) to demonstrate regression models for count outcomes.</p>
<p>Methods discussed in the chapter include the following:</p>
<ul>
<li>Ordinary Least Squares</li>
<li>Poisson Regression</li>
<li>Overdispersed Quasi-Poisson Regression</li>
<li>Negative Binomial Regression</li>
<li>Zero-Inflated Poisson Regression</li>
<li>Zero-Inflated Negative Binomial Regression</li>
<li>Hurdle Models with Poisson counts</li>
<li>Hurdle Models with Negative Binomial counts</li>
<li>Tobit Regression</li>
</ul>
<div id="preliminaries" class="section level2">
<h2><span class="header-section-number">18.1</span> Preliminaries</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(boot)</code></pre></div>
<pre><code>
Attaching package: &#39;boot&#39;</code></pre>
<pre><code>The following object is masked from &#39;package:survival&#39;:

    aml</code></pre>
<pre><code>The following object is masked from &#39;package:lattice&#39;:

    melanoma</code></pre>
<pre><code>The following object is masked from &#39;package:arm&#39;:

    logit</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lmtest)</code></pre></div>
<pre><code>Loading required package: zoo</code></pre>
<pre><code>
Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>The following objects are masked from &#39;package:base&#39;:

    as.Date, as.Date.numeric</code></pre>
<pre><code>
Attaching package: &#39;lmtest&#39;</code></pre>
<pre><code>The following object is masked from &#39;package:rms&#39;:

    lrtest</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sandwich)
<span class="kw">library</span>(countreg)
<span class="kw">library</span>(VGAM)</code></pre></div>
<pre><code>Loading required package: stats4</code></pre>
<pre><code>Loading required package: splines</code></pre>
<pre><code>
Attaching package: &#39;VGAM&#39;</code></pre>
<pre><code>The following objects are masked from &#39;package:countreg&#39;:

    dzipois, pzipois, qzipois, rzipois</code></pre>
<pre><code>The following object is masked from &#39;package:lmtest&#39;:

    lrtest</code></pre>
<pre><code>The following objects are masked from &#39;package:boot&#39;:

    logit, simplex</code></pre>
<pre><code>The following object is masked from &#39;package:tidyr&#39;:

    fill</code></pre>
<pre><code>The following objects are masked from &#39;package:rms&#39;:

    calibrate, lrtest</code></pre>
<pre><code>The following object is masked from &#39;package:arm&#39;:

    logit</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">smart_oh &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/smart_ohio.csv&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>tbl_df</code></pre></div>
</div>
<div id="a-subset-of-the-ohio-smart-data" class="section level2">
<h2><span class="header-section-number">18.2</span> A subset of the Ohio SMART data</h2>
<p>Let’s consider the following data. I’ll include the subset of all observations in <code>smart_oh</code> with complete data on these 14 variables.</p>
<table style="width:81%;">
<colgroup>
<col width="13%" />
<col width="66%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><code>SEQNO</code></td>
<td>Subject identification code</td>
</tr>
<tr class="even">
<td align="right"><code>MMSANAME</code></td>
<td>Name of statistical area</td>
</tr>
<tr class="odd">
<td align="right"><code>genhealth</code></td>
<td>Five categories (E, VG, G, F, P) on general health</td>
</tr>
<tr class="even">
<td align="right"><code>physhealth</code></td>
<td>Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?</td>
</tr>
<tr class="odd">
<td align="right"><code>menthlth</code></td>
<td>Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good?</td>
</tr>
<tr class="even">
<td align="right"><code>healthplan</code></td>
<td>1 if the subject has any kind of health care coverage, 0 otherwise</td>
</tr>
<tr class="odd">
<td align="right"><code>costprob</code></td>
<td>1 indicates Yes to “Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?”</td>
</tr>
<tr class="even">
<td align="right"><code>sleephrs</code></td>
<td>average amount of sleep the subject gets in a 24-hour period</td>
</tr>
<tr class="odd">
<td align="right"><code>agegroup</code></td>
<td>13 age groups from 18 through 80+</td>
</tr>
<tr class="even">
<td align="right"><code>female</code></td>
<td>1 if subject is female</td>
</tr>
<tr class="odd">
<td align="right"><code>incomegroup</code></td>
<td>8 income groups from &lt; 10,000 to 75,000 or more</td>
</tr>
<tr class="even">
<td align="right"><code>bmi</code></td>
<td>body-mass index</td>
</tr>
<tr class="odd">
<td align="right"><code>smoke100</code></td>
<td>1 if Yes to “Have you smoked at least 100 cigarettes in your entire life?”</td>
</tr>
<tr class="even">
<td align="right"><code>alcdays</code></td>
<td># of days out of the past 30 on which the subject had at least one alcoholic drink</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_oh_A &lt;-<span class="st"> </span>smart_oh <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">select</span>(SEQNO, MMSANAME, genhealth, physhealth, 
           menthealth, healthplan, costprob, sleephrs, 
           agegroup, female, incomegroup, bmi, smoke100, 
           alcdays) <span class="op">%&gt;%</span>
<span class="st">    </span>drop_na</code></pre></div>
<div id="is-age-group-associated-with-physhealth" class="section level3">
<h3><span class="header-section-number">18.2.1</span> Is age group associated with <code>physhealth</code>?</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sm_oh_A, <span class="kw">aes</span>(<span class="dt">x =</span> agegroup, <span class="dt">y =</span> physhealth)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_violin</span>(<span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-327-1.png" width="672" /></p>
<p>It’s hard to see much of anything here. The main conclusion seems to be that 0 is by far the most common response.</p>
<p>Here’s a table by age group of:</p>
<ul>
<li>the number of respondents in that age group,</li>
<li>the group’s mean <code>physhealth</code> response (remember that these are the number of poor physical health days in the last 30),</li>
<li>their median <code>physhealth</code> response (which turns out to be 0 in each group), and</li>
<li>the percentage of group members who responded 0.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_oh_A <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(agegroup) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">n =</span> <span class="kw">n</span>(), <span class="dt">mean =</span> <span class="kw">round</span>(<span class="kw">mean</span>(physhealth),<span class="dv">2</span>), 
              <span class="dt">median =</span> <span class="kw">median</span>(physhealth),
              <span class="dt">percent_0s =</span> <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span><span class="kw">sum</span>(physhealth <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)<span class="op">/</span>n,<span class="dv">1</span>))</code></pre></div>
<pre><code># A tibble: 13 x 5
   agegroup     n  mean median percent_0s
   &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;
 1 18-24      243  1.91     0.       65.4
 2 25-29      220  2.39     0.       66.4
 3 30-34      282  3.11     0.       68.8
 4 35-39      241  2.95     0.       70.1
 5 40-44      285  2.91     0.       66.0
 6 45-49      364  2.95     0.       69.8
 7 50-54      403  5.11     0.       61.5
 8 55-59      510  5.37     0.       57.3
 9 60-64      515  4.33     0.       61.2
10 65-69      550  4.34     0.       64.2
11 70-74      349  5.38     0.       64.5
12 75-79      270  5.54     0.       60.7
13 80-96      320  5.68     0.       61.6</code></pre>
<p>We can see a real change between the 45-49 age group and the 50-54 age group. The mean difference is clear from the table above, and the plot below (of the percentage with a zero response) in each age group identifies the same story.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_oh_A <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(agegroup) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">n =</span> <span class="kw">n</span>(), 
              <span class="dt">percent_0s =</span> <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span><span class="kw">sum</span>(physhealth <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)<span class="op">/</span>n,<span class="dv">1</span>)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> agegroup, <span class="dt">x =</span> percent_0s)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_label</span>(<span class="kw">aes</span>(<span class="dt">label =</span> percent_0s)) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;% with no Bad Physical Health Days in last 30&quot;</span>,
         <span class="dt">y =</span> <span class="st">&quot;Age Group&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-329-1.png" width="672" /></p>
<p>It looks like we have a fairly consistent result in the younger age range (18-49) or the older range (50+). On the theory that most of the people reading this document are in that younger range, we’ll focus on those respondents in what follows.</p>
</div>
</div>
<div id="exploratory-data-analysis-in-the-18-49-group" class="section level2">
<h2><span class="header-section-number">18.3</span> Exploratory Data Analysis (in the 18-49 group)</h2>
<p>We want to predict the 0-30 <code>physhealth</code> count variable for the 18-49 year old respondents.</p>
<p>To start, we’ll use two predictors:</p>
<ul>
<li>the respondent’s body mass index, and</li>
<li>whether the respondent has smoked 100 cigarettes in their lifetime.</li>
</ul>
<p>We anticipate that each of these variables will have positive associations with the <code>physhealth</code> score. That is, heavier people, and those who have used tobacco will be less healthy, and thus have higher numbers of poor physical health days.</p>
<div id="build-a-subset-of-those-ages-18-49" class="section level3">
<h3><span class="header-section-number">18.3.1</span> Build a subset of those ages 18-49</h3>
<p>First, we’ll identify the subset of respondents who are between 18 and 49 years of age.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_oh_A_young.raw &lt;-<span class="st"> </span>sm_oh_A <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">filter</span>(agegroup <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;18-24&quot;</span>, <span class="st">&quot;25-29&quot;</span>, <span class="st">&quot;30-34&quot;</span>, 
                           <span class="st">&quot;35-39&quot;</span>, <span class="st">&quot;40-44&quot;</span>, <span class="st">&quot;45-49&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">droplevels</span>()

sm_oh_A_young.raw <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">select</span>(physhealth, bmi, smoke100, agegroup) <span class="op">%&gt;%</span>
<span class="st">    </span>summary</code></pre></div>
<pre><code>   physhealth          bmi            smoke100       agegroup  
 Min.   : 0.000   Min.   : 12.71   Min.   :0.0000   18-24:243  
 1st Qu.: 0.000   1st Qu.: 23.50   1st Qu.:0.0000   25-29:220  
 Median : 0.000   Median : 26.69   Median :0.0000   30-34:282  
 Mean   : 2.741   Mean   : 28.21   Mean   :0.3676   35-39:241  
 3rd Qu.: 2.000   3rd Qu.: 31.39   3rd Qu.:1.0000   40-44:285  
 Max.   :30.000   Max.   :110.88   Max.   :1.0000   45-49:364  </code></pre>
<p>Now, let’s look more closely at the distribution of these variables, starting with our outcome.</p>
</div>
<div id="distribution-of-the-outcome" class="section level3">
<h3><span class="header-section-number">18.3.2</span> Distribution of the Outcome</h3>
<p>What’s the distribution of <code>physhealth</code>?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sm_oh_A_young.raw, <span class="kw">aes</span>(<span class="dt">x =</span> physhealth)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">1</span>, <span class="dt">fill =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;white&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-331-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_oh_A_young.raw <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">count</span>(physhealth <span class="op">==</span><span class="st"> </span><span class="dv">0</span>, physhealth <span class="op">==</span><span class="st"> </span><span class="dv">30</span>)</code></pre></div>
<pre><code># A tibble: 3 x 3
  `physhealth == 0` `physhealth == 30`     n
  &lt;lgl&gt;             &lt;lgl&gt;              &lt;int&gt;
1 FALSE             FALSE                459
2 FALSE             TRUE                  66
3 TRUE              FALSE               1110</code></pre>
<p>Most of our respondents said zero, the minimum allowable value, although there is also a much smaller bump at 30, the maximum value we will allow.</p>
<p>Dealing with this distribution is going to be a bit of a challenge. We will develop a series of potential modeling approaches for this sort of data, but before we do that, let’s look at the distribution of our other two variables, and the pairwise associations, in a scatterplot matrix.</p>
</div>
<div id="initial-scatterplot-matrix" class="section level3">
<h3><span class="header-section-number">18.3.3</span> Initial Scatterplot Matrix</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">GGally<span class="op">::</span><span class="kw">ggpairs</span>(sm_oh_A_young.raw <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">                    </span><span class="kw">select</span>(bmi, smoke100, physhealth))</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-333-1.png" width="672" /></p>
</div>
<div id="dropping-the-subject-with-an-unreasonably-large-bmi" class="section level3">
<h3><span class="header-section-number">18.3.4</span> Dropping the subject with an unreasonably large <code>bmi</code></h3>
<p>It looks like there is a very large outlier in the <code>bmi</code> data. This subject appears to be a full 40 kg/m<sup>2</sup> larger than the next largest subject. Without a good explanation for that discrepancy, I will remove that subject before fitting models. I’m also going to center the <code>bmi</code> variable to help me interpret the final models later.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_oh_A_young &lt;-<span class="st"> </span>sm_oh_A_young.raw <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">bmi_c =</span> bmi <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(bmi)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">filter</span>(bmi <span class="op">&lt;</span><span class="st"> </span><span class="dv">80</span>)

<span class="kw">nrow</span>(sm_oh_A_young)</code></pre></div>
<pre><code>[1] 1634</code></pre>
</div>
<div id="revised-final-scatterplot-matrix" class="section level3">
<h3><span class="header-section-number">18.3.5</span> Revised (Final) Scatterplot Matrix</h3>
<p>Now, here’s the revised scatterplot matrix for those 1634 subjects, now using the centered <code>bmi</code> data captured in the <code>bmi_c</code> variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">GGally<span class="op">::</span><span class="kw">ggpairs</span>(sm_oh_A_young <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">                    </span><span class="kw">select</span>(bmi_c, smoke100, physhealth))</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-335-1.png" width="672" /></p>
<p>So <code>bmi_c</code> and <code>smoke100</code> each have modest positive correlations with <code>physhealth</code> and only a very small correlation with each other. Here are some summary statistics for this final data.</p>
</div>
<div id="summary-of-the-final-subset-of-data" class="section level3">
<h3><span class="header-section-number">18.3.6</span> Summary of the final subset of data</h3>
<p>Remember that since the mean of <code>bmi</code> is 28.2, the <code>bmi_c</code> values are just <code>bmi</code> - 28.2 for each subject.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_oh_A_young <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">select</span>(bmi, bmi_c, smoke100, physhealth) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">skim</span>()</code></pre></div>
<pre><code>Skim summary statistics
 n obs: 1634 
 n variables: 4 

Variable type: integer 
   variable missing complete    n mean   sd p0 p25 median p75 p100
 physhealth       0     1634 1634 2.74 6.77  0   0      0   2   30
   smoke100       0     1634 1634 0.37 0.48  0   0      0   1    1

Variable type: numeric 
 variable missing complete    n   mean   sd     p0   p25 median   p75
      bmi       0     1634 1634 28.16  6.84  12.71 23.5   26.69 31.39
    bmi_c       0     1634 1634 -0.051 6.84 -15.5  -4.72  -1.53  3.18
  p100
 66.06
 37.85</code></pre>
</div>
</div>
<div id="modeling-strategies-explored-here" class="section level2">
<h2><span class="header-section-number">18.4</span> Modeling Strategies Explored Here</h2>
<p>We are going to predict <code>physhealth</code> using <code>bmi_c</code> and <code>smoke100</code>.</p>
<ul>
<li>Remember that <code>physhealth</code> is a count of the number of poor physical health days in the past 30.</li>
<li>As a result, <code>physhealth</code> is restricted to taking values between 0 and 30.</li>
</ul>
<p>We will demonstrate the use of each of the following regression models, some of which are better choices than others.</p>
<ol style="list-style-type: decimal">
<li>Ordinary Least Squares (OLS) predicting <code>physhealth</code></li>
<li>OLS predicting the logarithm of (<code>physhealth</code> + 1)</li>
<li>Poisson regression, which is appropriate for predicting counts</li>
<li>Poisson regression, adjusted to account for overdispersion</li>
<li>Negative binomial regression, also used for counts and which adjusts for overdispersion</li>
<li>Zero-inflated models, in both the Poisson and Negative Binomial varieties, which allow us to fit counts that have lots of zero values</li>
<li>A “hurdle” model, which allows us to separately fit a model to predict the incidence of “0” and then a separate model to predict the value of <code>physhealth</code> when we know it is not zero</li>
<li>Tobit regression, where a lower (and upper) bound may be set, but the underlying model describes a latent variable which can extend beyond these boundaries</li>
</ol>
<div id="what-will-we-demonstrate" class="section level3">
<h3><span class="header-section-number">18.4.1</span> What Will We Demonstrate?</h3>
<p>With each approach, we will fit the model and specify procedures for doing so in R. Then we will:</p>
<ol style="list-style-type: decimal">
<li>Specify the fitted model equation</li>
<li>Interpret the model’s coefficient estimates and 95% confidence intervals around those estimates.</li>
<li>Perform a test of whether each variable adds value to the model, when the other one is already included.</li>
<li>Store the fitted values and appropriate residuals for each model.</li>
<li>Summarize the model’s apparent R<sup>2</sup> value, the proportion of variation explained, and the model log likelihood.</li>
<li>Perform checks of model assumptions as appropriate.</li>
<li>Describe how predictions would be made for two new subjects.
<ul>
<li>Harry has a BMI that is 10 kg/m<sup>2</sup> higher than the average across all respondents and has smoked more than 100 cigarettes in his life.</li>
<li>Sally has a BMI that is 5 kg/m<sup>2</sup> less than the average across all respondents and has not smoked more than 100 cigarettes in her life.</li>
</ul></li>
</ol>
<p>In addition, for some of the new models, we provide a little of the mathematical background, and point to other resources you can use to learn more about the model.</p>
</div>
<div id="extra-data-file-for-harry-and-sally" class="section level3">
<h3><span class="header-section-number">18.4.2</span> Extra Data File for Harry and Sally</h3>
<p>To make our lives a little easier, I’ll create a little tibble containing the necessary data for Harry and Sally.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hs_data &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">subj =</span> <span class="kw">c</span>(<span class="st">&quot;Harry&quot;</span>, <span class="st">&quot;Sally&quot;</span>),
                      <span class="dt">bmi_c =</span> <span class="kw">c</span>(<span class="dv">10</span>, <span class="op">-</span><span class="dv">5</span>),
                      <span class="dt">smoke100 =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>))
hs_data</code></pre></div>
<pre><code># A tibble: 2 x 3
  subj  bmi_c smoke100
  &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;
1 Harry   10.       1.
2 Sally   -5.       0.</code></pre>
</div>
</div>
<div id="the-ols-approach" class="section level2">
<h2><span class="header-section-number">18.5</span> The OLS Approach</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_ols1 &lt;-<span class="st"> </span><span class="kw">lm</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100, 
               <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_ols1)</code></pre></div>
<pre><code>
Call:
lm(formula = physhealth ~ bmi_c + smoke100, data = sm_oh_A_young)

Residuals:
   Min     1Q Median     3Q    Max 
-8.320 -3.119 -1.715 -0.722 29.364 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.00443    0.20576   9.741  &lt; 2e-16 ***
bmi_c        0.15283    0.02394   6.385 2.23e-10 ***
smoke100     2.03065    0.33978   5.976 2.80e-09 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 6.612 on 1631 degrees of freedom
Multiple R-squared:  0.04711,   Adjusted R-squared:  0.04595 
F-statistic: 40.32 on 2 and 1631 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(mod_ols1)</code></pre></div>
<pre><code>                2.5 %    97.5 %
(Intercept) 1.6008443 2.4080211
bmi_c       0.1058826 0.1997872
smoke100    1.3642003 2.6971050</code></pre>
<div id="the-fitted-equation" class="section level3">
<h3><span class="header-section-number">18.5.1</span> The Fitted Equation</h3>
<p>The OLS fitted model equation is</p>
<pre><code>physhealth = 2.00 + 0.15 bmi_c + 2.03 smoke100</code></pre>
</div>
<div id="interpreting-the-coefficients" class="section level3">
<h3><span class="header-section-number">18.5.2</span> Interpreting the Coefficients</h3>
<ul>
<li>The intercept, 2.00, is the predicted <code>physhealth</code> (in days) for a subject with average BMI who has not smoked 100 cigarettes or more.</li>
<li>The <code>bmi_c</code> coefficient, 0.15, indicates that for each additional kg/m<sup>2</sup> of BMI, while holding <code>smoke100</code> constant, the predicted <code>physhealth</code> value increases by 0.15 day.</li>
<li>The <code>smoke100</code> coefficient, 2.03, indicates that a subject who has smoked 100 cigarettes or more has a predicted <code>physhealth</code> value 2.03 days larger than another subject with the same <code>bmi</code> but who has not smoked 100 cigarettes.</li>
</ul>
</div>
<div id="testing-the-predictors" class="section level3">
<h3><span class="header-section-number">18.5.3</span> Testing the Predictors</h3>
<p>We can use the t test results provided above to assess the significance of each term, after the other is already included in the model. Each <em>p</em> value is well below our usual <span class="math inline">\(\alpha\)</span> levels, so we conclude that each predictor adds statistically detectable predictive value to the model.</p>
</div>
<div id="store-fitted-values-and-residuals" class="section level3">
<h3><span class="header-section-number">18.5.4</span> Store fitted values and residuals</h3>
<p>We can use <code>broom</code> to do this. Here, for instance, is a table of the first six predictions and residuals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_ols_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">augment</span>(mod_ols1, sm_oh_A_young)

sm_ols_<span class="dv">1</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(physhealth, .fitted, .resid) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>()</code></pre></div>
<pre><code>  physhealth  .fitted     .resid
1          0 1.767314 -1.7673143
2          0 2.912048 -2.9120475
3          0 1.619064 -1.6190645
4          2 2.219706 -0.2197055
5          4 5.427710 -1.4277096
6          6 1.397454  4.6025461</code></pre>
<p>It turns out that 3 of the 1634 predictions that we make are below 0, and the largest prediction made by this model is 9.82 days.</p>
</div>
<div id="specify-the-r2-and-loglikelihood-values" class="section level3">
<h3><span class="header-section-number">18.5.5</span> Specify the R<sup>2</sup> and log(likelihood) values</h3>
<p>The <code>glance</code> function in the <code>broom</code> package gives us the raw and adjusted R<sup>2</sup> values, and the model log(likelihood), among other summaries.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glance</span>(mod_ols1) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">3</span>)</code></pre></div>
<pre><code>  r.squared adj.r.squared sigma statistic p.value df    logLik      AIC
1     0.047         0.046 6.612    40.321       0  3 -5403.463 10814.93
       BIC deviance df.residual
1 10836.52 71303.07        1631</code></pre>
<p>Here, we have</p>
<table>
<thead>
<tr class="header">
<th align="right">Model</th>
<th align="right">R<sup>2</sup></th>
<th align="right">log(likelihood)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">OLS</td>
<td align="right">.047</td>
<td align="right">-5409.3</td>
</tr>
</tbody>
</table>
</div>
<div id="check-model-assumptions" class="section level3">
<h3><span class="header-section-number">18.5.6</span> Check model assumptions</h3>
<p>Here is a plot of the residuals vs. the fitted values for this OLS model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sm_ols_<span class="dv">1</span>, <span class="kw">aes</span>(<span class="dt">x =</span> .fitted, <span class="dt">y =</span> .resid)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Residuals vs. Fitted Values for OLS model&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-341-1.png" width="672" /></p>
<p>As usual, we can check OLS assumptions (linearity, homoscedasticity and normality) with R’s complete set of residual plots.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(mod_ols1)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-342-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>We see the problem with our residuals. They don’t follow a Normal distribution.</p>
</div>
<div id="predictions-for-harry-and-sally" class="section level3">
<h3><span class="header-section-number">18.5.7</span> Predictions for Harry and Sally</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(mod_ols1, <span class="dt">newdata =</span> hs_data,
        <span class="dt">interval =</span> <span class="st">&quot;prediction&quot;</span>)</code></pre></div>
<pre><code>       fit        lwr      upr
1 5.563434  -7.423906 18.55077
2 1.240258 -11.736608 14.21712</code></pre>
<p>The prediction for Harry is 5.6 days, and for Sally is 1.2 days. The prediction intervals for each include some values below 0, which is the smallest possible value.</p>
</div>
<div id="notes" class="section level3">
<h3><span class="header-section-number">18.5.8</span> Notes</h3>
<ul>
<li>This model could have been estimated using the <code>ols</code> function in the <code>rms</code> package, as well.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dd &lt;-<span class="st"> </span><span class="kw">datadist</span>(sm_oh_A_young)
<span class="kw">options</span>(<span class="dt">datadist =</span> <span class="st">&quot;dd&quot;</span>)

(mod_ols1a &lt;-<span class="st"> </span><span class="kw">ols</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100,
                 <span class="dt">data =</span> sm_oh_A_young, <span class="dt">x =</span> <span class="ot">TRUE</span>, <span class="dt">y =</span> <span class="ot">TRUE</span>))</code></pre></div>
<pre><code>Linear Regression Model
 
 ols(formula = physhealth ~ bmi_c + smoke100, data = sm_oh_A_young, 
     x = TRUE, y = TRUE)
 
                Model Likelihood     Discrimination    
                   Ratio Test           Indexes        
 Obs    1634    LR chi2     78.86    R2       0.047    
 sigma6.6119    d.f.            2    R2 adj   0.046    
 d.f.   1631    Pr(&gt; chi2) 0.0000    g        1.641    
 
 Residuals
 
    Min     1Q Median     3Q    Max 
 -8.320 -3.119 -1.715 -0.722 29.364 
 
 
           Coef   S.E.   t    Pr(&gt;|t|)
 Intercept 2.0044 0.2058 9.74 &lt;0.0001 
 bmi_c     0.1528 0.0239 6.38 &lt;0.0001 
 smoke100  2.0307 0.3398 5.98 &lt;0.0001 
 </code></pre>
</div>
</div>
<div id="ols-model-on-logphyshealth-1-days" class="section level2">
<h2><span class="header-section-number">18.6</span> OLS model on log(<code>physhealth</code> + 1) days</h2>
<p>We could try to solve the problem of fitting some predictions below 0 by log-transforming the data, so as to force values to be at least 0. Since we have undefined values when we take the log of 0, we’ll add one to each of the <code>physhealth</code> values before taking logs, and then transform back when we want to make predictions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_ols_log1 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(physhealth <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100,
                   <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_ols_log1)</code></pre></div>
<pre><code>
Call:
lm(formula = log(physhealth + 1) ~ bmi_c + smoke100, data = sm_oh_A_young)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.4934 -0.6119 -0.3893  0.3721  3.1911 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.477789   0.030053  15.898  &lt; 2e-16 ***
bmi_c       0.026244   0.003496   7.506 9.96e-14 ***
smoke100    0.279783   0.049627   5.638 2.03e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.9657 on 1631 degrees of freedom
Multiple R-squared:  0.05381,   Adjusted R-squared:  0.05265 
F-statistic: 46.38 on 2 and 1631 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(mod_ols_log1)</code></pre></div>
<pre><code>                 2.5 %     97.5 %
(Intercept) 0.41884235 0.53673474
bmi_c       0.01938609 0.03310135
smoke100    0.18244395 0.37712164</code></pre>
<div id="the-fitted-equation-1" class="section level3">
<h3><span class="header-section-number">18.6.1</span> The Fitted Equation</h3>
<p>The equation here is</p>
<pre><code>log(physhealth + 1) = 0.48 + 0.026 bmi_c + 0.28 smoke100</code></pre>
</div>
<div id="interpreting-the-coefficients-1" class="section level3">
<h3><span class="header-section-number">18.6.2</span> Interpreting the Coefficients</h3>
<ul>
<li>The intercept, 0.48, is the predicted logarithm of (<code>physhealth</code> + 1) (in days) for a subject with average BMI who has not smoked 100 cigarettes or more.
<ul>
<li>We can exponentiate to see that the prediction for (<code>physhealth</code> + 1) here is <code>exp(0.48)</code> = 1.62 so the predicted <code>physhealth</code> for a subject with average BMI who has not smoked 100 cigarettes is 0.62 days.</li>
</ul></li>
<li>The <code>bmi_c</code> coefficient, 0.15, indicates that for each additional kg/m<sup>2</sup> of BMI, while holding <code>smoke100</code> constant, the predicted logarithm of (<code>physhealth</code> + 1) increases by 0.026</li>
<li>The <code>smoke100</code> coefficient, 0.28, indicates that a subject who has smoked 100 cigarettes or more has a predicted log of (<code>physhealth</code> + 1) value that is 0.28 larger than another subject with the same <code>bmi</code> but who has not smoked 100 cigarettes.</li>
</ul>
</div>
<div id="testing-the-predictors-1" class="section level3">
<h3><span class="header-section-number">18.6.3</span> Testing the Predictors</h3>
<p>We are still fitting an OLS model here, so we can still use the t test results provided above to assess the significance of each term, after the other is already included in the model. Each <em>p</em> value is well below our usual <span class="math inline">\(\alpha\)</span> levels, so we conclude that each predictor adds statistically detectable predictive value to the model.</p>
</div>
<div id="store-fitted-values-and-residuals-1" class="section level3">
<h3><span class="header-section-number">18.6.4</span> Store fitted values and residuals</h3>
<p>We can use <code>broom</code> to help us with this. Here, for instance, is a table of the first six predictions and residuals, on the scale of our transformed response, log(<code>physhealth</code> + 1).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_ols_log1 &lt;-<span class="st"> </span><span class="kw">augment</span>(mod_ols_log1, sm_oh_A_young)

sm_ols_log1 &lt;-<span class="st"> </span>sm_ols_log1 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">outcome =</span> <span class="kw">log</span>(physhealth <span class="op">+</span><span class="st"> </span><span class="dv">1</span>))

sm_ols_log1 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">select</span>(physhealth, outcome, .fitted, .resid) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">head</span>()</code></pre></div>
<pre><code>  physhealth  outcome   .fitted     .resid
1          0 0.000000 0.4370723 -0.4370723
2          0 0.000000 0.6336377 -0.6336377
3          0 0.000000 0.4116159 -0.4116159
4          2 1.098612 0.5147537  0.5838586
5          4 1.609438 1.0656093  0.5438286
6          6 1.945910 0.3735625  1.5723477</code></pre>
<p>Note that the <code>outcome</code> used in this model is log(<code>physhealth</code> + 1), so the <code>.fitted</code> and <code>.resid</code> values react to that outcome, and not to our original <code>physhealth</code>.</p>
<p>Another option would be to calculate the model-predicted <code>physhealth</code>, which I’ll call <em>ph</em> for a moment, with the formula:</p>
<p><span class="math display">\[
ph = e^{.fitted} - 1
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_ols_log1 &lt;-<span class="st"> </span>sm_ols_log1 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">pred.physhealth =</span> <span class="kw">exp</span>(.fitted) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>,
           <span class="dt">res.physhealth =</span> physhealth <span class="op">-</span><span class="st"> </span>pred.physhealth)

sm_ols_log1 <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">select</span>(physhealth, pred.physhealth, res.physhealth) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">head</span>()</code></pre></div>
<pre><code>  physhealth pred.physhealth res.physhealth
1          0       0.5481679     -0.5481679
2          0       0.8844532     -0.8844532
3          0       0.5092545     -0.5092545
4          2       0.6732263      1.3267737
5          4       1.9026070      2.0973930
6          6       0.4529013      5.5470987</code></pre>
<p>It turns out that 0 of the 1634 predictions that we make are below 0, and the largest prediction made by this model is 4.76 days.</p>
</div>
<div id="specify-the-r2-and-loglikelihood-values-1" class="section level3">
<h3><span class="header-section-number">18.6.5</span> Specify the R<sup>2</sup> and log(likelihood) values</h3>
<p>The <code>glance</code> function in the <code>broom</code> package gives us the raw and adjusted R<sup>2</sup> values, and the model log(likelihood), among other summaries.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glance</span>(mod_ols_log1) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">3</span>)</code></pre></div>
<pre><code>  r.squared adj.r.squared sigma statistic p.value df    logLik      AIC
1     0.054         0.053 0.966    46.376       0  3 -2260.022 4528.044
       BIC deviance df.residual
1 4549.639 1521.046        1631</code></pre>
<p>Here, we have</p>
<table>
<thead>
<tr class="header">
<th align="right">Model</th>
<th align="right">Scale</th>
<th align="right">R<sup>2</sup></th>
<th align="right">log(likelihood)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">OLS on log</td>
<td align="right">log(<code>physhealth</code> + 1)</td>
<td align="right">.054</td>
<td align="right">-2262.0</td>
</tr>
</tbody>
</table>
</div>
<div id="getting-r2-on-the-scale-of-physhealth" class="section level3">
<h3><span class="header-section-number">18.6.6</span> Getting R<sup>2</sup> on the scale of <code>physhealth</code></h3>
<p>We could find the correlation of our model-predicted <code>physhealth</code> values, after back-transformation, and our observed <code>physhealth</code> values, if we wanted to, and then square that to get a sort of R<sup>2</sup> value. But this model is not linear in <code>physhealth</code>, of course, so it’s not completely comparable to our prior OLS model.</p>
</div>
<div id="check-model-assumptions-1" class="section level3">
<h3><span class="header-section-number">18.6.7</span> Check model assumptions</h3>
<p>As usual, we can check OLS assumptions (linearity, homoscedasticity and normality) with R’s complete set of residual plots. Of course, these residuals and fitted values are now on the log(<code>physhealth</code> + 1) scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(mod_ols_log1)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-349-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
</div>
<div id="predictions-for-harry-and-sally-1" class="section level3">
<h3><span class="header-section-number">18.6.8</span> Predictions for Harry and Sally</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(mod_ols_log1, <span class="dt">newdata =</span> hs_data, 
        <span class="dt">interval =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre></div>
<pre><code>       fit        lwr      upr
1 1.020009 -0.8768604 2.916877
2 0.346570 -1.5487692 2.241909</code></pre>
<p>Again, these predictions are on the log(<code>physhealth</code> + 1) scale, and so we have to exponentiate them, and then subtract 1, to see them on the original <code>physhealth</code> scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">predict</span>(mod_ols_log1, <span class="dt">newdata =</span> hs_data, 
            <span class="dt">interval =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)) <span class="op">-</span><span class="st"> </span><span class="dv">1</span></code></pre></div>
<pre><code>        fit        lwr       upr
1 1.7732184 -0.5839128 17.483482
2 0.4142084 -0.7874906  8.411281</code></pre>
<p>The prediction for Harry is now 1.77 days, and for Sally is 0.41 days. The prediction intervals for each again include some values below 0, which is the smallest possible value.</p>
</div>
</div>
<div id="a-poisson-regression-model" class="section level2">
<h2><span class="header-section-number">18.7</span> A Poisson Regression Model</h2>
<p>The <code>physhealth</code> data describe a count. Specifically a count of the number of days where the subject felt poorly in the last 30. Why wouldn’t we model this count with linear regression?</p>
<ul>
<li>A count can only be positive. Linear regression would estimate some subjects as having negative counts.</li>
<li>A count is unlikely to follow a Normal distribution. In fact, it’s far more likely that the log of the counts will follow a Poisson distribution.</li>
</ul>
<p>So, we’ll try that. The Poisson distribution is used to model a <em>count</em> outcome - that is, an outcome with possible values (0, 1, 2, …). The model takes a somewhat familiar form to the models we’ve used for linear and logistic regression<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a>. If our outcome is <em>y</em> and our linear predictors <em>X</em>, then the model is:</p>
<p><span class="math display">\[
y_i \sim \mbox{Poisson}(\lambda_i)
\]</span></p>
<p>The parameter <span class="math inline">\(\lambda\)</span> must be positive, so it makes sense to fit a linear regression on the logarithm of this…</p>
<p><span class="math display">\[
\lambda_i = exp(\beta_0 + \beta_1 X_1 + ... \beta_k X_k)
\]</span></p>
<p>The coefficients <span class="math inline">\(\beta\)</span> can be exponentiated and treated as multiplicative effects.</p>
<p>We’ll run a generalized linear model with a log link function, ensuring that all of the predicted values will be positive, and using a Poisson error distribution. This is called <strong>Poisson regression</strong>.</p>
<p>Poisson regression may be appropriate when the dependent variable is a count of events. The events must be independent - the occurrence of one event must not make any other more or less likely. That’s hard to justify in our case, but we can take a look.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_poiss1 &lt;-<span class="st"> </span><span class="kw">glm</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100, 
                  <span class="dt">family =</span> <span class="kw">poisson</span>(),
                  <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_poiss1)</code></pre></div>
<pre><code>
Call:
glm(formula = physhealth ~ bmi_c + smoke100, family = poisson(), 
    data = sm_oh_A_young)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-5.0671  -2.3418  -1.8163  -0.7165  11.4801  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 0.634600   0.022548   28.14   &lt;2e-16 ***
bmi_c       0.043260   0.001707   25.34   &lt;2e-16 ***
smoke100    0.704834   0.030062   23.45   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 15034  on 1633  degrees of freedom
Residual deviance: 13880  on 1631  degrees of freedom
AIC: 15678

Number of Fisher Scoring iterations: 7</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(mod_poiss1)</code></pre></div>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>                 2.5 %     97.5 %
(Intercept) 0.59008614 0.67847727
bmi_c       0.03989052 0.04658361
smoke100    0.64596664 0.76381419</code></pre>
<div id="the-fitted-equation-2" class="section level3">
<h3><span class="header-section-number">18.7.1</span> The Fitted Equation</h3>
<p>The model equation is</p>
<pre><code>log(physhealth) = 0.63 + 0.043 bmi_c + 0.71 smoke100</code></pre>
<p>It looks like both <code>bmi</code> and <code>smoke_100</code> have confidence intervals excluding 0.</p>
</div>
<div id="interpreting-the-coefficients-2" class="section level3">
<h3><span class="header-section-number">18.7.2</span> Interpreting the Coefficients</h3>
<p>Our new model for <span class="math inline">\(y_i\)</span> = counts of poor <code>physhealth</code> days in the last 30, follows the regression equation:</p>
<p><span class="math display">\[
y_i \sim \mbox{Poisson}(exp(0.63 + 0.043 bmi_c + 0.71 smoke100))
\]</span></p>
<p>where <code>smoke100</code> is 1 if the subject has smoked 100 cigarettes (lifetime) and 0 otherwise, and <code>bmi_c</code> is just the centered body-mass index value in kg/m<sup>2</sup>. We interpret the coefficients as follows:</p>
<ul>
<li>The constant term, 0.63, gives us the intercept of the regression - the prediction if <code>smoke100 = 0</code> and <code>bmi_c = 0</code>. In this case, because we’ve centered BMI, it implies that <code>exp(0.63)</code> = 1.88 is the predicted days of poor <code>physhealth</code> for a non-smoker with average BMI.</li>
<li>The coefficient of <code>bmi_c</code>, 0.043, is the expected difference in count of poor <code>physhealth</code> days (on the log scale) for each additional kg/m<sup>2</sup> of body mass index. The expected multiplicative <em>increase</em> is <span class="math inline">\(e^{0.043}\)</span> = 1.044, corresponding to a 4.4% difference in the count.</li>
<li>The coefficient of <code>smoke100</code>, 0.71, tells us that the predictive difference between those who have and who have not smoked 100 cigarettes can be found by multiplying the <code>physhealth</code> count by exp(0.71) = 2.03, yielding essentially a doubling of the <code>physhealth</code> count.</li>
</ul>
<p>As with linear or logistic regression, each coefficient is interpreted as a comparison where one predictor changes by one unit, while the others remain constant.</p>
</div>
<div id="testing-the-predictors-2" class="section level3">
<h3><span class="header-section-number">18.7.3</span> Testing the Predictors</h3>
<p>We can use the Wald tests (z tests) provided with the Poisson regression output, or we can fit the model and then run an ANOVA to obtain a test based on the deviance (a simple transformation of the log likelihood ratio.)</p>
<ul>
<li>By the Wald tests shown above, each predictor clearly adds significant predictive value to the model given the other predictor, and we note that the <em>p</em> values are as small as R will support.</li>
<li>The ANOVA approach for this model lets us check the impact of adding <code>smoke100</code> to a model already containing <code>bmi_c</code>.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(mod_poiss1, <span class="dt">test =</span> <span class="st">&quot;Chisq&quot;</span>)</code></pre></div>
<pre><code>Analysis of Deviance Table

Model: poisson, link: log

Response: physhealth

Terms added sequentially (first to last)

         Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                      1633      15034              
bmi_c     1   602.50      1632      14432 &lt; 2.2e-16 ***
smoke100  1   551.04      1631      13880 &lt; 2.2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>To obtain a <em>p</em> value for <code>smoke100</code>’s impact after <code>bmi_c</code> is accounted for, we compare the difference in deviance to a chi-square distribution with 1 degree of freedom. To check the effect of <code>bmi_c</code>, we could refit the model with <code>bmi_c</code> entering last, and again run an ANOVA.</p>
<p>We could also run a likelihood-ratio test for each predictor, by fitting the model with and without that predictor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_poiss1_without_bmi &lt;-<span class="st"> </span><span class="kw">glm</span>(physhealth <span class="op">~</span><span class="st"> </span>smoke100,
                              <span class="dt">family =</span> <span class="kw">poisson</span>(),
                              <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">anova</span>(mod_poiss1, mod_poiss1_without_bmi, <span class="dt">test =</span> <span class="st">&quot;Chisq&quot;</span>)</code></pre></div>
<pre><code>Analysis of Deviance Table

Model 1: physhealth ~ bmi_c + smoke100
Model 2: physhealth ~ smoke100
  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
1      1631      13880                          
2      1632      14426 -1  -545.19 &lt; 2.2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="correcting-for-overdispersion-with-coeftestcoefci" class="section level3">
<h3><span class="header-section-number">18.7.4</span> Correcting for Overdispersion with <code>coeftest</code>/<code>coefci</code></h3>
<p>The main assumption we’ll think about in a Poisson model is about <strong>overdispersion</strong>. We might deal with the overdispersion we see in this model by changing the nature of the tests we run within this model, using the <code>coeftest</code> or <code>coefci</code> approaches from the <code>lmtest</code> package, as I’ll demonstrate next, or we might refit the model using a quasi-likelihood approach, as I’ll show in the material to come.</p>
<p>Here, we’ll use the <code>coeftest</code> and <code>coefci</code> approach from <code>lmtest</code> combined with robust sandwich estimation (via the <code>sandwich</code> package) to re-compute the Wald tests.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coeftest</span>(mod_poiss1, <span class="dt">vcov. =</span> sandwich)</code></pre></div>
<pre><code>
z test of coefficients:

             Estimate Std. Error z value  Pr(&gt;|z|)    
(Intercept) 0.6346000  0.0849574  7.4696 8.042e-14 ***
bmi_c       0.0432600  0.0068998  6.2697 3.617e-10 ***
smoke100    0.7048343  0.1199970  5.8738 4.260e-09 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coefci</span>(mod_poiss1, <span class="dt">vcov. =</span> sandwich)</code></pre></div>
<pre><code>                 2.5 %     97.5 %
(Intercept) 0.46808660 0.80111331
bmi_c       0.02973658 0.05678343
smoke100    0.46964440 0.94002412</code></pre>
<p>Both predictors are still significant, but the standard errors are more appropriate. Later, we’ll fit this approach by changing the estimation method to a quasi-likelihood approach.</p>
</div>
<div id="store-fitted-values-and-residuals-2" class="section level3">
<h3><span class="header-section-number">18.7.5</span> Store fitted values and residuals</h3>
<p>What happens if we try using the <code>broom</code> package in this case? We can, if we like, get our residuals and predicted values right on the scale of our <code>physhealth</code> response.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_poiss1 &lt;-<span class="st"> </span><span class="kw">augment</span>(mod_poiss1, sm_oh_A_young,
                     <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>,
                     <span class="dt">type.residuals =</span> <span class="st">&quot;response&quot;</span>)

sm_poiss1 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">select</span>(physhealth, .fitted, .resid) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">head</span>()</code></pre></div>
<pre><code>  physhealth  .fitted       .resid
1          0 1.763823 -1.763822711
2          0 2.438787 -2.438787228
3          0 1.691340 -1.691340221
4          2 2.004777 -0.004777456
5          4 4.970699 -0.970699342
6          6 1.588506  4.411493544</code></pre>
</div>
<div id="rootogram-see-the-fit-of-a-count-regression-model" class="section level3">
<h3><span class="header-section-number">18.7.6</span> Rootogram: see the fit of a count regression model</h3>
<p>A <strong>rootogram</strong> is a very useful way to visualize the fit of a count regression model<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a>. The <code>rootogram</code> function in the <code>countreg</code> package makes this pretty straightforward. By default, this fits a hanging rootogram on the square root of the frequencies.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rootogram</span>(mod_poiss1, <span class="dt">max =</span> <span class="dv">30</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-357-1.png" width="672" /></p>
<p>The red curved line is the theoretical Poisson fit. “Hanging” from each point on the red line is a bar, the height of which represents the difference between expected and observed counts. A bar hanging below 0 indicates underfitting. A bar hanging above 0 indicates overfitting. The counts have been transformed with a square root transformation to prevent smaller counts from getting obscured and overwhelmed by larger counts. We see a great deal of underfitting for counts of 0, and overfitting for most other counts, especially 1-6, with some underfitting again by <code>physhealth</code> above 14 days.</p>
</div>
<div id="specify-the-r2-and-loglikelihood-values-2" class="section level3">
<h3><span class="header-section-number">18.7.7</span> Specify the R<sup>2</sup> and log(likelihood) values</h3>
<p>We can calculate the R<sup>2</sup> as the squared correlation of the fitted values and the observed values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The correlation of observed and fitted values</span>
(poiss_r &lt;-<span class="st"> </span><span class="kw">with</span>(sm_poiss1, <span class="kw">cor</span>(physhealth, .fitted)))</code></pre></div>
<pre><code>[1] 0.2217254</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># R-square</span>
poiss_r<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>[1] 0.04916215</code></pre>
<p>The <code>glance</code> function in the <code>broom</code> package gives us model log(likelihood), among other summaries.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glance</span>(mod_poiss1) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">3</span>)</code></pre></div>
<pre><code>  null.deviance df.null    logLik      AIC      BIC deviance df.residual
1      15034.01    1633 -7835.914 15677.83 15694.02 13880.47        1631</code></pre>
<p>Here, we have</p>
<table>
<thead>
<tr class="header">
<th align="right">Model</th>
<th align="right">Scale</th>
<th align="right">R<sup>2</sup></th>
<th align="right">log(likelihood)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Poisson</td>
<td align="right">log(<code>physhealth</code>)</td>
<td align="right">.049</td>
<td align="right">-7841.69</td>
</tr>
</tbody>
</table>
</div>
<div id="check-model-assumptions-2" class="section level3">
<h3><span class="header-section-number">18.7.8</span> Check model assumptions</h3>
<p>The Poisson model is a classical generalized linear model, estimated using the method of maximum likelihood. While the default <code>plot</code> option for a <code>glm</code> still shows the plots we would use to assess the assumptions of an OLS model, we don’t actually get much from that, since our Poisson model has different assumptions. It can be useful to look at a plot of residuals vs. fitted values on the original <code>physhealth</code> scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sm_poiss1, <span class="kw">aes</span>(<span class="dt">x =</span> .fitted, <span class="dt">y =</span> .resid)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Residuals vs. Fitted `physhealth`&quot;</span>,
         <span class="dt">subtitle =</span> <span class="st">&quot;Original Poisson Regression model&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-360-1.png" width="672" /></p>
</div>
<div id="using-glm.diag.plots-from-the-boot-package" class="section level3">
<h3><span class="header-section-number">18.7.9</span> Using <code>glm.diag.plots</code> from the <code>boot</code> package</h3>
<p>The <code>glm.diag.plots</code> function from the <code>boot</code> package makes a series of diagnostic plots for generalized linear models.</p>
<ul>
<li>(Top, Left) Jackknife deviance residuals against fitted values. This is essentially identical to what you obtain with <code>plot(mod_poiss1, which = 1)</code>. A <em>jackknife deviance</em> residual is also called a likelihood residual. It is the change in deviance when this observation is omitted from the data.</li>
<li>(Top, Right) Normal Q-Q plot of standardized deviance residuals. (Dotted line shows expectation if those standardized residuals followed a Normal distribution, and these residuals generally should.) The result is similar to what you obtain with <code>plot(mod_poiss1, which = 2)</code>.</li>
<li>(Bottom, Left) Cook statistic vs. standardized leverage
<ul>
<li>n = # of observations, p = # of parameters estimated</li>
<li>Horizontal dotted line is at <span class="math inline">\(\frac{8}{n - 2p}\)</span>. Points above the line have high influence on the model.</li>
<li>Vertical line is at <span class="math inline">\(\frac{2p}{n - 2p}\)</span>. Points to the right of the line have high leverage.</li>
</ul></li>
<li>(Bottom, Right) Index plot of Cook’s statistic to help identify the observations with high influence. This is essentially the same plot as <code>plot(mod_poiss1, which = 4)</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glm.diag.plots</span>(mod_poiss1)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-361-1.png" width="672" /></p>
<p>When working with these plots, it is possible to use the <code>iden</code> command to perform some interactive identification of points in your R terminal. But that doesn’t play out effectively in an HTML summary document like this, so we’ll leave that out.</p>
</div>
<div id="predictions-for-harry-and-sally-2" class="section level3">
<h3><span class="header-section-number">18.7.10</span> Predictions for Harry and Sally</h3>
<p>The predictions from a <code>glm</code> fit like this don’t include prediction intervals. But we can get predictions on the scale of our original response variable, <code>physhealth</code>, like this.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(mod_poiss1, <span class="dt">newdata =</span> hs_data, <span class="dt">se.fit =</span> <span class="ot">TRUE</span>,
        <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre></div>
<pre><code>$fit
       1        2 
5.882808 1.519376 

$se.fit
         1          2 
0.13739453 0.03858279 

$residual.scale
[1] 1</code></pre>
<p>By using <code>response</code> as the type, these predictions fall on the original <code>physhealth</code> scale. The prediction for Harry is now 5.87 days, and for Sally is 1.52 days.</p>
</div>
</div>
<div id="overdispersion-in-a-poisson-model" class="section level2">
<h2><span class="header-section-number">18.8</span> Overdispersion in a Poisson Model</h2>
<p>Poisson regressions do not supply an independent variance parameter <span class="math inline">\(\sigma\)</span>, and as a result can be overdispersed, and usually are. Under the Poisson distribution, the variance equals the mean - so the standard deviation equals the square root of the mean. The notion of <strong>overdispersion</strong> arises here. When fitting generalized linear models with Poisson error distributions, the residual deviance and its degrees of freedom should be approximately equal if the model fits well.</p>
<p>If the residual deviance is far greater than the degrees of freedom, then overdispersion may well be a problem. In this case, the residual deviance is about 8.5 times the size of the residual degrees of freedom, so that’s a clear indication of overdispersion. We saw earlier that the Poisson regression model requires that the outcome (here the <code>physhealth</code> counts) be independent. A possible reason for the overdispersion we see here is that <code>physhealth</code> on different days likely do not occur independently of one another but likely “cluster” together.</p>
<div id="testing-for-overdispersion" class="section level3">
<h3><span class="header-section-number">18.8.1</span> Testing for Overdispersion?</h3>
<p>Gelman and Hill provide an overdispersion test in R for a Poisson model as follows…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(mod_poiss1, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
n &lt;-<span class="st"> </span>arm<span class="op">::</span><span class="kw">display</span>(mod_poiss1)<span class="op">$</span>n</code></pre></div>
<pre><code>glm(formula = physhealth ~ bmi_c + smoke100, family = poisson(), 
    data = sm_oh_A_young)
            coef.est coef.se
(Intercept) 0.63     0.02   
bmi_c       0.04     0.00   
smoke100    0.70     0.03   
---
  n = 1634, k = 3
  residual deviance = 13880.5, null deviance = 15034.0 (difference = 1153.5)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">k &lt;-<span class="st"> </span>arm<span class="op">::</span><span class="kw">display</span>(mod_poiss1)<span class="op">$</span>k</code></pre></div>
<pre><code>glm(formula = physhealth ~ bmi_c + smoke100, family = poisson(), 
    data = sm_oh_A_young)
            coef.est coef.se
(Intercept) 0.63     0.02   
bmi_c       0.04     0.00   
smoke100    0.70     0.03   
---
  n = 1634, k = 3
  residual deviance = 13880.5, null deviance = 15034.0 (difference = 1153.5)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z &lt;-<span class="st"> </span>(sm_oh_A_young<span class="op">$</span>physhealth <span class="op">-</span><span class="st"> </span>yhat) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(yhat)
<span class="kw">cat</span>(<span class="st">&quot;overdispersion ratio is &quot;</span>, <span class="kw">sum</span>(z<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span>k), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</code></pre></div>
<pre><code>overdispersion ratio is  15.49983 </code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="st">&quot;p value of overdispersion test is &quot;</span>, <span class="kw">pchisq</span>(<span class="kw">sum</span>(z<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(n<span class="op">-</span>k), n<span class="op">-</span>k), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</code></pre></div>
<pre><code>p value of overdispersion test is  0 </code></pre>
<p>The p value here is 0, indicating that the probability is essentially zero that a random variable from a <span class="math inline">\(\chi^2\)</span> distribution with (n - k) = 1633 degrees of freedom would be as large as what we observed in this case.</p>
<p>In summary, the polyps counts are overdispersed by a factor of 15.499, which is enormous (even a factor of 2 would be considered large) and also highly statistically significant. The basic correction for overdisperson is to multiply all regression standard errors by <span class="math inline">\(\sqrt{15.499}\)</span> = 3.94.</p>
<p>The <code>quasipoisson</code> model and the negative binomial model that we’ll fit below are very similar. We write the overdispersed “quasiPoisson” model as:</p>
<p><span class="math display">\[
y_i \sim \mbox{overdispersed Poisson} (\mu_i exp(X_i \beta), \omega)
\]</span></p>
<p>where <span class="math inline">\(\omega\)</span> is the overdispersion parameter, 15.499, in our case. The Poisson model we saw previously is then just the overdispersed Poisson model with <span class="math inline">\(\omega = 1\)</span>.</p>
</div>
</div>
<div id="fitting-the-quasi-poisson-model" class="section level2">
<h2><span class="header-section-number">18.9</span> Fitting the Quasi-Poisson Model</h2>
<p>To deal with overdispersion, one useful approach is to apply a <strong>quasi-likelihood estimation procedure</strong>, as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_poiss_od1 &lt;-<span class="st"> </span><span class="kw">glm</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100, 
                  <span class="dt">family =</span> <span class="kw">quasipoisson</span>(),
                  <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_poiss_od1)</code></pre></div>
<pre><code>
Call:
glm(formula = physhealth ~ bmi_c + smoke100, family = quasipoisson(), 
    data = sm_oh_A_young)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-5.0671  -2.3418  -1.8163  -0.7165  11.4801  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.634600   0.088771   7.149 1.32e-12 ***
bmi_c       0.043260   0.006722   6.436 1.61e-10 ***
smoke100    0.704834   0.118352   5.955 3.17e-09 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for quasipoisson family taken to be 15.49983)

    Null deviance: 15034  on 1633  degrees of freedom
Residual deviance: 13880  on 1631  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 7</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(mod_poiss_od1)</code></pre></div>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>                2.5 %     97.5 %
(Intercept) 0.4555430 0.80378993
bmi_c       0.0297236 0.05608646
smoke100    0.4734806 0.93793099</code></pre>
<p>This “quasi-Poisson regression” model uses the same mean function as Poisson regression, but now estimated by quasi-maximum likelihood estimation or, equivalently, through the method of generalized estimating equations, where the inference is adjusted by an estimated dispersion parameter. Sometimes, though I won’t demonstrate this here, people fit an “adjusted” Poisson regression model, where this estimation by quasi-ML is augmented to adjust the inference via sandwich estimates of the covariances<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a>.</p>
<div id="the-fitted-equation-3" class="section level3">
<h3><span class="header-section-number">18.9.1</span> The Fitted Equation</h3>
<p>The model equation is still <code>log(physhealth) = 0.63 + 0.04 bmi_c + 0.71 smoke100</code>. The estimated coefficients are still statistically significant, but the standard errors for each coefficient are considerably larger when we account for overdispersion.</p>
<p>The dispersion parameter for the quasi-Poisson family is now taken to be a bit less than the square root of the ratio of the residual deviance and its degrees of freedom. This is a much more believable model, as a result.</p>
</div>
<div id="interpreting-the-coefficients-3" class="section level3">
<h3><span class="header-section-number">18.9.2</span> Interpreting the Coefficients</h3>
<p>No meaningful change from the Poisson model we saw previously.</p>
<ul>
<li>The constant term, 0.63, gives us the intercept of the regression - the prediction if <code>smoke100 = 0</code> and <code>bmi_c = 0</code>. In this case, because we’ve centered BMI, it implies that <code>exp(0.63)</code> = 1.88 is the predicted days of poor <code>physhealth</code> for a non-smoker with average BMI.</li>
<li>The coefficient of <code>bmi_c</code>, 0.043, is the expected difference in count of poor <code>physhealth</code> days (on the log scale) for each additional kg/m<sup>2</sup> of body mass index. The expected multiplicative <em>increase</em> is <span class="math inline">\(e^{0.043}\)</span> = 1.044, corresponding to a 4.4% difference in the count.</li>
<li>The coefficient of <code>smoke100</code>, 0.71, tells us that the predictive difference between those who have and who have not smoked 100 cigarettes can be found by multiplying the <code>physhealth</code> count by exp(0.71) = 2.03, yielding essentially a doubling of the <code>physhealth</code> count.</li>
</ul>
</div>
<div id="testing-the-predictors-3" class="section level3">
<h3><span class="header-section-number">18.9.3</span> Testing the Predictors</h3>
<p>Again, we can use the Wald tests (z tests) provided with the Poisson regression output, or we can fit the model and then run an ANOVA to obtain a test based on the deviance (a simple transformation of the log likelihood ratio.)</p>
<ul>
<li>By the Wald tests shown above, each predictor clearly adds significant predictive value to the model given the other predictor, and we note that the <em>p</em> values are as small as R will support.</li>
<li>The ANOVA approach for this model lets us check the impact of adding <code>smoke100</code> to a model already containing <code>bmi_c</code>.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(mod_poiss_od1, <span class="dt">test =</span> <span class="st">&quot;Chisq&quot;</span>)</code></pre></div>
<pre><code>Analysis of Deviance Table

Model: quasipoisson, link: log

Response: physhealth

Terms added sequentially (first to last)

         Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                      1633      15034              
bmi_c     1   602.50      1632      14432 4.526e-10 ***
smoke100  1   551.04      1631      13880 2.484e-09 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The result is unchanged. To obtain a <em>p</em> value for <code>smoke100</code>’s impact after <code>bmi_c</code> is accounted for, we compare the difference in deviance to a chi-square distribution with 1 degree of freedom. The result is incredibly statistically significant.</p>
<p>To check the effect of <code>bmi_c</code>, we could refit the model with and without <code>bmi_c</code>, and again run an ANOVA. I’ll skip that here.</p>
</div>
<div id="store-fitted-values-and-residuals-3" class="section level3">
<h3><span class="header-section-number">18.9.4</span> Store fitted values and residuals</h3>
<p>What happens if we try using the <code>broom</code> package in this case? We can, if we like, get our residuals and predicted values right on the scale of our <code>physhealth</code> response.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_poiss_od1 &lt;-<span class="st"> </span><span class="kw">augment</span>(mod_poiss_od1, sm_oh_A_young,
                     <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>,
                     <span class="dt">type.residuals =</span> <span class="st">&quot;response&quot;</span>)

sm_poiss_od1 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">select</span>(physhealth, .fitted, .resid) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">head</span>()</code></pre></div>
<pre><code>  physhealth  .fitted       .resid
1          0 1.763823 -1.763822711
2          0 2.438787 -2.438787228
3          0 1.691340 -1.691340221
4          2 2.004777 -0.004777456
5          4 4.970699 -0.970699342
6          6 1.588506  4.411493544</code></pre>
<p>It turns out that 0 of the 1634 predictions that we make are below 0, and the largest prediction made by this model is 19.62 days.</p>
<p>The <code>rootogram</code> function we’ve shown doesn’t support overdispersed Poisson models at the moment.</p>
</div>
<div id="specify-the-r2-and-loglikelihood-values-3" class="section level3">
<h3><span class="header-section-number">18.9.5</span> Specify the R<sup>2</sup> and log(likelihood) values</h3>
<p>We can calculate the R<sup>2</sup> as the squared correlation of the fitted values and the observed values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The correlation of observed and fitted values</span>
(poiss_od_r &lt;-<span class="st"> </span><span class="kw">with</span>(sm_poiss_od1, <span class="kw">cor</span>(physhealth, .fitted)))</code></pre></div>
<pre><code>[1] 0.2217254</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># R-square</span>
poiss_od_r<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>[1] 0.04916215</code></pre>
<p>The <code>glance</code> function in the <code>broom</code> package gives us model log(likelihood), among other summaries.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glance</span>(mod_poiss_od1) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">3</span>)</code></pre></div>
<pre><code>  null.deviance df.null logLik AIC BIC deviance df.residual
1      15034.01    1633     NA  NA  NA 13880.47        1631</code></pre>
<p>Here, we have</p>
<table>
<thead>
<tr class="header">
<th align="right">Model</th>
<th align="right">Scale</th>
<th align="right">R<sup>2</sup></th>
<th align="right">log(likelihood)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Poisson</td>
<td align="right">log(<code>physhealth</code>)</td>
<td align="right">.049</td>
<td align="right">NA</td>
</tr>
</tbody>
</table>
</div>
<div id="check-model-assumptions-3" class="section level3">
<h3><span class="header-section-number">18.9.6</span> Check model assumptions</h3>
<p>Having dealt with the overdispersion, this should be a cleaner model in some ways, but the diagnostics (other than the dispersion) will be the same. Here is a plot of residuals vs. fitted values on the original <code>physhealth</code> scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sm_poiss_od1, <span class="kw">aes</span>(<span class="dt">x =</span> .fitted, <span class="dt">y =</span> .resid)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Residuals vs. Fitted `physhealth`&quot;</span>,
         <span class="dt">subtitle =</span> <span class="st">&quot;Overdispersed Poisson Regression model&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-369-1.png" width="672" /></p>
<p>I’ll skip the <code>glm.diag.plots</code> results, since you’ve already seen them.</p>
</div>
<div id="predictions-for-harry-and-sally-3" class="section level3">
<h3><span class="header-section-number">18.9.7</span> Predictions for Harry and Sally</h3>
<p>The predictions from this overdispersed Poisson regression will match those in the original Poisson regression, but the standard error will be larger.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(mod_poiss_od1, <span class="dt">newdata =</span> hs_data, <span class="dt">se.fit =</span> <span class="ot">TRUE</span>,
        <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre></div>
<pre><code>$fit
       1        2 
5.882808 1.519376 

$se.fit
        1         2 
0.5409199 0.1518998 

$residual.scale
[1] 3.936983</code></pre>
<p>By using <code>response</code> as the type, these predictions fall on the original <code>physhealth</code> scale. Again, the prediction for Harry is 5.87 days, and for Sally is 1.52 days.</p>
</div>
</div>
<div id="poisson-and-quasi-poisson-models-using-glm-from-the-rms-package" class="section level2">
<h2><span class="header-section-number">18.10</span> Poisson and Quasi-Poisson models using <code>Glm</code> from the <code>rms</code> package</h2>
<p>The <code>Glm</code> function in the <code>rms</code> package can be used to fit both the original Poisson regression and the quasi-Poisson model accounting for overdispersion.</p>
<div id="refitting-the-original-poisson-regression-with-glm" class="section level3">
<h3><span class="header-section-number">18.10.1</span> Refitting the original Poisson regression with <code>Glm</code></h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="kw">datadist</span>(sm_oh_A_young)
<span class="kw">options</span>(<span class="dt">datadist =</span> <span class="st">&quot;d&quot;</span>)

mod_poi_Glm_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">Glm</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100,
                     <span class="dt">family =</span> <span class="kw">poisson</span>(), 
                     <span class="dt">data =</span> sm_oh_A_young, 
                     <span class="dt">x =</span> T, <span class="dt">y =</span> T)

mod_poi_Glm_<span class="dv">1</span></code></pre></div>
<pre><code>General Linear Model
 
 Glm(formula = physhealth ~ bmi_c + smoke100, family = poisson(), 
     data = sm_oh_A_young, x = T, y = T)
 
                       Model Likelihood     
                          Ratio Test        
 Obs    1634          LR chi2    1153.54    
 Residual d.f.1631    d.f.             2    
 g 0.5185238          Pr(&gt; chi2) &lt;0.0001    
 
           Coef   S.E.   Wald Z Pr(&gt;|Z|)
 Intercept 0.6346 0.0225 28.14  &lt;0.0001 
 bmi_c     0.0433 0.0017 25.34  &lt;0.0001 
 smoke100  0.7048 0.0301 23.45  &lt;0.0001 
 </code></pre>
</div>
<div id="refitting-the-overdispersed-poisson-regression-with-glm" class="section level3">
<h3><span class="header-section-number">18.10.2</span> Refitting the overdispersed Poisson regression with <code>Glm</code></h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="kw">datadist</span>(sm_oh_A_young)
<span class="kw">options</span>(<span class="dt">datadist =</span> <span class="st">&quot;d&quot;</span>)

mod_poi_od_Glm_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">Glm</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100,
                     <span class="dt">family =</span> <span class="kw">quasipoisson</span>(), 
                     <span class="dt">data =</span> sm_oh_A_young, 
                     <span class="dt">x =</span> T, <span class="dt">y =</span> T)

mod_poi_od_Glm_<span class="dv">1</span></code></pre></div>
<pre><code>General Linear Model
 
 Glm(formula = physhealth ~ bmi_c + smoke100, family = quasipoisson(), 
     data = sm_oh_A_young, x = T, y = T)
 
                       Model Likelihood     
                          Ratio Test        
 Obs    1634          LR chi2    1153.54    
 Residual d.f.1631    d.f.             2    
 g 0.5185238          Pr(&gt; chi2) &lt;0.0001    
 
           Coef   S.E.   Wald Z Pr(&gt;|Z|)
 Intercept 0.6346 0.0888 7.15   &lt;0.0001 
 bmi_c     0.0433 0.0067 6.44   &lt;0.0001 
 smoke100  0.7048 0.1184 5.96   &lt;0.0001 
 </code></pre>
<p>The big advantage here is that we have access to the usual <code>ANOVA</code>, <code>summary</code>, and <code>nomogram</code> features that <code>rms</code> brings to fitting models.</p>
</div>
<div id="anova-on-a-glm-fit" class="section level3">
<h3><span class="header-section-number">18.10.3</span> ANOVA on a <code>Glm</code> fit</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(mod_poi_od_Glm_<span class="dv">1</span>)</code></pre></div>
<pre><code>                Wald Statistics          Response: physhealth 

 Factor     Chi-Square d.f. P     
 bmi_c      41.42      1    &lt;.0001
 smoke100   35.47      1    &lt;.0001
 TOTAL      81.11      2    &lt;.0001</code></pre>
<p>This shows the individual Wald <span class="math inline">\(\chi^2\)</span> tests without having to refit the model.</p>
</div>
<div id="ggplots-from-glm-fit" class="section level3">
<h3><span class="header-section-number">18.10.4</span> ggplots from <code>Glm</code> fit</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">Predict</span>(mod_poi_od_Glm_<span class="dv">1</span>, <span class="dt">fun =</span> exp))</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-374-1.png" width="672" /></p>
</div>
<div id="summary-of-a-glm-fit" class="section level3">
<h3><span class="header-section-number">18.10.5</span> Summary of a <code>Glm</code> fit</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod_poi_od_Glm_<span class="dv">1</span>)</code></pre></div>
<pre><code>             Effects              Response : physhealth 

 Factor   Low     High   Diff. Effect  S.E.    Lower 0.95 Upper 0.95
 bmi_c    -4.7165 3.1785 7.895 0.34154 0.05307 0.23745    0.44563   
 smoke100  0.0000 1.0000 1.000 0.70483 0.11835 0.47270    0.93697   </code></pre>
</div>
<div id="plot-of-the-summary" class="section level3">
<h3><span class="header-section-number">18.10.6</span> Plot of the Summary</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">summary</span>(mod_poi_od_Glm_<span class="dv">1</span>))</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-376-1.png" width="672" /></p>
</div>
<div id="nomogram-of-a-glm-fit" class="section level3">
<h3><span class="header-section-number">18.10.7</span> Nomogram of a <code>Glm</code> fit</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">nomogram</span>(mod_poi_od_Glm_<span class="dv">1</span>, <span class="dt">fun =</span> exp, 
              <span class="dt">funlabel =</span> <span class="st">&quot;physhealth days&quot;</span>))</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-377-1.png" width="672" /></p>
<p>Note the use of <code>fun=exp</code> in both the ggplot of Predict and the nomogram. What’s that doing?</p>
</div>
</div>
<div id="negative-binomial-model" class="section level2">
<h2><span class="header-section-number">18.11</span> Negative Binomial Model</h2>
<p>Another approach to dealing with overdispersion is to fit a negative binomial model<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a> to predict the log(<code>physhealth</code>) counts. This involves the fitting of an additional parameter, <span class="math inline">\(\theta\)</span>. That’s our dispersion parameter<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a></p>
<p>Sometimes, people will fit a model where <span class="math inline">\(\theta\)</span> is known, for instance a geometric model (where <span class="math inline">\(\theta\)</span> = 1), and then this can be directly plugged into a <code>glm()</code> fit, but the more common scenario is that we are going to iteratively estimate the <span class="math inline">\(\beta\)</span> coefficients and <span class="math inline">\(\theta\)</span>. To do this, I’ll use the <code>glm.nb</code> function from the <code>MASS</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_nb1 &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">glm.nb</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100, <span class="dt">link =</span> log,
                  <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_nb1)</code></pre></div>
<pre><code>
Call:
MASS::glm.nb(formula = physhealth ~ bmi_c + smoke100, data = sm_oh_A_young, 
    link = log, init.theta = 0.1343652349)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1087  -0.9046  -0.8288  -0.1662   2.2323  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.61095    0.08804   6.939 3.94e-12 ***
bmi_c        0.04238    0.01010   4.198 2.70e-05 ***
smoke100     0.75724    0.14363   5.272 1.35e-07 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for Negative Binomial(0.1344) family taken to be 1)

    Null deviance: 1132.9  on 1633  degrees of freedom
Residual deviance: 1081.9  on 1631  degrees of freedom
AIC: 5171.4

Number of Fisher Scoring iterations: 1

              Theta:  0.13437 
          Std. Err.:  0.00743 

 2 x log-likelihood:  -5163.36000 </code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(mod_nb1)</code></pre></div>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>                 2.5 %     97.5 %
(Intercept) 0.44260603 0.78783368
bmi_c       0.02436096 0.06150643
smoke100    0.47867108 1.04219722</code></pre>
<div id="the-fitted-equation-4" class="section level3">
<h3><span class="header-section-number">18.11.1</span> The Fitted Equation</h3>
<p>The form of the model equation for a negative binomial regression is the same as that for Poisson regression.</p>
<pre><code>log(physhealth) = 0.61 + 0.043 bmi_c + 0.756 smoke100</code></pre>
</div>
<div id="comparison-with-the-raw-poisson-model" class="section level3">
<h3><span class="header-section-number">18.11.2</span> Comparison with the (raw) Poisson model</h3>
<p>To compare the negative binomial model to the Poisson model (without the overdispersion) we can use the <code>logLik</code> function to make a comparison. Note that the Poisson model is a subset of the negative binomial.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">logLik</span>(mod_nb1)</code></pre></div>
<pre><code>&#39;log Lik.&#39; -2581.68 (df=4)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">logLik</span>(mod_poiss1)</code></pre></div>
<pre><code>&#39;log Lik.&#39; -7835.914 (df=3)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pchisq</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">logLik</span>(mod_nb1) <span class="op">-</span><span class="st"> </span><span class="kw">logLik</span>(mod_poiss1)), <span class="dt">df =</span> <span class="dv">1</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>&#39;log Lik.&#39; 0 (df=4)</code></pre>
<p>Here, the difference in the log likelihoods, multiplied by 2, is 10512 with 1 degree of freedom. This strongly suggests that the negative binomial model, estimating the dispersion parameter, is more appropriate than the raw Poisson model.</p>
<p>However, both the regression coefficients and the standard errors are rather similar to the quasi-Poisson and the sandwich-adjusted Poisson results above. Thus, in terms of predicted means, all three models give very similar results; the associated Wald tests also lead to the same conclusions.</p>
</div>
<div id="interpreting-the-coefficients-4" class="section level3">
<h3><span class="header-section-number">18.11.3</span> Interpreting the Coefficients</h3>
<p>There’s only a small change here from the Poisson models we saw previously.</p>
<ul>
<li>The constant term, 0.61, gives us the intercept of the regression - the prediction if <code>smoke100 = 0</code> and <code>bmi_c = 0</code>. In this case, because we’ve centered BMI, it implies that <code>exp(0.61)</code> = 1.84 is the predicted days of poor <code>physhealth</code> for a non-smoker with average BMI.</li>
<li>The coefficient of <code>bmi_c</code>, 0.043, is the expected difference in count of poor <code>physhealth</code> days (on the log scale) for each additional kg/m<sup>2</sup> of body mass index. The expected multiplicative <em>increase</em> is <span class="math inline">\(e^{0.043}\)</span> = 1.044, corresponding to a 4.4% difference in the count.</li>
<li>The coefficient of <code>smoke100</code>, 0.756, tells us that the predictive difference between those who have and who have not smoked 100 cigarettes can be found by multiplying the <code>physhealth</code> count by exp(0.756) = 2.13, yielding essentially a doubling of the <code>physhealth</code> count.</li>
</ul>
</div>
<div id="interpretation-of-coefficients-in-terms-of-irrs" class="section level3">
<h3><span class="header-section-number">18.11.4</span> Interpretation of Coefficients in terms of IRRs</h3>
<p>We might be interested in looking at incident rate ratios rather than coefficients. The coefficients have an additive effect in the log(y) scale, and the IRR have a multiplicative effect in the y scale. To do this, we can exponentiate our model coefficients. This also applies to the confidence intervals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">coef</span>(mod_nb1))</code></pre></div>
<pre><code>(Intercept)       bmi_c    smoke100 
   1.842189    1.043292    2.132374 </code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">confint</span>(mod_nb1))</code></pre></div>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>               2.5 %   97.5 %
(Intercept) 1.556759 2.198628
bmi_c       1.024660 1.063437
smoke100    1.613928 2.835440</code></pre>
<p>As an example, then, the incident rate for <code>smoke100</code> = 1 is 2.13 times the incident rate of <code>physhealth</code> days for the reference group (<code>smoke100</code> = 0). The percent change in the incident rate of <code>physhealth</code> is a 4.3% increase for every kg/m<sup>2</sup> increase in centered <code>bmi</code>.</p>
</div>
<div id="testing-the-predictors-4" class="section level3">
<h3><span class="header-section-number">18.11.5</span> Testing the Predictors</h3>
<p>Again, we can use the Wald tests (z tests) provided with the negative binomial regression output.</p>
<p>As an alternative, we probably should not use the standard <code>anova</code> process, because the models there don’t re-estimate <span class="math inline">\(\theta\)</span> for each new model, as the warning message below indicates.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(mod_nb1)</code></pre></div>
<pre><code>Warning in anova.negbin(mod_nb1): tests made without re-estimating &#39;theta&#39;</code></pre>
<pre><code>Analysis of Deviance Table

Model: Negative Binomial(0.1344), link: log

Response: physhealth

Terms added sequentially (first to last)

         Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                      1633     1132.9              
bmi_c     1   22.000      1632     1110.9 2.726e-06 ***
smoke100  1   29.004      1631     1081.9 7.224e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>So, instead, if we want, for instance to assess the significance of <code>bmi_c</code>, after <code>smoke100</code> is already included in the model, we fit both models (with and without <code>bmi_c</code>) and then compare those models with a likelihood ratio test.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_nb1_without_bmi &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">glm.nb</span>(physhealth <span class="op">~</span><span class="st"> </span>smoke100, 
                                      <span class="dt">link =</span> log,
                                      <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">anova</span>(mod_nb1, mod_nb1_without_bmi)</code></pre></div>
<pre><code>Likelihood ratio tests of Negative Binomial Models

Response: physhealth
             Model     theta Resid. df    2 x log-lik.   Test    df
1         smoke100 0.1300079      1632       -5186.181             
2 bmi_c + smoke100 0.1343652      1631       -5163.360 1 vs 2     1
  LR stat.      Pr(Chi)
1                      
2 22.82102 1.778106e-06</code></pre>
<p>And we could compare the negative binomial models with and without <code>smoke100</code> in a similar way.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_nb1_without_smoke &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">glm.nb</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c, 
                                      <span class="dt">link =</span> log,
                                      <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">anova</span>(mod_nb1, mod_nb1_without_smoke)</code></pre></div>
<pre><code>Likelihood ratio tests of Negative Binomial Models

Response: physhealth
             Model     theta Resid. df    2 x log-lik.   Test    df
1            bmi_c 0.1289940      1632       -5191.807             
2 bmi_c + smoke100 0.1343652      1631       -5163.360 1 vs 2     1
  LR stat.      Pr(Chi)
1                      
2 28.44694 9.630178e-08</code></pre>
</div>
<div id="store-fitted-values-and-residuals-4" class="section level3">
<h3><span class="header-section-number">18.11.6</span> Store fitted values and residuals</h3>
<p>The <code>broom</code> package works in this case, too. We’ll look here at residuals and predicted (fitted) values on the scale of our <code>physhealth</code> response.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_nb1 &lt;-<span class="st"> </span><span class="kw">augment</span>(mod_nb1, sm_oh_A_young,
                     <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>,
                     <span class="dt">type.residuals =</span> <span class="st">&quot;response&quot;</span>)

sm_nb1 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">select</span>(physhealth, .fitted, .resid) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">head</span>()</code></pre></div>
<pre><code>  physhealth  .fitted      .resid
1          0 1.724956 -1.72495599
2          0 2.369399 -2.36939884
3          0 1.655481 -1.65548138
4          2 1.955507  0.04449258
5          4 4.759915 -0.75991504
6          6 1.556811  4.44318950</code></pre>
</div>
<div id="rootogram-for-negative-binomial-model" class="section level3">
<h3><span class="header-section-number">18.11.7</span> Rootogram for Negative Binomial model</h3>
<p>Here’s the rootogram for the negative binomial model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rootogram</span>(mod_nb1, <span class="dt">max =</span> <span class="dv">30</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-385-1.png" width="672" /></p>
<p>Again, the red curved line is the theoretical (negative binomial) fit. “Hanging” from each point on the red line is a bar, the height of which represents the difference between expected and observed counts. A bar hanging below 0 indicates underfitting. A bar hanging above 0 indicates overfitting. The counts have been transformed with a square root transformation to prevent smaller counts from getting obscured and overwhelmed by larger counts.</p>
<p>The match looks much better than the Poisson model, which is a sign that accounting for overdispersion is very important. Even this model badly underfits the number of 30 values, however.</p>
</div>
<div id="simulating-what-the-negative-binomial-model-predicts" class="section level3">
<h3><span class="header-section-number">18.11.8</span> Simulating what the Negative Binomial model predicts</h3>
<p>We can use the parameters of the negative binomial model to simulate data<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a> and compare the simulated results to our observed <code>physhealth</code> data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
sm_oh_A_young<span class="op">$</span>physhealth <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">table</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">barplot</span>(<span class="dt">main =</span> <span class="st">&quot;Observed physhealth&quot;</span>)
<span class="kw">set.seed</span>(<span class="dv">432122</span>)
<span class="kw">rnbinom</span>(<span class="dt">n =</span> <span class="kw">nrow</span>(sm_oh_A_young), 
        <span class="dt">size =</span> mod_nb1<span class="op">$</span>theta,
        <span class="dt">mu =</span> <span class="kw">exp</span>(<span class="kw">coef</span>(mod_nb1)[<span class="dv">1</span>])) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">table</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">barplot</span>(<span class="dt">main =</span> <span class="st">&quot;Simulated physhealth&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-386-1.png" width="672" /></p>
<p>Again we see that the simulated data badly underfits the 30 values, and includes some predictions larger than 30.</p>
</div>
<div id="specify-the-r2-and-loglikelihood-values-4" class="section level3">
<h3><span class="header-section-number">18.11.9</span> Specify the R<sup>2</sup> and log(likelihood) values</h3>
<p>We can calculate the R<sup>2</sup> as the squared correlation of the fitted values and the observed values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The correlation of observed and fitted values</span>
(nb_r &lt;-<span class="st"> </span><span class="kw">with</span>(sm_nb1, <span class="kw">cor</span>(physhealth, .fitted)))</code></pre></div>
<pre><code>[1] 0.220983</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># R-square</span>
nb_r<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>[1] 0.04883348</code></pre>
<p>The <code>glance</code> function in the <code>broom</code> package gives us model log(likelihood), among other summaries.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glance</span>(mod_nb1) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">3</span>)</code></pre></div>
<pre><code>  null.deviance df.null   logLik     AIC      BIC deviance df.residual
1      1132.866    1633 -2581.68 5171.36 5192.956 1081.862        1631</code></pre>
<p>Here, we have</p>
<table>
<thead>
<tr class="header">
<th align="right">Model</th>
<th align="right">Scale</th>
<th align="right">R<sup>2</sup></th>
<th align="right">log(likelihood)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Negative Binomial</td>
<td align="right">log(<code>physhealth</code>)</td>
<td align="right">.049</td>
<td align="right">-2585.6</td>
</tr>
</tbody>
</table>
</div>
<div id="check-model-assumptions-4" class="section level3">
<h3><span class="header-section-number">18.11.10</span> Check model assumptions</h3>
<p>Here is a plot of residuals vs. fitted values on the original <code>physhealth</code> scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sm_nb1, <span class="kw">aes</span>(<span class="dt">x =</span> .fitted, <span class="dt">y =</span> .resid)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Residuals vs. Fitted `physhealth`&quot;</span>,
         <span class="dt">subtitle =</span> <span class="st">&quot;Negative Binomial Regression model&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-389-1.png" width="672" /></p>
<p>Here are the <code>glm</code> diagnostic plots from the <code>boot</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glm.diag.plots</span>(mod_nb1)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-390-1.png" width="672" /></p>
<p>From the lower left plot, we see fewer points with large values of both Cook’s distance and leverage, so that’s a step in the right direction. The upper right plot still has some issues, but we’re closer to a desirable result there, too.</p>
</div>
<div id="predictions-for-harry-and-sally-4" class="section level3">
<h3><span class="header-section-number">18.11.11</span> Predictions for Harry and Sally</h3>
<p>The predictions from this negative binomial regression model will be only a little different than those from the Poisson models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(mod_nb1, <span class="dt">newdata =</span> hs_data, <span class="dt">se.fit =</span> <span class="ot">TRUE</span>,
        <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre></div>
<pre><code>$fit
       1        2 
6.001454 1.490406 

$se.fit
        1         2 
0.8920031 0.1497235 

$residual.scale
[1] 1</code></pre>
<p>As we’ve seen in the past, when we use <code>response</code> as the type, the predictions fall on the original <code>physhealth</code> scale. The prediction for Harry is 5.99 days, and for Sally is 1.49 days.</p>
</div>
</div>
<div id="the-problem-too-few-zeros" class="section level2">
<h2><span class="header-section-number">18.12</span> The Problem: Too Few Zeros</h2>
<p>Remember that we observe more than 1000 zeros in our <code>physhealth</code> data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_oh_A_young <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(physhealth <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)</code></pre></div>
<pre><code># A tibble: 2 x 2
  `physhealth == 0`     n
  &lt;lgl&gt;             &lt;int&gt;
1 FALSE               525
2 TRUE               1109</code></pre>
<p>Let’s go back to our Poisson model (without overdispersion) for a moment, and concentrate on the zero values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict expected mean physhealth for each subject</span>
mu &lt;-<span class="st"> </span><span class="kw">predict</span>(mod_poiss1, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)

<span class="co"># sum the probabilities of a zero count for each mean</span>
exp &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dpois</span>(<span class="dt">x =</span> <span class="dv">0</span>, <span class="dt">lambda =</span> mu))

<span class="co"># predicted number of zeros from Poisson model</span>
<span class="kw">round</span>(exp)</code></pre></div>
<pre><code>[1] 190</code></pre>
<p>As we’ve seen previously, we’re severely underfitting zero counts. We can compare the observed number of zero <code>physhealth</code> results to the expected number of zero values from the likelihood-based models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">c</span>(<span class="st">&quot;Obs&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(sm_oh_A_young<span class="op">$</span>physhealth <span class="op">==</span><span class="st"> </span><span class="dv">0</span>),
  <span class="st">&quot;Poisson&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dpois</span>(<span class="dv">0</span>, <span class="kw">fitted</span>(mod_poiss1))),
  <span class="st">&quot;NB&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dnbinom</span>(<span class="dv">0</span>, <span class="dt">mu =</span> <span class="kw">fitted</span>(mod_nb1), <span class="dt">size =</span> mod_nb1<span class="op">$</span>theta))),<span class="dv">0</span>)</code></pre></div>
<pre><code>    Obs Poisson      NB 
   1109     190    1101 </code></pre>
<p>There are at least two ways to tackle this problem.</p>
<ul>
<li>Fitting a model which deliberately inflates the number of zeros that are fitted</li>
<li>Fitting a hurdle model</li>
</ul>
<p>We’ll look at those options, next.</p>
</div>
<div id="the-zero-inflated-poisson-regression-model" class="section level2">
<h2><span class="header-section-number">18.13</span> The Zero-Inflated Poisson Regression Model</h2>
<p>The zero-inflated Poisson or (ZIP) model is used to describe count data with an excess of zero counts<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a>. The model posits that there are two processes involved:</p>
<ul>
<li>a logit model is used to predict excess zeros</li>
<li>while a Poisson model is used to predict the counts, generally</li>
</ul>
<p>The <code>pscl</code> package is used here, which can conflict with the <code>countreg</code> package we used to fit rootograms. That’s why I’m loading it here.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pscl)</code></pre></div>
<pre><code>Classes and Methods for R developed in the
Political Science Computational Laboratory
Department of Political Science
Stanford University
Simon Jackman
hurdle and zeroinfl functions by Achim Zeileis</code></pre>
<pre><code>
Attaching package: &#39;pscl&#39;</code></pre>
<pre><code>The following objects are masked from &#39;package:countreg&#39;:

    hurdle, hurdle.control, hurdletest, zeroinfl, zeroinfl.control</code></pre>
<p>To run the zero-inflated Poisson model, we use the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_zip1 &lt;-<span class="st"> </span><span class="kw">zeroinfl</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100, 
                     <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_zip1)</code></pre></div>
<pre><code>
Call:
zeroinfl(formula = physhealth ~ bmi_c + smoke100, data = sm_oh_A_young)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-1.4159 -0.6461 -0.5315 -0.2562 11.0564 

Count model coefficients (poisson with log link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 1.897651   0.022696  83.613   &lt;2e-16 ***
bmi_c       0.013984   0.001698   8.235   &lt;2e-16 ***
smoke100    0.437642   0.030107  14.536   &lt;2e-16 ***

Zero-inflation model coefficients (binomial with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.921514   0.069911  13.181  &lt; 2e-16 ***
bmi_c       -0.050103   0.007793  -6.430 1.28e-10 ***
smoke100    -0.416061   0.110166  -3.777 0.000159 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 

Number of iterations in BFGS optimization: 14 
Log-likelihood: -4178 on 6 Df</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(mod_zip1)</code></pre></div>
<pre><code>                        2.5 %      97.5 %
count_(Intercept)  1.85316852  1.94213386
count_bmi_c        0.01065621  0.01731264
count_smoke100     0.37863312  0.49664993
zero_(Intercept)   0.78449191  1.05853706
zero_bmi_c        -0.06537671 -0.03482990
zero_smoke100     -0.63198306 -0.20013969</code></pre>
<p>The output describes two separate regression models. Below the model call, we see information on a Poisson regression model. Then we see another block describing the inflation model.</p>
<p>Each predictor (<code>bmi_c</code> and <code>smoke100</code>) appears to be statistically significant in each part of the model.</p>
<div id="comparison-to-a-null-model" class="section level3">
<h3><span class="header-section-number">18.13.1</span> Comparison to a null model</h3>
<p>To show that this model fits better than the null model (the model with intercept only), we can compare them directly with a chi-squared test. Since we have two predictors in the full model, the degrees of freedom for this test is 2.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_zipnull &lt;-<span class="st"> </span><span class="kw">zeroinfl</span>(physhealth <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, 
                     <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_zipnull)</code></pre></div>
<pre><code>
Call:
zeroinfl(formula = physhealth ~ 1, data = sm_oh_A_young)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.6355 -0.6355 -0.6355 -0.1720  6.3162 

Count model coefficients (poisson with log link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  2.14401    0.01495   143.4   &lt;2e-16 ***

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.74753    0.05298   14.11   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 

Number of iterations in BFGS optimization: 9 
Log-likelihood: -4351 on 2 Df</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pchisq</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">logLik</span>(mod_zip1) <span class="op">-</span><span class="st"> </span><span class="kw">logLik</span>(mod_zipnull)), <span class="dt">df =</span> <span class="dv">2</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>&#39;log Lik.&#39; 1.470287e-75 (df=6)</code></pre>
</div>
<div id="comparison-to-a-poisson-model-with-the-vuong-test" class="section level3">
<h3><span class="header-section-number">18.13.2</span> Comparison to a Poisson Model with the Vuong test</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vuong</span>(mod_zip1, mod_poiss1)</code></pre></div>
<pre><code>Vuong Non-Nested Hypothesis Test-Statistic: 
(test-statistic is asymptotically distributed N(0,1) under the
 null that the models are indistinguishible)
-------------------------------------------------------------
              Vuong z-statistic             H_A    p-value
Raw                    15.54767 model1 &gt; model2 &lt; 2.22e-16
AIC-corrected          15.53492 model1 &gt; model2 &lt; 2.22e-16
BIC-corrected          15.50049 model1 &gt; model2 &lt; 2.22e-16</code></pre>
<p>Certainly, the ZIP model is a significant improvement over the standard Poisson model, as the Vuong test reveals.</p>
</div>
<div id="the-fitted-equation-5" class="section level3">
<h3><span class="header-section-number">18.13.3</span> The Fitted Equation</h3>
<p>The form of the model equation for a zero-inflated Poisson regression requires us to take two separate models into account. First we have a logistic regression model to predict the log odds of zero <code>physhealth</code> days…</p>
<pre><code>logit(physhealth = 0) = 0.919 - 0.051 bmi_c - 0.411 smoke100</code></pre>
<p>That takes care of the <em>extra</em> zeros. Then, to predict the number of <code>physhealth</code> days, we have:</p>
<pre><code>log(physhealth) = 1.90 + 0.014 bmi_c + 0.441 smoke100</code></pre>
<p>which may produce some additional zero count estimates.</p>
</div>
<div id="interpreting-the-coefficients-5" class="section level3">
<h3><span class="header-section-number">18.13.4</span> Interpreting the Coefficients</h3>
<p>We can exponentiate the logistic regression coefficients to obtain results in terms of odds ratios for that model, and that can be of some help in understanding the process behind excess zeros.</p>
<p>Also, exponentiating the coefficients of the count model help us describe those counts on the original scale of <code>physhealth</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">coef</span>(mod_zip1))</code></pre></div>
<pre><code>count_(Intercept)       count_bmi_c    count_smoke100  zero_(Intercept) 
        6.6702090         1.0140827         1.5490495         2.5130936 
       zero_bmi_c     zero_smoke100 
        0.9511312         0.6596398 </code></pre>
<p>For example,</p>
<ul>
<li>in the model for <code>physhealth</code> = 0, the odds of <code>physhealth</code> = 0 are 66% as high for subjects with <code>smoke100</code> = 1 as for non-smokers with the same BMI.</li>
<li>in the Poisson model for <code>physhealth</code>, the <code>physhealth</code> count is estimated to increase by 1.55 for smokers as compared to non-smokers with the same BMI.</li>
</ul>
</div>
<div id="testing-the-predictors-5" class="section level3">
<h3><span class="header-section-number">18.13.5</span> Testing the Predictors</h3>
<p>We can test the model with and without <code>bmi_c</code>, for example, by fitting the model both ways, and comparing the results with either a Wald or Likelihood Ratio test, each of which is available in the <code>lmtest</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_zip1_nobmi &lt;-<span class="st"> </span><span class="kw">zeroinfl</span>(physhealth <span class="op">~</span><span class="st"> </span>smoke100, 
                     <span class="dt">data =</span> sm_oh_A_young)

lmtest<span class="op">::</span><span class="kw">waldtest</span>(mod_zip1, mod_zip1_nobmi)</code></pre></div>
<pre><code>Wald test

Model 1: physhealth ~ bmi_c + smoke100
Model 2: physhealth ~ smoke100
  Res.Df Df Chisq Pr(&gt;Chisq)    
1   1628                        
2   1630 -2 109.3  &lt; 2.2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lmtest<span class="op">::</span><span class="kw">lrtest</span>(mod_zip1, mod_zip1_nobmi)</code></pre></div>
<pre><code>Likelihood ratio test

Model 1: physhealth ~ bmi_c + smoke100
Model 2: physhealth ~ smoke100
  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
1   6 -4178.5                         
2   4 -4232.0 -2 106.96  &lt; 2.2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="store-fitted-values-and-residuals-5" class="section level3">
<h3><span class="header-section-number">18.13.6</span> Store fitted values and residuals</h3>
<p>The <code>broom</code> package does not work with the <code>zeroinfl</code> tool. So we need to build up the fitted values and residuals ourselves.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_zip1 &lt;-<span class="st"> </span>sm_oh_A_young <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">fitted =</span> <span class="kw">fitted</span>(mod_zip1, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>),
           <span class="dt">resid =</span> <span class="kw">resid</span>(mod_zip1, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>))

sm_zip1 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(physhealth, fitted, resid) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">head</span>()</code></pre></div>
<pre><code># A tibble: 6 x 3
  physhealth fitted   resid
       &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;
1          0   1.76 -1.76  
2          0   2.53 -2.53  
3          0   1.67 -1.67  
4          2   2.04 -0.0357
5          4   5.02 -1.02  
6          6   1.55  4.45  </code></pre>
</div>
<div id="modeled-number-of-zero-counts" class="section level3">
<h3><span class="header-section-number">18.13.7</span> Modeled Number of Zero Counts</h3>
<p>The zero-inflated model is designed to perfectly match the number of observed zeros. We can compare the observed number of zero <code>physhealth</code> results to the expected number of zero values from the likelihood-based models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">c</span>(<span class="st">&quot;Obs&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(sm_oh_A_young<span class="op">$</span>physhealth <span class="op">==</span><span class="st"> </span><span class="dv">0</span>),
  <span class="st">&quot;Poisson&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dpois</span>(<span class="dv">0</span>, <span class="kw">fitted</span>(mod_poiss1))),
  <span class="st">&quot;NB&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dnbinom</span>(<span class="dv">0</span>, <span class="dt">mu =</span> <span class="kw">fitted</span>(mod_nb1), <span class="dt">size =</span> mod_nb1<span class="op">$</span>theta)),
  <span class="st">&quot;ZIP&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">predict</span>(mod_zip1, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)[,<span class="dv">1</span>])), <span class="dv">0</span>)</code></pre></div>
<pre><code>    Obs Poisson      NB     ZIP 
   1109     190    1101    1109 </code></pre>
</div>
<div id="rootogram-for-zip-model" class="section level3">
<h3><span class="header-section-number">18.13.8</span> Rootogram for ZIP model</h3>
<p>Here’s the rootogram for the zero-inflated Poisson model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">countreg<span class="op">::</span><span class="kw">rootogram</span>(mod_zip1, <span class="dt">max =</span> <span class="dv">30</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-404-1.png" width="672" /></p>
<p>The zero frequencies are perfectly matched here, but we can see that counts of 1 and 2 are now substantially underfit, and values between 6 and 13 are overfit.</p>
</div>
<div id="specify-the-r2-and-log-likelihood-values" class="section level3">
<h3><span class="header-section-number">18.13.9</span> Specify the R<sup>2</sup> and log (likelihood) values</h3>
<p>We can calculate a proxy for R<sup>2</sup> as the squared correlation of the fitted values and the observed values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The correlation of observed and fitted values</span>
(zip_r &lt;-<span class="st"> </span><span class="kw">with</span>(sm_zip1, <span class="kw">cor</span>(physhealth, fitted)))</code></pre></div>
<pre><code>[1] 0.2236014</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># R-square</span>
zip_r<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>[1] 0.04999758</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">logLik</span>(mod_zip1)</code></pre></div>
<pre><code>&#39;log Lik.&#39; -4178.496 (df=6)</code></pre>
<p>Here, we have</p>
<table>
<thead>
<tr class="header">
<th align="right">Model</th>
<th align="right">Scale</th>
<th align="right">R<sup>2</sup></th>
<th align="right">log(likelihood)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Zero-Inflated Poisson</td>
<td align="right">Complex: log(<code>physhealth</code>)</td>
<td align="right">.050</td>
<td align="right">-4184.1</td>
</tr>
</tbody>
</table>
</div>
<div id="check-model-assumptions-5" class="section level3">
<h3><span class="header-section-number">18.13.10</span> Check model assumptions</h3>
<p>Here is a plot of residuals vs. fitted values on the original <code>physhealth</code> scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sm_zip1, <span class="kw">aes</span>(<span class="dt">x =</span> fitted, <span class="dt">y =</span> resid)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Residuals vs. Fitted `physhealth`&quot;</span>,
         <span class="dt">subtitle =</span> <span class="st">&quot;Zero-Inflated Poisson Regression model&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-407-1.png" width="672" /></p>
</div>
<div id="predictions-for-harry-and-sally-5" class="section level3">
<h3><span class="header-section-number">18.13.11</span> Predictions for Harry and Sally</h3>
<p>The predictions from this ZIP regression model are obtained as follows…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(mod_zip1, <span class="dt">newdata =</span> hs_data, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre></div>
<pre><code>       1        2 
5.928536 1.470896 </code></pre>
<p>As we’ve seen in the past, when we use <code>response</code> as the type, the predictions fall on the original <code>physhealth</code> scale. The prediction for Harry is 5.92 days, and for Sally is 1.47 days.</p>
</div>
</div>
<div id="the-zero-inflated-negative-binomial-regression-model" class="section level2">
<h2><span class="header-section-number">18.14</span> The Zero-Inflated Negative Binomial Regression Model</h2>
<p>As an alternative to the ZIP model, we might consider a zero-inflated negative binomial regression<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a>. This will involve a logistic regression to predict the probability of a 0, and then a negative binomial model to describe the counts of <code>physhealth</code>.</p>
<p>To run the zero-inflated negative binomial model, we use the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_zinb1 &lt;-<span class="st"> </span><span class="kw">zeroinfl</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100, 
                      <span class="dt">dist =</span> <span class="st">&quot;negbin&quot;</span>, <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_zinb1)</code></pre></div>
<pre><code>
Call:
zeroinfl(formula = physhealth ~ bmi_c + smoke100, data = sm_oh_A_young, 
    dist = &quot;negbin&quot;)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.5432 -0.3914 -0.3471 -0.1542  7.7462 

Count model coefficients (negbin with log link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.353151   0.126131  10.728  &lt; 2e-16 ***
bmi_c        0.020419   0.007825   2.609  0.00907 ** 
smoke100     0.573530   0.135112   4.245 2.19e-05 ***
Log(theta)  -1.028636   0.172175  -5.974 2.31e-09 ***

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.04144    0.20698   0.200   0.8413    
bmi_c       -0.06160    0.01354  -4.550 5.37e-06 ***
smoke100    -0.34373    0.17023  -2.019   0.0435 *  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 

Theta = 0.3575 
Number of iterations in BFGS optimization: 25 
Log-likelihood: -2564 on 7 Df</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(mod_zinb1)</code></pre></div>
<pre><code>                         2.5 %      97.5 %
count_(Intercept)  1.105938639  1.60036316
count_bmi_c        0.005081859  0.03575643
count_smoke100     0.308714260  0.83834496
zero_(Intercept)  -0.364229869  0.44711077
zero_bmi_c        -0.088141789 -0.03506528
zero_smoke100     -0.677368528 -0.01009594</code></pre>
<div id="comparison-to-a-null-model-1" class="section level3">
<h3><span class="header-section-number">18.14.1</span> Comparison to a null model</h3>
<p>To show that this model fits better than the null model (the model with intercept only), we can compare them directly with a chi-squared test. Since we have two predictors in the full model, the degrees of freedom for this test is 2.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_zinbnull &lt;-<span class="st"> </span><span class="kw">zeroinfl</span>(physhealth <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">dist =</span> <span class="st">&quot;negbin&quot;</span>,
                     <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_zinbnull)</code></pre></div>
<pre><code>
Call:
zeroinfl(formula = physhealth ~ 1, data = sm_oh_A_young, dist = &quot;negbin&quot;)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.3700 -0.3700 -0.3700 -0.1002  3.6775 

Count model coefficients (negbin with log link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   1.5356     0.1499  10.246  &lt; 2e-16 ***
Log(theta)   -1.3053     0.2330  -5.603  2.1e-08 ***

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -0.3660     0.3352  -1.092    0.275
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 

Theta = 0.2711 
Number of iterations in BFGS optimization: 13 
Log-likelihood: -2603 on 3 Df</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pchisq</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">logLik</span>(mod_nb1) <span class="op">-</span><span class="st"> </span><span class="kw">logLik</span>(mod_zinbnull)), <span class="dt">df =</span> <span class="dv">2</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>&#39;log Lik.&#39; 4.170115e-10 (df=4)</code></pre>
</div>
<div id="comparison-to-a-negative-binomial-model-vuong-test" class="section level3">
<h3><span class="header-section-number">18.14.2</span> Comparison to a Negative Binomial Model: Vuong test</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vuong</span>(mod_zinb1, mod_nb1)</code></pre></div>
<pre><code>Vuong Non-Nested Hypothesis Test-Statistic: 
(test-statistic is asymptotically distributed N(0,1) under the
 null that the models are indistinguishible)
-------------------------------------------------------------
              Vuong z-statistic             H_A   p-value
Raw                    3.113058 model1 &gt; model2 0.0009258
AIC-corrected          2.574484 model1 &gt; model2 0.0050195
BIC-corrected          1.120661 model1 &gt; model2 0.1312160</code></pre>
<p>The zero-inflated negative binomial model is a significant improvement over the standard negative binomial model according to the the raw or AIC-corrected Vuong tests, but not according to the BIC-corrected test.</p>
</div>
<div id="the-fitted-equation-6" class="section level3">
<h3><span class="header-section-number">18.14.3</span> The Fitted Equation</h3>
<p>Like the ZIP, the zero-inflated negative binomial regression also requires us to take two separate models into account. First we have a logistic regression model to predict the log odds of zero <code>physhealth</code> days…</p>
<pre><code>logit(physhealth = 0) = 0.044 - 0.062 bmi_c - 0.337 smoke100</code></pre>
<p>That takes care of the <em>extra</em> zeros. Then, to predict the number of <code>physhealth</code> days, we have:</p>
<pre><code>log(physhealth) = 1.36 + 0.020 bmi_c + 0.575 smoke100</code></pre>
<p>and there’s a <span class="math inline">\(\theta\)</span> term, which is estimated as <code>exp(-1.023)</code> or 0.36, and this negative binomial regression model may also produce some additional zero count estimates.</p>
</div>
<div id="interpreting-the-coefficients-6" class="section level3">
<h3><span class="header-section-number">18.14.4</span> Interpreting the Coefficients</h3>
<p>As with the zip, we can exponentiate the logistic regression coefficients to obtain results in terms of odds ratios for that model, and that can be of some help in understanding the process behind excess zeros.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">coef</span>(mod_zinb1))</code></pre></div>
<pre><code>count_(Intercept)       count_bmi_c    count_smoke100  zero_(Intercept) 
        3.8695991         1.0206290         1.7745194         1.0423111 
       zero_bmi_c     zero_smoke100 
        0.9402556         0.7091188 </code></pre>
<p>For example,</p>
<ul>
<li>in the model for <code>physhealth</code> = 0, the odds of <code>physhealth</code> = 0 are 71.4% as high for subjects with <code>smoke100</code> = 1 as for non-smokers with the same BMI.</li>
</ul>
<p>Interpreting the negative binomial piece works the same way as it did in the negative binomial regression.</p>
</div>
<div id="testing-the-predictors-6" class="section level3">
<h3><span class="header-section-number">18.14.5</span> Testing the Predictors</h3>
<p>We can test the model with and without <code>bmi_c</code>, for example, by fitting the model both ways, and comparing the results with either a Wald or Likelihood Ratio test, each of which is available in the <code>lmtest</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_zinb1_nobmi &lt;-<span class="st"> </span><span class="kw">zeroinfl</span>(physhealth <span class="op">~</span><span class="st"> </span>smoke100, 
                            <span class="dt">dist =</span> <span class="st">&quot;negbin&quot;</span>,
                            <span class="dt">data =</span> sm_oh_A_young)

lmtest<span class="op">::</span><span class="kw">waldtest</span>(mod_zinb1, mod_zinb1_nobmi)</code></pre></div>
<pre><code>Wald test

Model 1: physhealth ~ bmi_c + smoke100
Model 2: physhealth ~ smoke100
  Res.Df Df  Chisq Pr(&gt;Chisq)    
1   1627                         
2   1629 -2 33.611  5.028e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lmtest<span class="op">::</span><span class="kw">lrtest</span>(mod_zinb1, mod_zinb1_nobmi)</code></pre></div>
<pre><code>Likelihood ratio test

Model 1: physhealth ~ bmi_c + smoke100
Model 2: physhealth ~ smoke100
  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
1   7 -2564.3                         
2   5 -2587.1 -2 45.442  1.357e-10 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="store-fitted-values-and-residuals-6" class="section level3">
<h3><span class="header-section-number">18.14.6</span> Store fitted values and residuals</h3>
<p>Again, we need to build up the fitted values and residuals without the <code>broom</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_zinb1 &lt;-<span class="st"> </span>sm_oh_A_young <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">fitted =</span> <span class="kw">fitted</span>(mod_zinb1, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>),
           <span class="dt">resid =</span> <span class="kw">resid</span>(mod_zinb1, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>))

sm_zip1 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(physhealth, fitted, resid) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">head</span>()</code></pre></div>
<pre><code># A tibble: 6 x 3
  physhealth fitted   resid
       &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;
1          0   1.76 -1.76  
2          0   2.53 -2.53  
3          0   1.67 -1.67  
4          2   2.04 -0.0357
5          4   5.02 -1.02  
6          6   1.55  4.45  </code></pre>
</div>
<div id="modeled-number-of-zero-counts-1" class="section level3">
<h3><span class="header-section-number">18.14.7</span> Modeled Number of Zero Counts</h3>
<p>Once again, we can compare the observed number of zero <code>physhealth</code> results to the expected number of zero values from the likelihood-based models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">c</span>(<span class="st">&quot;Obs&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(sm_oh_A_young<span class="op">$</span>physhealth <span class="op">==</span><span class="st"> </span><span class="dv">0</span>),
  <span class="st">&quot;Poisson&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dpois</span>(<span class="dv">0</span>, <span class="kw">fitted</span>(mod_poiss1))),
  <span class="st">&quot;NB&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dnbinom</span>(<span class="dv">0</span>, <span class="dt">mu =</span> <span class="kw">fitted</span>(mod_nb1), <span class="dt">size =</span> mod_nb1<span class="op">$</span>theta)),
  <span class="st">&quot;ZIP&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">predict</span>(mod_zip1, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)[,<span class="dv">1</span>]),
  <span class="st">&quot;ZINB&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">predict</span>(mod_zinb1, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)[,<span class="dv">1</span>])),<span class="dv">0</span>)</code></pre></div>
<pre><code>    Obs Poisson      NB     ZIP    ZINB 
   1109     190    1101    1109    1111 </code></pre>
<p>So, the Poisson model is clearly inappropriate, but the zero-inflated (Poisson and NB) and the negative binomial model all give reasonable fits in this regard.</p>
</div>
<div id="rootogram-for-zero-inflated-negative-binomial-model" class="section level3">
<h3><span class="header-section-number">18.14.8</span> Rootogram for Zero-Inflated Negative Binomial model</h3>
<p>Here’s the rootogram for the zero-inflated negative binomial model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">countreg<span class="op">::</span><span class="kw">rootogram</span>(mod_zinb1, <span class="dt">max =</span> <span class="dv">30</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-417-1.png" width="672" /></p>
<p>As in the ZIP model, the zero frequencies are perfectly matched here, but we can see that counts of 1 and 2 are now closer to the data we observe than in the ZIP model. We are still substantially underfitting values of 30.</p>
</div>
<div id="specify-the-r2-and-log-likelihood-values-1" class="section level3">
<h3><span class="header-section-number">18.14.9</span> Specify the R<sup>2</sup> and log (likelihood) values</h3>
<p>We can calculate a proxy for R<sup>2</sup> as the squared correlation of the fitted values and the observed values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The correlation of observed and fitted values</span>
(zinb_r &lt;-<span class="st"> </span><span class="kw">with</span>(sm_zinb1, <span class="kw">cor</span>(physhealth, fitted)))</code></pre></div>
<pre><code>[1] 0.2218276</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># R-square</span>
zinb_r<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>[1] 0.04920749</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">logLik</span>(mod_zinb1)</code></pre></div>
<pre><code>&#39;log Lik.&#39; -2564.34 (df=7)</code></pre>
<p>Here, we have</p>
<table>
<thead>
<tr class="header">
<th align="right">Model</th>
<th align="right">Scale</th>
<th align="right">R<sup>2</sup></th>
<th align="right">log(likelihood)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Zero-Inflated Negative Binomial</td>
<td align="right">Complex: log(<code>physhealth</code>)</td>
<td align="right">.049</td>
<td align="right">-2567.8</td>
</tr>
</tbody>
</table>
</div>
<div id="check-model-assumptions-6" class="section level3">
<h3><span class="header-section-number">18.14.10</span> Check model assumptions</h3>
<p>Here is a plot of residuals vs. fitted values on the original <code>physhealth</code> scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sm_zinb1, <span class="kw">aes</span>(<span class="dt">x =</span> fitted, <span class="dt">y =</span> resid)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Residuals vs. Fitted `physhealth`&quot;</span>,
         <span class="dt">subtitle =</span> <span class="st">&quot;Zero-Inflated Negative Binomial Regression model&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-420-1.png" width="672" /></p>
</div>
<div id="predictions-for-harry-and-sally-6" class="section level3">
<h3><span class="header-section-number">18.14.11</span> Predictions for Harry and Sally</h3>
<p>The predictions from this zero-inflated negative binomial regression model are obtained as follows…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(mod_zinb1, <span class="dt">newdata =</span> hs_data, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre></div>
<pre><code>       1        2 
6.019361 1.444829 </code></pre>
<p>As we’ve seen in the past, when we use <code>response</code> as the type, the predictions fall on the original <code>physhealth</code> scale. The prediction for Harry is 6.01 days, and for Sally is 1.45 days.</p>
</div>
</div>
<div id="a-hurdle-model-with-poisson" class="section level2">
<h2><span class="header-section-number">18.15</span> A “hurdle” model (with Poisson)</h2>
<p>Much of the discussion here of hurdle models comes from Clay Ford at the University of Virginia<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a>. Ford describes a hurdle model as follows:</p>
<blockquote>
<p>The hurdle model is a two-part model that specifies one process for zero counts and another process for positive counts. The idea is that positive counts occur once a threshold is crossed, or put another way, a hurdle is cleared. If the hurdle is not cleared, then we have a count of 0.</p>
</blockquote>
<blockquote>
<p>The first part of the model is typically a binary logit model. This models whether an observation takes a positive count or not. The second part of the model is usually a truncated Poisson or Negative Binomial model. Truncated means we’re only fitting positive counts. If we were to fit a hurdle model to our [medicare] data, the interpretation would be that one process governs whether a patient visits a doctor or not, and another process governs how many visits are made.</p>
</blockquote>
<p>To fit a hurdle model, we’ll use the <code>hurdle</code> function in the <code>pscl</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_hur1 &lt;-<span class="st"> </span><span class="kw">hurdle</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100,
                   <span class="dt">dist =</span> <span class="st">&quot;poisson&quot;</span>, <span class="dt">zero.dist =</span> <span class="st">&quot;binomial&quot;</span>,
                   <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_hur1)</code></pre></div>
<pre><code>
Call:
hurdle(formula = physhealth ~ bmi_c + smoke100, data = sm_oh_A_young, 
    dist = &quot;poisson&quot;, zero.dist = &quot;binomial&quot;)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-1.4170 -0.6460 -0.5316 -0.2561 11.0526 

Count model coefficients (truncated poisson with log link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 1.897676   0.022696  83.612   &lt;2e-16 ***
bmi_c       0.013985   0.001698   8.235   &lt;2e-16 ***
smoke100    0.437649   0.030107  14.537   &lt;2e-16 ***
Zero hurdle model coefficients (binomial with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.923456   0.069872 -13.216  &lt; 2e-16 ***
bmi_c        0.050172   0.007791   6.440  1.2e-10 ***
smoke100     0.417888   0.110144   3.794 0.000148 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 

Number of iterations in BFGS optimization: 14 
Log-likelihood: -4178 on 6 Df</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(mod_hur1)</code></pre></div>
<pre><code>                        2.5 %      97.5 %
count_(Intercept)  1.85319158  1.94215943
count_bmi_c        0.01065671  0.01731347
count_smoke100     0.37864083  0.49665711
zero_(Intercept)  -1.06040337 -0.78650834
zero_bmi_c         0.03490142  0.06544259
zero_smoke100      0.20201037  0.63376638</code></pre>
<p>We are using the default settings here, using the same predictors for both models:</p>
<ul>
<li>a <strong>Binomial</strong> model to predict the probability of <code>physhealth</code> = 0 given our predictors, as specified by the <code>zero.dist</code> argument in the <code>hurdle</code> function, and</li>
<li>a (truncated) <strong>Poisson</strong> model to predict the positive-count of <code>physhealth</code> given those same predictors, as specified by the <code>dist</code> argument in the <code>hurdle</code> function.</li>
</ul>
<div id="comparison-to-a-null-model-2" class="section level3">
<h3><span class="header-section-number">18.15.1</span> Comparison to a null model</h3>
<p>To show that this model fits better than the null model (the model with intercept only), we can compare them directly with a chi-squared test. Since we have two predictors in the full model, the degrees of freedom for this test is 2.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_hurnull &lt;-<span class="st"> </span><span class="kw">hurdle</span>(physhealth <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">dist =</span> <span class="st">&quot;poisson&quot;</span>, 
                      <span class="dt">zero.dist =</span> <span class="st">&quot;binomial&quot;</span>, 
                      <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_hurnull)</code></pre></div>
<pre><code>
Call:
hurdle(formula = physhealth ~ 1, data = sm_oh_A_young, dist = &quot;poisson&quot;, 
    zero.dist = &quot;binomial&quot;)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.6355 -0.6355 -0.6355 -0.1720  6.3162 

Count model coefficients (truncated poisson with log link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  2.14401    0.01495   143.4   &lt;2e-16 ***
Zero hurdle model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.74782    0.05298  -14.12   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 

Number of iterations in BFGS optimization: 8 
Log-likelihood: -4351 on 2 Df</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pchisq</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">logLik</span>(mod_hur1) <span class="op">-</span><span class="st"> </span><span class="kw">logLik</span>(mod_hurnull)), <span class="dt">df =</span> <span class="dv">2</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>&#39;log Lik.&#39; 1.46309e-75 (df=6)</code></pre>
</div>
<div id="comparison-to-a-poisson-model-vuong-test" class="section level3">
<h3><span class="header-section-number">18.15.2</span> Comparison to a Poisson Model: Vuong test</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vuong</span>(mod_hur1, mod_poiss1)</code></pre></div>
<pre><code>Vuong Non-Nested Hypothesis Test-Statistic: 
(test-statistic is asymptotically distributed N(0,1) under the
 null that the models are indistinguishible)
-------------------------------------------------------------
              Vuong z-statistic             H_A    p-value
Raw                    15.54734 model1 &gt; model2 &lt; 2.22e-16
AIC-corrected          15.53459 model1 &gt; model2 &lt; 2.22e-16
BIC-corrected          15.50016 model1 &gt; model2 &lt; 2.22e-16</code></pre>
<p>The hurdle model is a significant improvement over the standard Poisson model according to this test.</p>
</div>
<div id="comparison-to-a-zero-inflated-poisson-model-vuong-test" class="section level3">
<h3><span class="header-section-number">18.15.3</span> Comparison to a Zero-Inflated Poisson Model: Vuong test</h3>
<p>Is the hurdle model comparable to the zero-inflated Poisson?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vuong</span>(mod_hur1, mod_zip1)</code></pre></div>
<pre><code>Vuong Non-Nested Hypothesis Test-Statistic: 
(test-statistic is asymptotically distributed N(0,1) under the
 null that the models are indistinguishible)
-------------------------------------------------------------
              Vuong z-statistic             H_A p-value
Raw                   0.4811757 model1 &gt; model2  0.3152
AIC-corrected         0.4811757 model1 &gt; model2  0.3152
BIC-corrected         0.4811757 model1 &gt; model2  0.3152</code></pre>
<p>The hurdle model is not a significant improvement over the zero-inflated Poisson model according to this test.</p>
</div>
<div id="the-fitted-equation-7" class="section level3">
<h3><span class="header-section-number">18.15.4</span> The Fitted Equation</h3>
<p>The form of the model equation for this hurdle also requires us to take two separate models into account. First we have a logistic regression model to predict the log odds of zero <code>physhealth</code> days…</p>
<pre><code>logit(physhealth = 0) = 0.921 - 0.051 bmi_c - 0.413 smoke100</code></pre>
<p>That takes care of the zeros. Then, to predict the number of <code>physhealth</code> days, we use the following truncated Poisson model:</p>
<pre><code>log(physhealth) = 1.90 + 0.014 bmi_c + 0.441 smoke100</code></pre>
<p>which is truncated to produce only estimates greater than zero.</p>
</div>
<div id="interpreting-the-coefficients-7" class="section level3">
<h3><span class="header-section-number">18.15.5</span> Interpreting the Coefficients</h3>
<p>We can exponentiate the logistic regression coefficients to obtain results in terms of odds ratios for that model, and that can be of some help in understanding the process behind excess zeros.</p>
<p>Also, exponentiating the coefficients of the count model help us describe those counts on the original scale of <code>physhealth</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">coef</span>(mod_hur1))</code></pre></div>
<pre><code>count_(Intercept)       count_bmi_c    count_smoke100  zero_(Intercept) 
        6.6703712         1.0140833         1.5490611         0.3971442 
       zero_bmi_c     zero_smoke100 
        1.0514519         1.5187511 </code></pre>
<p>For example,</p>
<ul>
<li>in the model for <code>physhealth</code> = 0, the odds of <code>physhealth</code> = 0 are 151% as high for subjects with <code>smoke100</code> = 1 as for non-smokers with the same BMI.</li>
<li>in the Poisson model for <code>physhealth</code>, the <code>physhealth</code> count is estimated to increase by 1.55 for smokers as compared to non-smokers with the same BMI.</li>
</ul>
</div>
<div id="testing-the-predictors-7" class="section level3">
<h3><span class="header-section-number">18.15.6</span> Testing the Predictors</h3>
<p>We can test the model with and without <code>bmi_c</code>, for example, by fitting the model both ways, and comparing the results with either a Wald or Likelihood Ratio test, each of which is available in the <code>lmtest</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_hur1_nobmi &lt;-<span class="st"> </span><span class="kw">hurdle</span>(physhealth <span class="op">~</span><span class="st"> </span>smoke100,
                         <span class="dt">dist =</span> <span class="st">&quot;poisson&quot;</span>, 
                         <span class="dt">zero.dist =</span> <span class="st">&quot;binomial&quot;</span>,
                         <span class="dt">data =</span> sm_oh_A_young)

lmtest<span class="op">::</span><span class="kw">waldtest</span>(mod_hur1, mod_hur1_nobmi)</code></pre></div>
<pre><code>Wald test

Model 1: physhealth ~ bmi_c + smoke100
Model 2: physhealth ~ smoke100
  Res.Df Df  Chisq Pr(&gt;Chisq)    
1   1628                         
2   1630 -2 109.29  &lt; 2.2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lmtest<span class="op">::</span><span class="kw">lrtest</span>(mod_hur1, mod_hur1_nobmi)</code></pre></div>
<pre><code>Likelihood ratio test

Model 1: physhealth ~ bmi_c + smoke100
Model 2: physhealth ~ smoke100
  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
1   6 -4178.5                         
2   4 -4232.0 -2 106.97  &lt; 2.2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="store-fitted-values-and-residuals-7" class="section level3">
<h3><span class="header-section-number">18.15.7</span> Store fitted values and residuals</h3>
<p>The <code>broom</code> package does not work with the <code>hurdle</code> class of models. Again we need to build up the fitted values and residuals ourselves.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_hur1 &lt;-<span class="st"> </span>sm_oh_A_young <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">fitted =</span> <span class="kw">fitted</span>(mod_hur1, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>),
           <span class="dt">resid =</span> <span class="kw">resid</span>(mod_hur1, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>))

sm_hur1 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(physhealth, fitted, resid) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">head</span>()</code></pre></div>
<pre><code># A tibble: 6 x 3
  physhealth fitted   resid
       &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;
1          0   1.76 -1.76  
2          0   2.53 -2.53  
3          0   1.67 -1.67  
4          2   2.04 -0.0353
5          4   5.02 -1.02  
6          6   1.55  4.45  </code></pre>
</div>
<div id="modeled-number-of-zero-counts-2" class="section level3">
<h3><span class="header-section-number">18.15.8</span> Modeled Number of Zero Counts</h3>
<p>Once again, we can compare the observed number of zero <code>physhealth</code> results to the expected number of zero values from the likelihood-based models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">c</span>(<span class="st">&quot;Obs&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(sm_oh_A_young<span class="op">$</span>physhealth <span class="op">==</span><span class="st"> </span><span class="dv">0</span>),
  <span class="st">&quot;Poisson&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dpois</span>(<span class="dv">0</span>, <span class="kw">fitted</span>(mod_poiss1))),
  <span class="st">&quot;NB&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dnbinom</span>(<span class="dv">0</span>, <span class="dt">mu =</span> <span class="kw">fitted</span>(mod_nb1), <span class="dt">size =</span> mod_nb1<span class="op">$</span>theta)),
  <span class="st">&quot;ZIP&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">predict</span>(mod_zip1, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)[,<span class="dv">1</span>]),
  <span class="st">&quot;ZINB&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">predict</span>(mod_zinb1, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)[,<span class="dv">1</span>]),
  <span class="st">&quot;Hurdle&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">predict</span>(mod_hur1, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)[,<span class="dv">1</span>])),<span class="dv">0</span>)</code></pre></div>
<pre><code>    Obs Poisson      NB     ZIP    ZINB  Hurdle 
   1109     190    1101    1109    1111    1109 </code></pre>
<p>The hurdle model does about as well as the negative binomial and zero-inflated models. All but the Poisson give reasonable fits in this regard.</p>
</div>
<div id="rootogram-for-hurdle-model" class="section level3">
<h3><span class="header-section-number">18.15.9</span> Rootogram for Hurdle Model</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">countreg<span class="op">::</span><span class="kw">rootogram</span>(mod_hur1, <span class="dt">max =</span> <span class="dv">30</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-431-1.png" width="672" /></p>
<p>The results are still not perfect, of course. In fitting the zeros exactly, we’re underfitting counts of 1, 2, and 30, and overfitting many of the counts between 6 and 20. We still have a problem here with overdispersion. That’s why we’ll consider a hurdle model with a negative binomial regression for the counts in a moment.</p>
</div>
<div id="understanding-the-modeled-counts-in-detail" class="section level3">
<h3><span class="header-section-number">18.15.10</span> Understanding the Modeled Counts in Detail</h3>
<p>The expected mean count uses both parts of the hurdle model. Mathematically, we want…</p>
<p><span class="math display">\[
E[y | {\bf x}] = \frac{1 - f_1(0 | {\bf x})}{1 - f_2(0 | {\bf x})} \mu_2({\bf x})
\]</span></p>
<p>where</p>
<ul>
<li>our count of <code>physhealth</code> is <span class="math inline">\(y\)</span></li>
<li>our predictors are represented by <strong>x</strong></li>
<li>and the expected count is the product of a ratio and a mean.</li>
</ul>
<blockquote>
<p>The ratio is the probability of a non-zero in the first process divided the probability of a non-zero in the second untruncated process. The f symbols represent distributions. Recall these are logistic and Poisson, respectively, by default but can be others. The mean is for the untruncated version of the positive-count process.</p>
</blockquote>
<p>If we want to see the expected hurdle counts, we can get them using some clever applications of the <code>predict</code> function.</p>
<p>The first six expected mean counts (<span class="math inline">\(E[y | {\bf x}]\)</span> from the equation above) are:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">predict</span>(mod_hur1, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>))</code></pre></div>
<pre><code>       1        2        3        4        5        6 
1.756341 2.527945 1.671933 2.035348 5.018026 1.551978 </code></pre>
<p>The ratio of non-zero probabilities, <span class="math inline">\(\frac{1 - f_1(0 | {\bf x})}{1 - f_2(0 | {\bf x})}\)</span>, from the mathematical expression above can be extracted by:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">predict</span>(mod_hur1, <span class="dt">type =</span> <span class="st">&quot;zero&quot;</span>))</code></pre></div>
<pre><code>        1         2         3         4         5         6 
0.2690804 0.3487780 0.2596470 0.2991808 0.5499740 0.2459557 </code></pre>
<p>The mean for the untruncated process, <span class="math inline">\(\mu_2({\bf x})\)</span>, can also be obtained by:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">predict</span>(mod_hur1, <span class="dt">type =</span> <span class="st">&quot;count&quot;</span>))</code></pre></div>
<pre><code>       1        2        3        4        5        6 
6.527200 7.248006 6.439253 6.803070 9.124116 6.309990 </code></pre>
<p>and we can multiply these last two pieces together to verify that they match our expected hurdle counts.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">predict</span>(mod_hur1, <span class="dt">type =</span> <span class="st">&quot;zero&quot;</span>) <span class="op">*</span><span class="st"> </span><span class="kw">predict</span>(mod_hur1, <span class="dt">type =</span> <span class="st">&quot;count&quot;</span>),<span class="dv">5</span>)</code></pre></div>
<pre><code>       1        2        3        4        5 
1.756341 2.527945 1.671933 2.035348 5.018026 </code></pre>
</div>
<div id="specify-the-r2-and-log-likelihood-values-2" class="section level3">
<h3><span class="header-section-number">18.15.11</span> Specify the R<sup>2</sup> and log (likelihood) values</h3>
<p>We can calculate a proxy for R<sup>2</sup> as the squared correlation of the fitted values and the observed values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The correlation of observed and fitted values</span>
(hur1_r &lt;-<span class="st"> </span><span class="kw">with</span>(sm_hur1, <span class="kw">cor</span>(physhealth, fitted)))</code></pre></div>
<pre><code>[1] 0.2235982</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># R-square</span>
hur1_r<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>[1] 0.04999615</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">logLik</span>(mod_hur1)</code></pre></div>
<pre><code>&#39;log Lik.&#39; -4178.491 (df=6)</code></pre>
<p>Here, we have</p>
<table>
<thead>
<tr class="header">
<th align="right">Model</th>
<th align="right">Scale</th>
<th align="right">R<sup>2</sup></th>
<th align="right">log(likelihood)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Hurdle Model (Poisson)</td>
<td align="right">Complex: log(<code>physhealth</code>)</td>
<td align="right">.050</td>
<td align="right">-4184.1</td>
</tr>
</tbody>
</table>
</div>
<div id="check-model-assumptions-7" class="section level3">
<h3><span class="header-section-number">18.15.12</span> Check model assumptions</h3>
<p>Here is a plot of residuals vs. fitted values on the original <code>physhealth</code> scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sm_hur1, <span class="kw">aes</span>(<span class="dt">x =</span> fitted, <span class="dt">y =</span> resid)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Residuals vs. Fitted `physhealth`&quot;</span>,
         <span class="dt">subtitle =</span> <span class="st">&quot;Hurdle model with Poisson counts&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-438-1.png" width="672" /></p>
</div>
<div id="predictions-for-harry-and-sally-7" class="section level3">
<h3><span class="header-section-number">18.15.13</span> Predictions for Harry and Sally</h3>
<p>The predictions from this zero-inflated negative binomial regression model are obtained as follows…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(mod_hur1, <span class="dt">newdata =</span> hs_data, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre></div>
<pre><code>       1        2 
5.930506 1.471289 </code></pre>
<p>As we’ve seen in the past, when we use <code>response</code> as the type, the predictions fall on the original <code>physhealth</code> scale. The prediction for Harry is 5.93 days, and for Sally is 1.47 days.</p>
</div>
</div>
<div id="a-hurdle-model-with-negative-binomial-for-overdispersion" class="section level2">
<h2><span class="header-section-number">18.16</span> A “hurdle” model (with negative binomial for overdispersion)</h2>
<p>Let’s account for overdispersion better with a negative binomial model for the counts in our hurdle model. We specify that the positive-count process be fit with this NB model using <code>dist = negbin</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_hur_nb1 &lt;-<span class="st"> </span><span class="kw">hurdle</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100,
                   <span class="dt">dist =</span> <span class="st">&quot;negbin&quot;</span>, <span class="dt">zero.dist =</span> <span class="st">&quot;binomial&quot;</span>,
                   <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_hur_nb1)</code></pre></div>
<pre><code>
Call:
hurdle(formula = physhealth ~ bmi_c + smoke100, data = sm_oh_A_young, 
    dist = &quot;negbin&quot;, zero.dist = &quot;binomial&quot;)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.5763 -0.3843 -0.3424 -0.1548  7.4930 

Count model coefficients (truncated negbin with log link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.305260   0.144512   9.032  &lt; 2e-16 ***
bmi_c        0.018524   0.008131   2.278   0.0227 *  
smoke100     0.573519   0.138747   4.134 3.57e-05 ***
Log(theta)  -1.123030   0.206483  -5.439 5.36e-08 ***
Zero hurdle model coefficients (binomial with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.923456   0.069872 -13.216  &lt; 2e-16 ***
bmi_c        0.050172   0.007791   6.440  1.2e-10 ***
smoke100     0.417888   0.110144   3.794 0.000148 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 

Theta: count = 0.3253
Number of iterations in BFGS optimization: 17 
Log-likelihood: -2563 on 7 Df</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(mod_hur_nb1)</code></pre></div>
<pre><code>                         2.5 %      97.5 %
count_(Intercept)  1.022021965  1.58849765
count_bmi_c        0.002586927  0.03446173
count_smoke100     0.301580258  0.84545695
zero_(Intercept)  -1.060403367 -0.78650834
zero_bmi_c         0.034901419  0.06544259
zero_smoke100      0.202010366  0.63376638</code></pre>
<div id="comparison-to-a-null-model-3" class="section level3">
<h3><span class="header-section-number">18.16.1</span> Comparison to a null model</h3>
<p>To show that this model fits better than the null model (the model with intercept only), we can compare them directly with a chi-squared test. Since we have two predictors in the full model, the degrees of freedom for this test is 2.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_hur_nb_null &lt;-<span class="st"> </span><span class="kw">hurdle</span>(physhealth <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">dist =</span> <span class="st">&quot;negbin&quot;</span>, 
                      <span class="dt">zero.dist =</span> <span class="st">&quot;binomial&quot;</span>, 
                      <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_hur_nb_null)</code></pre></div>
<pre><code>
Call:
hurdle(formula = physhealth ~ 1, data = sm_oh_A_young, dist = &quot;negbin&quot;, 
    zero.dist = &quot;binomial&quot;)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.3700 -0.3700 -0.3700 -0.1002  3.6775 

Count model coefficients (truncated negbin with log link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   1.5356     0.1499  10.246  &lt; 2e-16 ***
Log(theta)   -1.3053     0.2330  -5.603  2.1e-08 ***
Zero hurdle model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.74782    0.05298  -14.12   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 

Theta: count = 0.2711
Number of iterations in BFGS optimization: 11 
Log-likelihood: -2603 on 3 Df</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pchisq</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">logLik</span>(mod_hur_nb1) <span class="op">-</span><span class="st"> </span><span class="kw">logLik</span>(mod_hur_nb_null)), <span class="dt">df =</span> <span class="dv">2</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>&#39;log Lik.&#39; 3.064911e-18 (df=7)</code></pre>
</div>
<div id="comparison-to-a-negative-binomial-model-vuong-test-1" class="section level3">
<h3><span class="header-section-number">18.16.2</span> Comparison to a Negative Binomial Model: Vuong test</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vuong</span>(mod_hur_nb1, mod_nb1)</code></pre></div>
<pre><code>Vuong Non-Nested Hypothesis Test-Statistic: 
(test-statistic is asymptotically distributed N(0,1) under the
 null that the models are indistinguishible)
-------------------------------------------------------------
              Vuong z-statistic             H_A    p-value
Raw                    3.224162 model1 &gt; model2 0.00063171
AIC-corrected          2.707707 model1 &gt; model2 0.00338749
BIC-corrected          1.313591 model1 &gt; model2 0.09449192</code></pre>
<p>The hurdle model is a significant improvement over the standard negative binomial model according to the raw and AIC-corrected versions of this test, but not the BIC-corrected version.</p>
</div>
<div id="comparison-to-a-zero-inflated-nb-model-vuong-test" class="section level3">
<h3><span class="header-section-number">18.16.3</span> Comparison to a Zero-Inflated NB Model: Vuong test</h3>
<p>Is the hurdle model comparable to the zero-inflated Poisson?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vuong</span>(mod_hur_nb1, mod_zinb1)</code></pre></div>
<pre><code>Vuong Non-Nested Hypothesis Test-Statistic: 
(test-statistic is asymptotically distributed N(0,1) under the
 null that the models are indistinguishible)
-------------------------------------------------------------
              Vuong z-statistic             H_A  p-value
Raw                    1.625551 model1 &gt; model2 0.052023
AIC-corrected          1.625551 model1 &gt; model2 0.052023
BIC-corrected          1.625551 model1 &gt; model2 0.052023</code></pre>
<p>The hurdle model is just barely a significant improvement over the zero-inflated Negative Binomial model.</p>
</div>
<div id="comparing-the-hurdle-models-with-aic-and-bic" class="section level3">
<h3><span class="header-section-number">18.16.4</span> Comparing the Hurdle Models with AIC and BIC</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">AIC</span>(mod_hur1); <span class="kw">BIC</span>(mod_hur1)</code></pre></div>
<pre><code>[1] 8368.981</code></pre>
<pre><code>[1] 8401.374</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">AIC</span>(mod_hur_nb1); <span class="kw">BIC</span>(mod_hur_nb1)</code></pre></div>
<pre><code>[1] 5139.903</code></pre>
<pre><code>[1] 5177.695</code></pre>
<p>The negative binomial approach certainly looks better than the Poisson here.</p>
</div>
<div id="the-fitted-equation-8" class="section level3">
<h3><span class="header-section-number">18.16.5</span> The Fitted Equation</h3>
<p>The form of the model equation for this hurdle also requires us to take two separate models into account. First we have a logistic regression model to predict the log odds of zero <code>physhealth</code> days…</p>
<pre><code>logit(physhealth = 0) = -0.921 + 0.051 bmi_c + 0.413 smoke100</code></pre>
<p>That takes care of the zeros. Then, to predict the number of <code>physhealth</code> days, we use the following truncated negative binomial model:</p>
<pre><code>log(physhealth) = 1.30 + 0.018 bmi_c + 0.576 smoke100</code></pre>
<p>which is truncated to produce only estimates greater than zero, with <span class="math inline">\(\theta\)</span> estimated as <code>exp(-1.123)</code> or 0.325.</p>
</div>
<div id="interpreting-the-coefficients-8" class="section level3">
<h3><span class="header-section-number">18.16.6</span> Interpreting the Coefficients</h3>
<p>We can exponentiate the logistic regression coefficients to obtain results in terms of odds ratios for that model, and that can be of some help in understanding the process behind excess zeros.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="kw">coef</span>(mod_hur_nb1))</code></pre></div>
<pre><code>count_(Intercept)       count_bmi_c    count_smoke100  zero_(Intercept) 
        3.6886473         1.0186970         1.7744998         0.3971442 
       zero_bmi_c     zero_smoke100 
        1.0514519         1.5187511 </code></pre>
<p>For example,</p>
<ul>
<li>in the model for <code>physhealth</code> = 0, the odds of <code>physhealth</code> = 0 are 151% as high for subjects with <code>smoke100</code> = 1 as for non-smokers with the same BMI.</li>
</ul>
</div>
<div id="testing-the-predictors-8" class="section level3">
<h3><span class="header-section-number">18.16.7</span> Testing the Predictors</h3>
<p>We can test the model with and without <code>bmi_c</code>, for example, by fitting the model both ways, and comparing the results with either a Wald or Likelihood Ratio test, each of which is available in the <code>lmtest</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_hurnb1_nobmi &lt;-<span class="st"> </span><span class="kw">hurdle</span>(physhealth <span class="op">~</span><span class="st"> </span>smoke100,
                         <span class="dt">dist =</span> <span class="st">&quot;negbin&quot;</span>, 
                         <span class="dt">zero.dist =</span> <span class="st">&quot;binomial&quot;</span>,
                         <span class="dt">data =</span> sm_oh_A_young)

lmtest<span class="op">::</span><span class="kw">waldtest</span>(mod_hur_nb1, mod_hurnb1_nobmi)</code></pre></div>
<pre><code>Wald test

Model 1: physhealth ~ bmi_c + smoke100
Model 2: physhealth ~ smoke100
  Res.Df Df  Chisq Pr(&gt;Chisq)    
1   1627                         
2   1629 -2 46.657  7.388e-11 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lmtest<span class="op">::</span><span class="kw">lrtest</span>(mod_hur_nb1, mod_hurnb1_nobmi)</code></pre></div>
<pre><code>Likelihood ratio test

Model 1: physhealth ~ bmi_c + smoke100
Model 2: physhealth ~ smoke100
  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
1   7 -2562.9                         
2   5 -2587.1 -2 48.218  3.385e-11 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="store-fitted-values-and-residuals-8" class="section level3">
<h3><span class="header-section-number">18.16.8</span> Store fitted values and residuals</h3>
<p>Again we need to build up the fitted values and residuals, without <code>broom</code> to help.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_hur_nb1 &lt;-<span class="st"> </span>sm_oh_A_young <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">fitted =</span> <span class="kw">fitted</span>(mod_hur_nb1, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>),
           <span class="dt">resid =</span> <span class="kw">resid</span>(mod_hur_nb1, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>))

sm_hur_nb1 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(physhealth, fitted, resid) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">head</span>()</code></pre></div>
<pre><code># A tibble: 6 x 3
  physhealth fitted   resid
       &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;
1          0   1.74 -1.74  
2          0   2.51 -2.51  
3          0   1.65 -1.65  
4          2   2.01 -0.0138
5          4   5.03 -1.03  
6          6   1.53  4.47  </code></pre>
</div>
<div id="rootogram-for-nb-hurdle-model" class="section level3">
<h3><span class="header-section-number">18.16.9</span> Rootogram for NB Hurdle Model</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">countreg<span class="op">::</span><span class="kw">rootogram</span>(mod_hur_nb1, <span class="dt">max =</span> <span class="dv">30</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-450-1.png" width="672" /></p>
<p>This improves the situation, but we’re still underfitting the 30s.</p>
</div>
<div id="specify-the-r2-and-log-likelihood-values-3" class="section level3">
<h3><span class="header-section-number">18.16.10</span> Specify the R<sup>2</sup> and log (likelihood) values</h3>
<p>We can calculate a proxy for R<sup>2</sup> as the squared correlation of the fitted values and the observed values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The correlation of observed and fitted values</span>
(hurnb1_r &lt;-<span class="st"> </span><span class="kw">with</span>(sm_hur_nb1, <span class="kw">cor</span>(physhealth, fitted)))</code></pre></div>
<pre><code>[1] 0.2232941</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># R-square</span>
hurnb1_r<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>[1] 0.04986026</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">logLik</span>(mod_hur_nb1)</code></pre></div>
<pre><code>&#39;log Lik.&#39; -2562.952 (df=7)</code></pre>
<p>Here, we have</p>
<table>
<thead>
<tr class="header">
<th align="right">Model</th>
<th align="right">Scale</th>
<th align="right">R<sup>2</sup></th>
<th align="right">log(likelihood)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Hurdle Model (Neg. Bin.)</td>
<td align="right">Complex: log(<code>physhealth</code>)</td>
<td align="right">.050</td>
<td align="right">-2566.3</td>
</tr>
</tbody>
</table>
</div>
<div id="check-model-assumptions-8" class="section level3">
<h3><span class="header-section-number">18.16.11</span> Check model assumptions</h3>
<p>Here is a plot of residuals vs. fitted values on the original <code>physhealth</code> scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sm_hur_nb1, <span class="kw">aes</span>(<span class="dt">x =</span> fitted, <span class="dt">y =</span> resid)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Residuals vs. Fitted `physhealth`&quot;</span>,
         <span class="dt">subtitle =</span> <span class="st">&quot;Hurdle model with Negative Binomial counts&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-453-1.png" width="672" /></p>
</div>
<div id="predictions-for-harry-and-sally-8" class="section level3">
<h3><span class="header-section-number">18.16.12</span> Predictions for Harry and Sally</h3>
<p>The predictions from this zero-inflated negative binomial regression model are obtained as follows…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(mod_hur_nb1, <span class="dt">newdata =</span> hs_data, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre></div>
<pre><code>       1        2 
6.047813 1.453596 </code></pre>
<p>The prediction for Harry is 6.04 days, and for Sally is 1.45 days.</p>
</div>
<div id="note-fitting-a-different-hurdle-model-for-counts-and-przero" class="section level3">
<h3><span class="header-section-number">18.16.13</span> Note: Fitting a Different Hurdle Model for Counts and Pr(zero)</h3>
<p>Suppose we wanted to use only <code>bmi_c</code> to predict the probability of a zero count, but use both predictors in the model for the positive counts. We use the <code>|</code> command.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_hur_new1 &lt;-<span class="st"> </span>
<span class="st">    </span><span class="kw">hurdle</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100 <span class="op">|</span><span class="st"> </span>bmi_c,
           <span class="dt">dist =</span> <span class="st">&quot;negbin&quot;</span>, <span class="dt">zero.dist =</span> <span class="st">&quot;binomial&quot;</span>,
           <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_hur_new1)</code></pre></div>
<pre><code>
Call:
hurdle(formula = physhealth ~ bmi_c + smoke100 | bmi_c, data = sm_oh_A_young, 
    dist = &quot;negbin&quot;, zero.dist = &quot;binomial&quot;)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.5681 -0.3809 -0.3488 -0.1517  7.0650 

Count model coefficients (truncated negbin with log link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.305260   0.144512   9.032  &lt; 2e-16 ***
bmi_c        0.018524   0.008131   2.278   0.0227 *  
smoke100     0.573519   0.138747   4.134 3.57e-05 ***
Log(theta)  -1.123030   0.206483  -5.439 5.36e-08 ***
Zero hurdle model coefficients (binomial with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.762681   0.053905 -14.149  &lt; 2e-16 ***
bmi_c        0.051171   0.007757   6.597  4.2e-11 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 

Theta: count = 0.3253
Number of iterations in BFGS optimization: 17 
Log-likelihood: -2570 on 6 Df</code></pre>
</div>
<div id="hanging-rootogram-for-this-new-hurdle-model" class="section level3">
<h3><span class="header-section-number">18.16.14</span> Hanging Rootogram for this new Hurdle Model</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">countreg<span class="op">::</span><span class="kw">rootogram</span>(mod_hur_new1, <span class="dt">max =</span> <span class="dv">30</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-456-1.png" width="672" /></p>
<p>Not a meaningful improvement, certainly.</p>
</div>
</div>
<div id="a-tobit-censored-regression-model" class="section level2">
<h2><span class="header-section-number">18.17</span> A Tobit (Censored) Regression Model</h2>
<p>The idea of the <strong>tobit</strong> model (sometimes called a <strong>censored regression</strong> model) is to estimate associations for outcomes where we can see either left-censoring (censoring from below) or right-censoring (censoring from above.)</p>
<p>Consider the variable <code>physhealth</code>, which is restricted to fall between 0 and 30 by the way the measure was constructed. But supposed we think about a broader and latent (unobserved) variable describing physical health. Among the people with <code>physhealth</code> = 0, some would be incredible athletes and others would be in much poorer physical health but still healthy enough to truthfully answer 0. On the other end, some of the people responding 30 are in substantially worse physical health than others with that same response.</p>
<ul>
<li>Censoring from below takes place when values at or below a threshold (in this case 0) take that value.</li>
<li>Censoring from above takes place when values at or above a threshold (here, 30) take that value.</li>
</ul>
<p>Several examples of tobit analysis are available at <a href="https://stats.idre.ucla.edu/r/dae/tobit-models/" class="uri">https://stats.idre.ucla.edu/r/dae/tobit-models/</a>, which is my primary source for the material here on those models.</p>
<p>The tobit model postulates that the value 0 in our model is just the lower limit of the underlying measure of poor physical health that we would actually observe in the population if we had a stronger measure. Similarly, we’ll postulate that 30 is just the upper limit of “poor health” that we can see. The approach I’ll take to run the tobit model comes from the <code>vglm</code> function in the <code>VGAM</code> package.</p>
<p>Here’s the model, and its summary. Note that the default Lower value for a tobit model is 0, so we didn’t technically have to list that here.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_tob1 &lt;-<span class="st"> </span><span class="kw">vglm</span>(physhealth <span class="op">~</span><span class="st"> </span>bmi_c <span class="op">+</span><span class="st"> </span>smoke100, 
                 <span class="kw">tobit</span>(<span class="dt">Lower =</span> <span class="dv">0</span>, <span class="dt">Upper =</span> <span class="dv">30</span>),
                 <span class="dt">type.fitted =</span> <span class="st">&quot;censored&quot;</span>,
                 <span class="dt">data =</span> sm_oh_A_young)

<span class="kw">summary</span>(mod_tob1)</code></pre></div>
<pre><code>
Call:
vglm(formula = physhealth ~ bmi_c + smoke100, family = tobit(Lower = 0, 
    Upper = 30), data = sm_oh_A_young, type.fitted = &quot;censored&quot;)


Pearson residuals:
            Min      1Q  Median      3Q   Max
mu       -4.963 -0.6059 -0.4371  0.9127 2.558
loge(sd) -1.065 -0.3039 -0.2862 -0.1981 9.019

Coefficients: 
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept):1 -11.02997    0.84462 -13.059  &lt; 2e-16 ***
(Intercept):2   2.81768    0.03553  79.310  &lt; 2e-16 ***
bmi_c           0.49105    0.07289   6.737 1.62e-11 ***
smoke100        5.69619    1.06207   5.363 8.17e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Number of linear predictors:  2 

Names of linear predictors: mu, loge(sd)

Log-likelihood: -2563.135 on 3264 degrees of freedom

Number of iterations: 7 

No Hauck-Donner effect found in any of the estimates</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(mod_tob1)</code></pre></div>
<pre><code>                    2.5 %     97.5 %
(Intercept):1 -12.6854002 -9.3745415
(Intercept):2   2.7480495  2.8873146
bmi_c           0.3481883  0.6339102
smoke100        3.6145827  7.7778052</code></pre>
<div id="the-fitted-equation-9" class="section level3">
<h3><span class="header-section-number">18.17.1</span> The Fitted Equation</h3>
<p>Because we’ve used the censoring approach, our model will limit its predictions to the range of [0, 30], where any predictions outside that range are censored. Values below 0 are fitted as 0, and values above 30 are fitted as 30.</p>
<p>The model equation is</p>
<pre><code>physhealth = -11.00 + 0.49 bmi_c + 5.67 smoke100</code></pre>
</div>
<div id="interpreting-the-coefficients-9" class="section level3">
<h3><span class="header-section-number">18.17.2</span> Interpreting the Coefficients</h3>
<p>Tobit model regression coefficients are interpreted as we would a set of OLS coefficients, <em>except</em> that the linear effect is on the uncensored <em>latent variable</em>, rather than on the observed outcome.</p>
<p>In our case,</p>
<ul>
<li>a one-unit increase in <code>bmi_c</code> is associated with a 0.49 day increase in the predicted value of <code>physhealth</code>, holding <code>smoke100</code> constant</li>
<li>a move from <code>smoke100</code> = 0 to 1 is associated with a 5.67 day increase in the predicted value of <code>physhealth</code>, holding <code>bmi_c</code> constant</li>
<li>the coefficient labeled <code>(Intercept):1</code> is the intercept for the model and is the predicted value of <code>physhealth</code> when <code>smoke100</code> = 0 and <code>bmi_c</code> = 0. Note that this value is -11, which is outside the range of <code>physhealth</code> values we observed.</li>
<li>the coefficient labeled <code>(Intercept):2</code> is a statistic we can use after we exponentiate it, as follows:
<ul>
<li>here <code>(Intercept):2</code> = 2.82, and exp(2.82) = 16.7768507, which is analogous to the square root of the residual variance in OLS regression, which is summarized for our OLS model as <code>Residual standard error: 6.609</code>.</li>
</ul></li>
</ul>
</div>
<div id="testing-the-predictors-9" class="section level3">
<h3><span class="header-section-number">18.17.3</span> Testing the Predictors</h3>
<p>We can test the model with and without <code>bmi_c</code>, for example, by fitting the model both ways, and comparing the results with either a Wald or Likelihood Ratio test, each of which is available in the <code>lmtest</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_tob_nobmi &lt;-<span class="st"> </span><span class="kw">vglm</span>(physhealth <span class="op">~</span><span class="st"> </span>smoke100, 
                      <span class="kw">tobit</span>(<span class="dt">Lower =</span> <span class="dv">0</span>, <span class="dt">Upper =</span> <span class="dv">30</span>),
                      <span class="dt">type.fitted =</span> <span class="st">&quot;censored&quot;</span>,
                      <span class="dt">data =</span> sm_oh_A_young)

lmtest<span class="op">::</span><span class="kw">waldtest</span>(mod_tob1, mod_tob_nobmi)</code></pre></div>
<pre><code>Wald test

Model 1: physhealth ~ bmi_c + smoke100
Model 2: physhealth ~ smoke100
  Res.Df Df  Chisq Pr(&gt;Chisq)    
1   3264                         
2   3265 -1 45.386  1.618e-11 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The likelihood ratio test we have used in some other settings isn’t available here.</p>
</div>
<div id="store-fitted-values-and-residuals-9" class="section level3">
<h3><span class="header-section-number">18.17.4</span> Store fitted values and residuals</h3>
<p>The residuals and fitted values from the tobit model can be stored and then summarized in several ways:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sm_tob1 &lt;-<span class="st"> </span>sm_oh_A_young <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">fitted =</span> <span class="kw">fitted</span>(mod_tob1,
                           <span class="dt">type.fitted =</span> <span class="st">&quot;censored&quot;</span>),
           <span class="dt">resid =</span> physhealth <span class="op">-</span><span class="st"> </span>fitted)

sm_tob1 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(physhealth, fitted, resid) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">head</span>()</code></pre></div>
<pre><code># A tibble: 6 x 3
  physhealth fitted resid
       &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;
1          0     0.    0.
2          0     0.    0.
3          0     0.    0.
4          2     0.    2.
5          4     0.    4.
6          6     0.    6.</code></pre>
</div>
<div id="building-something-like-a-rootogram" class="section level3">
<h3><span class="header-section-number">18.17.5</span> Building Something Like a Rootogram</h3>
<p>Building a rootogram is tricky for a tobit model, to say the least, but we can approximate a piece of the issue by plotting the rounded fitted values against the observed <code>physhealth</code> data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sm_tob1, <span class="kw">aes</span>(<span class="dt">x =</span> physhealth, <span class="dt">y =</span> <span class="kw">round</span>(fitted))) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_jitter</span>(<span class="dt">width =</span> <span class="fl">0.2</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-460-1.png" width="672" /></p>
<p>Note that the model never predicts a subject to have an underlying <code>physhealth</code> worse than about 12 (remember that larger numbers are worse here.)</p>
</div>
<div id="tables-of-the-observed-and-fitted-physhealth-from-tobit" class="section level3">
<h3><span class="header-section-number">18.17.6</span> Tables of the Observed and Fitted <code>physhealth</code> from Tobit</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">addmargins</span>(<span class="kw">table</span>(<span class="kw">round</span>(sm_tob1<span class="op">$</span>physhealth)))</code></pre></div>
<pre><code>
   0    1    2    3    4    5    6    7    8    9   10   12   13   14   15 
1109   93  112   52   24   43    5   32    1    1   19    3    1   25   26 
  16   17   18   20   21   24   25   26   27   28   30  Sum 
   1    1    1    8    6    1    1    1    1    1   66 1634 </code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">addmargins</span>(<span class="kw">table</span>(<span class="kw">round</span>(sm_tob1<span class="op">$</span>fitted)))</code></pre></div>
<pre><code>
   0    1    2    3    4    5    6    7    8   10   11   13  Sum 
1588   14    8    9    1    5    2    2    1    1    2    1 1634 </code></pre>
</div>
<div id="specify-the-r2-and-log-likelihood-values-4" class="section level3">
<h3><span class="header-section-number">18.17.7</span> Specify the R<sup>2</sup> and log (likelihood) values</h3>
<p>We can calculate a proxy for R<sup>2</sup> as the squared correlation of the fitted values and the observed values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The correlation of observed and fitted values</span>
(tob1_r &lt;-<span class="st"> </span><span class="kw">with</span>(sm_tob1, <span class="kw">cor</span>(physhealth, fitted)))</code></pre></div>
<pre><code>          [,1]
[1,] 0.1440334</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># R-square</span>
tob1_r<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>           [,1]
[1,] 0.02074562</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">logLik</span>(mod_tob1)</code></pre></div>
<pre><code>[1] -2563.135</code></pre>
<p>Here, we have</p>
<table>
<thead>
<tr class="header">
<th align="right">Model</th>
<th align="right">Scale</th>
<th align="right">R<sup>2</sup></th>
<th align="right">log(likelihood)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Tobit</td>
<td align="right"><code>physhealth</code></td>
<td align="right">.021</td>
<td align="right">-2567.3</td>
</tr>
</tbody>
</table>
</div>
<div id="check-model-assumptions-9" class="section level3">
<h3><span class="header-section-number">18.17.8</span> Check model assumptions</h3>
<p>Here is a plot of residuals vs. fitted values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sm_tob1, <span class="kw">aes</span>(<span class="dt">x =</span> fitted, <span class="dt">y =</span> resid)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Residuals vs. Fitted Values for Tobit 1&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-465-1.png" width="672" /></p>
<p>Here is a normal Q-Q plot of the Tobit Model 1 residuals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qqnorm</span>(sm_tob1<span class="op">$</span>resid)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-466-1.png" width="672" /></p>
</div>
<div id="predictions-for-harry-and-sally-9" class="section level3">
<h3><span class="header-section-number">18.17.9</span> Predictions for Harry and Sally</h3>
<p>The predictions from this tobit model are obtained as follows…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(mod_tob1, <span class="dt">newdata =</span> hs_data, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre></div>
<pre><code>         [,1]
1  -0.4232846
2 -13.4852170</code></pre>
<p>The prediction for both Harry and Sally under the tobit model would be truncated to 0 days.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="12">
<li id="fn12"><p>This discussion is motivated by Section 6.2 of Gelman and Hill.<a href="modeling-a-count-outcome-in-ohio-smart.html#fnref12">↩</a></p></li>
<li id="fn13"><p>See <a href="http://data.library.virginia.edu/getting-started-with-negative-binomial-regression-modeling/" class="uri">http://data.library.virginia.edu/getting-started-with-negative-binomial-regression-modeling/</a><a href="modeling-a-count-outcome-in-ohio-smart.html#fnref13">↩</a></p></li>
<li id="fn14"><p>See Zeileis A Kleiber C Jackman S <em>Regression Models for Count Data in R</em> Vignette at <a href="https://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf" class="uri">https://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf</a><a href="modeling-a-count-outcome-in-ohio-smart.html#fnref14">↩</a></p></li>
<li id="fn15"><p>See <a href="https://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf" class="uri">https://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf</a> for more details.<a href="modeling-a-count-outcome-in-ohio-smart.html#fnref15">↩</a></p></li>
<li id="fn16"><p>This <span class="math inline">\(\theta\)</span> is the inverse of the dispersion parameter estimated for these models by most other software packages, like SAS, Stata and SPSS. See <a href="https://stats.idre.ucla.edu/r/dae/negative-binomial-regression/" class="uri">https://stats.idre.ucla.edu/r/dae/negative-binomial-regression/</a> for more details.<a href="modeling-a-count-outcome-in-ohio-smart.html#fnref16">↩</a></p></li>
<li id="fn17"><p>See <a href="http://data.library.virginia.edu/getting-started-with-negative-binomial-regression-modeling/" class="uri">http://data.library.virginia.edu/getting-started-with-negative-binomial-regression-modeling/</a><a href="modeling-a-count-outcome-in-ohio-smart.html#fnref17">↩</a></p></li>
<li id="fn18"><p>See <a href="https://stats.idre.ucla.edu/r/dae/zip/" class="uri">https://stats.idre.ucla.edu/r/dae/zip/</a> for more on the zero-inflated poisson model.<a href="modeling-a-count-outcome-in-ohio-smart.html#fnref18">↩</a></p></li>
<li id="fn19"><p>See <a href="https://stats.idre.ucla.edu/r/dae/zinb/" class="uri">https://stats.idre.ucla.edu/r/dae/zinb/</a><a href="modeling-a-count-outcome-in-ohio-smart.html#fnref19">↩</a></p></li>
<li id="fn20"><p><a href="http://data.library.virginia.edu/getting-started-with-hurdle-models/" class="uri">http://data.library.virginia.edu/getting-started-with-hurdle-models/</a> is an excellent introduction, by Clay Ford, a Statistical Research Consultant at the University of Virginia Library. I can also recommend <a href="https://rpubs.com/kaz_yos/pscl-2" class="uri">https://rpubs.com/kaz_yos/pscl-2</a> as a place to learn more about the <code>pscl</code> package, and the fitting and interpretation of both hurdle and zero-inflated regression models. That <code>rpubs</code> site has a link to <a href="https://www.ncbi.nlm.nih.gov/pubmed/21854279">this article by Hu, Pavlicova and Nunes from the Am J Drug Alcohol Abuse</a> which provides a real set of examples from a trial of a behavioral health intervention meant to reduce the risk of unprotected sexual occasions as part of a strategy to reduce HIV risk.<a href="modeling-a-count-outcome-in-ohio-smart.html#fnref20">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="cleaning-the-brfss-smart-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="modeling-an-ordinal-categorical-outcome-in-ohio-smart.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/18_countoutcomes.Rmd",
"text": "Edit"
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
