<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Science for Biological, Medical and Health Research: Notes for 432</title>
  <meta name="description" content="These are the Course Notes for 432.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Data Science for Biological, Medical and Health Research: Notes for 432" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the Course Notes for 432." />
  <meta name="github-repo" content="thomaselove/432-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Science for Biological, Medical and Health Research: Notes for 432" />
  
  <meta name="twitter:description" content="These are the Course Notes for 432." />
  

<meta name="author" content="Thomas E. Love, Ph.D.">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="using-ols-from-the-rms-package-to-fit-linear-models.html">
<link rel="next" href="logistic-regression-the-foundations.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">432 Course Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="r-packages-used-in-these-notes.html"><a href="r-packages-used-in-these-notes.html"><i class="fa fa-check"></i>R Packages used in these notes</a></li>
<li class="chapter" data-level="" data-path="data-used-in-these-notes.html"><a href="data-used-in-these-notes.html"><i class="fa fa-check"></i>Data used in these notes</a></li>
<li class="chapter" data-level="" data-path="special-functions-used-in-these-notes.html"><a href="special-functions-used-in-these-notes.html"><i class="fa fa-check"></i>Special Functions used in these notes</a></li>
<li class="chapter" data-level="1" data-path="building-table-1.html"><a href="building-table-1.html"><i class="fa fa-check"></i><b>1</b> Building Table 1</a><ul>
<li class="chapter" data-level="1.1" data-path="building-table-1.html"><a href="building-table-1.html#two-examples-from-the-new-england-journal-of-medicine"><i class="fa fa-check"></i><b>1.1</b> Two examples from the <em>New England Journal of Medicine</em></a><ul>
<li class="chapter" data-level="1.1.1" data-path="building-table-1.html"><a href="building-table-1.html#a-simple-table-1"><i class="fa fa-check"></i><b>1.1.1</b> A simple Table 1</a></li>
<li class="chapter" data-level="1.1.2" data-path="building-table-1.html"><a href="building-table-1.html#a-group-comparison"><i class="fa fa-check"></i><b>1.1.2</b> A group comparison</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="building-table-1.html"><a href="building-table-1.html#the-mr-clean-trial"><i class="fa fa-check"></i><b>1.2</b> The MR CLEAN trial</a></li>
<li class="chapter" data-level="1.3" data-path="building-table-1.html"><a href="building-table-1.html#simulated-fakestroke-data"><i class="fa fa-check"></i><b>1.3</b> Simulated <code>fakestroke</code> data</a></li>
<li class="chapter" data-level="1.4" data-path="building-table-1.html"><a href="building-table-1.html#building-table-1-for-fakestroke-attempt-1"><i class="fa fa-check"></i><b>1.4</b> Building Table 1 for <code>fakestroke</code>: Attempt 1</a><ul>
<li class="chapter" data-level="1.4.1" data-path="building-table-1.html"><a href="building-table-1.html#some-of-this-is-very-useful-and-other-parts-need-to-be-fixed."><i class="fa fa-check"></i><b>1.4.1</b> Some of this is very useful, and other parts need to be fixed.</a></li>
<li class="chapter" data-level="1.4.2" data-path="building-table-1.html"><a href="building-table-1.html#fakestroke-cleaning-up-categorical-variables"><i class="fa fa-check"></i><b>1.4.2</b> <code>fakestroke</code> Cleaning Up Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="building-table-1.html"><a href="building-table-1.html#fakestroke-table-1-attempt-2"><i class="fa fa-check"></i><b>1.5</b> <code>fakestroke</code> Table 1: Attempt 2</a><ul>
<li class="chapter" data-level="1.5.1" data-path="building-table-1.html"><a href="building-table-1.html#what-summaries-should-we-show"><i class="fa fa-check"></i><b>1.5.1</b> What summaries should we show?</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="building-table-1.html"><a href="building-table-1.html#obtaining-a-more-detailed-summary"><i class="fa fa-check"></i><b>1.6</b> Obtaining a more detailed Summary</a></li>
<li class="chapter" data-level="1.7" data-path="building-table-1.html"><a href="building-table-1.html#exporting-the-completed-table-1-from-r-to-excel-or-word"><i class="fa fa-check"></i><b>1.7</b> Exporting the Completed Table 1 from R to Excel or Word</a><ul>
<li class="chapter" data-level="1.7.1" data-path="building-table-1.html"><a href="building-table-1.html#approach-a-save-and-open-in-excel"><i class="fa fa-check"></i><b>1.7.1</b> Approach A: Save and open in Excel</a></li>
<li class="chapter" data-level="1.7.2" data-path="building-table-1.html"><a href="building-table-1.html#approach-b-produce-the-table-so-you-can-cut-and-paste-it"><i class="fa fa-check"></i><b>1.7.2</b> Approach B: Produce the Table so you can cut and paste it</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="building-table-1.html"><a href="building-table-1.html#a-controlled-biological-experiment---the-blood-brain-barrier"><i class="fa fa-check"></i><b>1.8</b> A Controlled Biological Experiment - The Blood-Brain Barrier</a></li>
<li class="chapter" data-level="1.9" data-path="building-table-1.html"><a href="building-table-1.html#the-bloodbrain.csv-file"><i class="fa fa-check"></i><b>1.9</b> The <code>bloodbrain.csv</code> file</a></li>
<li class="chapter" data-level="1.10" data-path="building-table-1.html"><a href="building-table-1.html#a-table-1-for-bloodbrain"><i class="fa fa-check"></i><b>1.10</b> A Table 1 for <code>bloodbrain</code></a><ul>
<li class="chapter" data-level="1.10.1" data-path="building-table-1.html"><a href="building-table-1.html#generate-final-table-1-for-bloodbrain"><i class="fa fa-check"></i><b>1.10.1</b> Generate final Table 1 for <code>bloodbrain</code></a></li>
<li class="chapter" data-level="1.10.2" data-path="building-table-1.html"><a href="building-table-1.html#a-more-finished-version-after-cleanup-in-word"><i class="fa fa-check"></i><b>1.10.2</b> A More Finished Version (after Cleanup in Word)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html"><i class="fa fa-check"></i><b>2</b> Linear Regression on a small SMART data set</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#brfss-and-smart"><i class="fa fa-check"></i><b>2.1</b> BRFSS and SMART</a><ul>
<li class="chapter" data-level="2.1.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#key-resources"><i class="fa fa-check"></i><b>2.1.1</b> Key resources</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-smartcle1-data-cookbook"><i class="fa fa-check"></i><b>2.2</b> The <code>smartcle1</code> data: Cookbook</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#smartcle2-omitting-missing-observations-complete-case-analyses"><i class="fa fa-check"></i><b>2.3</b> <code>smartcle2</code>: Omitting Missing Observations: Complete-Case Analyses</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#summarizing-the-smartcle2-data-numerically"><i class="fa fa-check"></i><b>2.4</b> Summarizing the <code>smartcle2</code> data numerically</a><ul>
<li class="chapter" data-level="2.4.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-new-toy-the-skim-function"><i class="fa fa-check"></i><b>2.4.1</b> The New Toy: The <code>skim</code> function</a></li>
<li class="chapter" data-level="2.4.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-usual-summary-for-a-data-frame"><i class="fa fa-check"></i><b>2.4.2</b> The usual <code>summary</code> for a data frame</a></li>
<li class="chapter" data-level="2.4.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-describe-function-in-hmisc"><i class="fa fa-check"></i><b>2.4.3</b> The <code>describe</code> function in <code>Hmisc</code></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#counting-as-exploratory-data-analysis"><i class="fa fa-check"></i><b>2.5</b> Counting as exploratory data analysis</a><ul>
<li class="chapter" data-level="2.5.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-many-respondents-had-exercised-in-the-past-30-days-did-this-vary-by-sex"><i class="fa fa-check"></i><b>2.5.1</b> How many respondents had exercised in the past 30 days? Did this vary by sex?</a></li>
<li class="chapter" data-level="2.5.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#whats-the-distribution-of-sleephrs"><i class="fa fa-check"></i><b>2.5.2</b> What’s the distribution of <code>sleephrs</code>?</a></li>
<li class="chapter" data-level="2.5.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#whats-the-distribution-of-bmi"><i class="fa fa-check"></i><b>2.5.3</b> What’s the distribution of <code>BMI</code>?</a></li>
<li class="chapter" data-level="2.5.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-many-of-the-respondents-have-a-bmi-below-30"><i class="fa fa-check"></i><b>2.5.4</b> How many of the respondents have a BMI below 30?</a></li>
<li class="chapter" data-level="2.5.5" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-many-of-the-respondents-who-have-a-bmi-30-exercised"><i class="fa fa-check"></i><b>2.5.5</b> How many of the respondents who have a BMI &lt; 30 exercised?</a></li>
<li class="chapter" data-level="2.5.6" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#is-obesity-associated-with-sex-in-these-data"><i class="fa fa-check"></i><b>2.5.6</b> Is obesity associated with sex, in these data?</a></li>
<li class="chapter" data-level="2.5.7" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#comparing-sleephrs-summaries-by-obesity-status"><i class="fa fa-check"></i><b>2.5.7</b> Comparing <code>sleephrs</code> summaries by obesity status</a></li>
<li class="chapter" data-level="2.5.8" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#the-skim-function-within-a-pipe"><i class="fa fa-check"></i><b>2.5.8</b> The <code>skim</code> function within a pipe</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#first-modeling-attempt-can-bmi-predict-physhealth"><i class="fa fa-check"></i><b>2.6</b> First Modeling Attempt: Can <code>bmi</code> predict <code>physhealth</code>?</a><ul>
<li class="chapter" data-level="2.6.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#fitting-a-simple-regression-model"><i class="fa fa-check"></i><b>2.6.1</b> Fitting a Simple Regression Model</a></li>
<li class="chapter" data-level="2.6.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#model-summary-for-a-simple-one-predictor-regression"><i class="fa fa-check"></i><b>2.6.2</b> Model Summary for a Simple (One-Predictor) Regression</a></li>
<li class="chapter" data-level="2.6.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#using-the-broom-package"><i class="fa fa-check"></i><b>2.6.3</b> Using the <code>broom</code> package</a></li>
<li class="chapter" data-level="2.6.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#how-does-the-model-do-residuals-vs.fitted-values"><i class="fa fa-check"></i><b>2.6.4</b> How does the model do? (Residuals vs. Fitted Values)</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#a-new-small-study-predicting-bmi"><i class="fa fa-check"></i><b>2.7</b> A New Small Study: Predicting BMI</a><ul>
<li class="chapter" data-level="2.7.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#does-female-predict-bmi-well"><i class="fa fa-check"></i><b>2.7.1</b> Does <code>female</code> predict <code>bmi</code> well?</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m1-a-simple-t-test-model"><i class="fa fa-check"></i><b>2.8</b> <code>c2_m1</code>: A simple t-test model</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m2-adding-another-predictor-two-way-anova-without-interaction"><i class="fa fa-check"></i><b>2.9</b> <code>c2_m2</code>: Adding another predictor (two-way ANOVA without interaction)</a></li>
<li class="chapter" data-level="2.10" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m3-adding-the-interaction-term-two-way-anova-with-interaction"><i class="fa fa-check"></i><b>2.10</b> <code>c2_m3</code>: Adding the interaction term (Two-way ANOVA with interaction)</a></li>
<li class="chapter" data-level="2.11" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m4-using-female-and-sleephrs-in-a-model-for-bmi"><i class="fa fa-check"></i><b>2.11</b> <code>c2_m4</code>: Using <code>female</code> and <code>sleephrs</code> in a model for <code>bmi</code></a></li>
<li class="chapter" data-level="2.12" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#making-predictions-with-a-linear-regression-model"><i class="fa fa-check"></i><b>2.12</b> Making Predictions with a Linear Regression Model</a><ul>
<li class="chapter" data-level="2.12.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#fitting-an-individual-prediction-and-95-prediction-interval"><i class="fa fa-check"></i><b>2.12.1</b> Fitting an Individual Prediction and 95% Prediction Interval</a></li>
<li class="chapter" data-level="2.12.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#confidence-interval-for-an-average-prediction"><i class="fa fa-check"></i><b>2.12.2</b> Confidence Interval for an Average Prediction</a></li>
<li class="chapter" data-level="2.12.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#fitting-multiple-individual-predictions-to-new-data"><i class="fa fa-check"></i><b>2.12.3</b> Fitting Multiple Individual Predictions to New Data</a></li>
<li class="chapter" data-level="2.12.4" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#simulation-to-represent-predictive-uncertainty-in-model-4"><i class="fa fa-check"></i><b>2.12.4</b> Simulation to represent predictive uncertainty in Model 4</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#centering-the-model"><i class="fa fa-check"></i><b>2.13</b> Centering the model</a><ul>
<li class="chapter" data-level="2.13.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#plot-of-model-4-on-centered-sleephrs-c2_m4_c"><i class="fa fa-check"></i><b>2.13.1</b> Plot of Model 4 on Centered <code>sleephrs</code>: <code>c2_m4_c</code></a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#rescaling-an-input-by-subtracting-the-mean-and-dividing-by-2-standard-deviations"><i class="fa fa-check"></i><b>2.14</b> Rescaling an input by subtracting the mean and dividing by 2 standard deviations</a><ul>
<li class="chapter" data-level="2.14.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#refitting-model-c2_m4-to-the-rescaled-data"><i class="fa fa-check"></i><b>2.14.1</b> Refitting model <code>c2_m4</code> to the rescaled data</a></li>
<li class="chapter" data-level="2.14.2" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#interpreting-the-model-on-rescaled-data"><i class="fa fa-check"></i><b>2.14.2</b> Interpreting the model on rescaled data</a></li>
<li class="chapter" data-level="2.14.3" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#plot-of-model-on-rescaled-data"><i class="fa fa-check"></i><b>2.14.3</b> Plot of model on rescaled data</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m5-what-if-we-add-more-variables"><i class="fa fa-check"></i><b>2.15</b> <code>c2_m5</code>: What if we add more variables?</a></li>
<li class="chapter" data-level="2.16" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m6-would-adding-self-reported-health-help"><i class="fa fa-check"></i><b>2.16</b> <code>c2_m6</code>: Would adding self-reported health help?</a></li>
<li class="chapter" data-level="2.17" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#c2_m7-what-if-we-added-the-menthealth-variable"><i class="fa fa-check"></i><b>2.17</b> <code>c2_m7</code>: What if we added the <code>menthealth</code> variable?</a></li>
<li class="chapter" data-level="2.18" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#key-regression-assumptions-for-building-effective-prediction-models"><i class="fa fa-check"></i><b>2.18</b> Key Regression Assumptions for Building Effective Prediction Models</a><ul>
<li class="chapter" data-level="2.18.1" data-path="linear-regression-on-a-small-smart-data-set.html"><a href="linear-regression-on-a-small-smart-data-set.html#checking-assumptions-in-model-c2_m7"><i class="fa fa-check"></i><b>2.18.1</b> Checking Assumptions in model <code>c2_m7</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html"><i class="fa fa-check"></i><b>3</b> Analysis of Variance</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-bonding-data-a-designed-dental-experiment"><i class="fa fa-check"></i><b>3.1</b> The <code>bonding</code> data: A Designed Dental Experiment</a></li>
<li class="chapter" data-level="3.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-one-factor-analysis-of-variance"><i class="fa fa-check"></i><b>3.2</b> A One-Factor Analysis of Variance</a><ul>
<li class="chapter" data-level="3.2.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#look-at-the-data"><i class="fa fa-check"></i><b>3.2.1</b> Look at the Data!</a></li>
<li class="chapter" data-level="3.2.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#table-of-summary-statistics"><i class="fa fa-check"></i><b>3.2.2</b> Table of Summary Statistics</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-two-way-anova-looking-at-two-factors"><i class="fa fa-check"></i><b>3.3</b> A Two-Way ANOVA: Looking at Two Factors</a></li>
<li class="chapter" data-level="3.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-means-plot-with-standard-deviations-to-check-for-interaction"><i class="fa fa-check"></i><b>3.4</b> A Means Plot (with standard deviations) to check for interaction</a><ul>
<li class="chapter" data-level="3.4.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#skimming-the-data-after-grouping-by-resin-and-light"><i class="fa fa-check"></i><b>3.4.1</b> Skimming the data after grouping by <code>resin</code> and <code>light</code></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#fitting-the-two-way-anova-model-with-interaction"><i class="fa fa-check"></i><b>3.5</b> Fitting the Two-Way ANOVA model with Interaction</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-anova-table-for-our-model"><i class="fa fa-check"></i><b>3.5.1</b> The ANOVA table for our model</a></li>
<li class="chapter" data-level="3.5.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#is-the-interaction-important"><i class="fa fa-check"></i><b>3.5.2</b> Is the interaction important?</a></li>
<li class="chapter" data-level="3.5.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#interpreting-the-interaction"><i class="fa fa-check"></i><b>3.5.3</b> Interpreting the Interaction</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#comparing-individual-combinations-of-resin-and-light"><i class="fa fa-check"></i><b>3.6</b> Comparing Individual Combinations of <code>resin</code> and <code>light</code></a></li>
<li class="chapter" data-level="3.7" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-bonding-model-without-interaction"><i class="fa fa-check"></i><b>3.7</b> The <code>bonding</code> model without Interaction</a></li>
<li class="chapter" data-level="3.8" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#cortisol-a-hypothetical-clinical-trial"><i class="fa fa-check"></i><b>3.8</b> <code>cortisol</code>: A Hypothetical Clinical Trial</a><ul>
<li class="chapter" data-level="3.8.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#codebook-and-raw-data-for-cortisol"><i class="fa fa-check"></i><b>3.8.1</b> Codebook and Raw Data for <code>cortisol</code></a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#creating-a-factor-combining-sex-and-waist"><i class="fa fa-check"></i><b>3.9</b> Creating a factor combining sex and waist</a></li>
<li class="chapter" data-level="3.10" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-means-plot-for-the-cortisol-trial-with-standard-errors"><i class="fa fa-check"></i><b>3.10</b> A Means Plot for the <code>cortisol</code> trial (with standard errors)</a></li>
<li class="chapter" data-level="3.11" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-two-way-anova-model-for-cortisol-with-interaction"><i class="fa fa-check"></i><b>3.11</b> A Two-Way ANOVA model for <code>cortisol</code> with Interaction</a></li>
<li class="chapter" data-level="3.12" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#a-two-way-anova-model-for-cortisol-without-interaction"><i class="fa fa-check"></i><b>3.12</b> A Two-Way ANOVA model for <code>cortisol</code> without Interaction</a><ul>
<li class="chapter" data-level="3.12.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-graph"><i class="fa fa-check"></i><b>3.12.1</b> The Graph</a></li>
<li class="chapter" data-level="3.12.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-anova-model"><i class="fa fa-check"></i><b>3.12.2</b> The ANOVA Model</a></li>
<li class="chapter" data-level="3.12.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#the-regression-summary"><i class="fa fa-check"></i><b>3.12.3</b> The Regression Summary</a></li>
<li class="chapter" data-level="3.12.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#tukey-hsd-comparisons"><i class="fa fa-check"></i><b>3.12.4</b> Tukey HSD Comparisons</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html"><i class="fa fa-check"></i><b>4</b> Analysis of Covariance</a><ul>
<li class="chapter" data-level="4.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#an-emphysema-study"><i class="fa fa-check"></i><b>4.1</b> An Emphysema Study</a><ul>
<li class="chapter" data-level="4.1.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#codebook"><i class="fa fa-check"></i><b>4.1.1</b> Codebook</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#does-sex-affect-the-mean-change-in-theophylline"><i class="fa fa-check"></i><b>4.2</b> Does <code>sex</code> affect the mean change in theophylline?</a></li>
<li class="chapter" data-level="4.3" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#is-there-an-association-between-age-and-sex-in-this-study"><i class="fa fa-check"></i><b>4.3</b> Is there an association between <code>age</code> and <code>sex</code> in this study?</a></li>
<li class="chapter" data-level="4.4" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#adding-a-quantitative-covariate-age-to-the-model"><i class="fa fa-check"></i><b>4.4</b> Adding a quantitative covariate, <code>age</code>, to the model</a><ul>
<li class="chapter" data-level="4.4.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#the-ancova-model"><i class="fa fa-check"></i><b>4.4.1</b> The ANCOVA model</a></li>
<li class="chapter" data-level="4.4.2" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#the-ancova-table"><i class="fa fa-check"></i><b>4.4.2</b> The ANCOVA Table</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#rerunning-the-ancova-model-after-simple-imputation"><i class="fa fa-check"></i><b>4.5</b> Rerunning the ANCOVA model after simple imputation</a></li>
<li class="chapter" data-level="4.6" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#looking-at-a-factor-covariate-interaction"><i class="fa fa-check"></i><b>4.6</b> Looking at a factor-covariate interaction</a></li>
<li class="chapter" data-level="4.7" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#centering-the-covariate-to-facilitate-ancova-interpretation"><i class="fa fa-check"></i><b>4.7</b> Centering the Covariate to Facilitate ANCOVA Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html"><i class="fa fa-check"></i><b>5</b> Missing Data Mechanisms and Single Imputation</a><ul>
<li class="chapter" data-level="5.1" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#a-toy-example"><i class="fa fa-check"></i><b>5.1</b> A Toy Example</a><ul>
<li class="chapter" data-level="5.1.1" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#how-many-missing-values-do-we-have-in-each-column"><i class="fa fa-check"></i><b>5.1.1</b> How many missing values do we have in each column?</a></li>
<li class="chapter" data-level="5.1.2" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#what-is-the-pattern-of-missing-data"><i class="fa fa-check"></i><b>5.1.2</b> What is the pattern of missing data?</a></li>
<li class="chapter" data-level="5.1.3" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#how-can-we-identify-the-subjects-with-missing-data"><i class="fa fa-check"></i><b>5.1.3</b> How can we identify the subjects with missing data?</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#missing-data-mechanisms"><i class="fa fa-check"></i><b>5.2</b> Missing-data mechanisms</a></li>
<li class="chapter" data-level="5.3" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#options-for-dealing-with-missingness"><i class="fa fa-check"></i><b>5.3</b> Options for Dealing with Missingness</a></li>
<li class="chapter" data-level="5.4" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#complete-case-and-available-case-analyses"><i class="fa fa-check"></i><b>5.4</b> Complete Case (and Available Case) analyses</a></li>
<li class="chapter" data-level="5.5" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#single-imputation"><i class="fa fa-check"></i><b>5.5</b> Single Imputation</a></li>
<li class="chapter" data-level="5.6" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#multiple-imputation"><i class="fa fa-check"></i><b>5.6</b> Multiple Imputation</a></li>
<li class="chapter" data-level="5.7" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#building-a-complete-case-analysis"><i class="fa fa-check"></i><b>5.7</b> Building a Complete Case Analysis</a></li>
<li class="chapter" data-level="5.8" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#single-imputation-with-the-mean-or-mode"><i class="fa fa-check"></i><b>5.8</b> Single Imputation with the Mean or Mode</a></li>
<li class="chapter" data-level="5.9" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#doing-single-imputation-with-simputation"><i class="fa fa-check"></i><b>5.9</b> Doing Single Imputation with <code>simputation</code></a><ul>
<li class="chapter" data-level="5.9.1" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#mirroring-our-prior-approach-imputing-meansmediansmodes"><i class="fa fa-check"></i><b>5.9.1</b> Mirroring Our Prior Approach (imputing means/medians/modes)</a></li>
<li class="chapter" data-level="5.9.2" data-path="missing-data-mechanisms-and-single-imputation.html"><a href="missing-data-mechanisms-and-single-imputation.html#using-a-model-to-impute-sbp.before-and-diabetes"><i class="fa fa-check"></i><b>5.9.2</b> Using a model to impute <code>sbp.before</code> and <code>diabetes</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html"><i class="fa fa-check"></i><b>6</b> A Study of Prostate Cancer</a><ul>
<li class="chapter" data-level="6.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#data-load-and-background"><i class="fa fa-check"></i><b>6.1</b> Data Load and Background</a></li>
<li class="chapter" data-level="6.2" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#code-book"><i class="fa fa-check"></i><b>6.2</b> Code Book</a></li>
<li class="chapter" data-level="6.3" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#additions-for-later-use"><i class="fa fa-check"></i><b>6.3</b> Additions for Later Use</a></li>
<li class="chapter" data-level="6.4" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#fitting-and-evaluating-a-two-predictor-model"><i class="fa fa-check"></i><b>6.4</b> Fitting and Evaluating a Two-Predictor Model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#using-tidy"><i class="fa fa-check"></i><b>6.4.1</b> Using <code>tidy</code></a></li>
<li class="chapter" data-level="6.4.2" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#interpretation"><i class="fa fa-check"></i><b>6.4.2</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#exploring-model-c5_prost_a"><i class="fa fa-check"></i><b>6.5</b> Exploring Model <code>c5_prost_A</code></a><ul>
<li class="chapter" data-level="6.5.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#summary-for-model-c5_prost_a"><i class="fa fa-check"></i><b>6.5.1</b> <code>summary</code> for Model <code>c5_prost_A</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#adjusted-r2"><i class="fa fa-check"></i><b>6.5.2</b> Adjusted R<sup>2</sup></a></li>
<li class="chapter" data-level="6.5.3" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#coefficient-confidence-intervals"><i class="fa fa-check"></i><b>6.5.3</b> Coefficient Confidence Intervals</a></li>
<li class="chapter" data-level="6.5.4" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#anova-for-model-c5_prost_a"><i class="fa fa-check"></i><b>6.5.4</b> ANOVA for Model <code>c5_prost_A</code></a></li>
<li class="chapter" data-level="6.5.5" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#residuals-fitted-values-and-standard-errors-with-augment"><i class="fa fa-check"></i><b>6.5.5</b> Residuals, Fitted Values and Standard Errors with <code>augment</code></a></li>
<li class="chapter" data-level="6.5.6" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#making-predictions-with-c5_prost_a"><i class="fa fa-check"></i><b>6.5.6</b> Making Predictions with <code>c5_prost_A</code></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#plotting-model-c5_prost_a"><i class="fa fa-check"></i><b>6.6</b> Plotting Model <code>c5_prost_A</code></a><ul>
<li class="chapter" data-level="6.6.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#residual-plots-of-c5_prost_a"><i class="fa fa-check"></i><b>6.6.1</b> Residual Plots of <code>c5_prost_A</code></a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#cross-validation-of-model-c5_prost_a"><i class="fa fa-check"></i><b>6.7</b> Cross-Validation of Model <code>c5_prost_A</code></a><ul>
<li class="chapter" data-level="6.7.1" data-path="a-study-of-prostate-cancer.html"><a href="a-study-of-prostate-cancer.html#cross-validated-summaries-of-prediction-quality"><i class="fa fa-check"></i><b>6.7.1</b> Cross-Validated Summaries of Prediction Quality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html"><i class="fa fa-check"></i><b>7</b> Stepwise Variable Selection</a><ul>
<li class="chapter" data-level="7.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#strategy-for-model-selection"><i class="fa fa-check"></i><b>7.1</b> Strategy for Model Selection</a><ul>
<li class="chapter" data-level="7.1.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#how-do-we-choose-potential-subsets-of-predictors"><i class="fa fa-check"></i><b>7.1.1</b> How Do We Choose Potential Subsets of Predictors?</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#a-kitchen-sink-model-model-c5_prost_ks"><i class="fa fa-check"></i><b>7.2</b> A “Kitchen Sink” Model (Model <code>c5_prost_ks</code>)</a></li>
<li class="chapter" data-level="7.3" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#sequential-variable-selection-stepwise-approaches"><i class="fa fa-check"></i><b>7.3</b> Sequential Variable Selection: Stepwise Approaches</a><ul>
<li class="chapter" data-level="7.3.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#the-big-problems-with-stepwise-regression"><i class="fa fa-check"></i><b>7.3.1</b> The Big Problems with Stepwise Regression</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#forward-selection-with-the-step-function"><i class="fa fa-check"></i><b>7.4</b> Forward Selection with the <code>step</code> function</a></li>
<li class="chapter" data-level="7.5" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#backward-elimination-using-the-step-function"><i class="fa fa-check"></i><b>7.5</b> Backward Elimination using the <code>step</code> function</a></li>
<li class="chapter" data-level="7.6" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#allen-cady-modified-backward-elimination"><i class="fa fa-check"></i><b>7.6</b> Allen-Cady Modified Backward Elimination</a><ul>
<li class="chapter" data-level="7.6.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#demonstration-of-the-allen-cady-approach"><i class="fa fa-check"></i><b>7.6.1</b> Demonstration of the Allen-Cady approach</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#summarizing-the-results"><i class="fa fa-check"></i><b>7.7</b> Summarizing the Results</a><ul>
<li class="chapter" data-level="7.7.1" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#in-sample-testing-and-summaries"><i class="fa fa-check"></i><b>7.7.1</b> In-Sample Testing and Summaries</a></li>
<li class="chapter" data-level="7.7.2" data-path="stepwise-variable-selection.html"><a href="stepwise-variable-selection.html#validating-the-results-of-the-various-models"><i class="fa fa-check"></i><b>7.7.2</b> Validating the Results of the Various Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><i class="fa fa-check"></i><b>8</b> “Best Subsets” Variable Selection in our Prostate Cancer Study</a><ul>
<li class="chapter" data-level="8.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#four-key-summaries-well-use-to-evaluate-potential-models"><i class="fa fa-check"></i><b>8.1</b> Four Key Summaries We’ll Use to Evaluate Potential Models</a></li>
<li class="chapter" data-level="8.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#using-regsubsets-in-the-leaps-package"><i class="fa fa-check"></i><b>8.2</b> Using <code>regsubsets</code> in the <code>leaps</code> package</a><ul>
<li class="chapter" data-level="8.2.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#identifying-the-models-with-which-and-outmat"><i class="fa fa-check"></i><b>8.2.1</b> Identifying the models with <code>which</code> and <code>outmat</code></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#calculating-bias-corrected-aic"><i class="fa fa-check"></i><b>8.3</b> Calculating bias-corrected AIC</a><ul>
<li class="chapter" data-level="8.3.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#calculation-of-aic.c-in-our-setting"><i class="fa fa-check"></i><b>8.3.1</b> Calculation of aic.c in our setting</a></li>
<li class="chapter" data-level="8.3.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-uncorrected-aic-provides-no-more-useful-information-here"><i class="fa fa-check"></i><b>8.3.2</b> The Uncorrected AIC provides no more useful information here</a></li>
<li class="chapter" data-level="8.3.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#building-a-tibble-containing-the-necessary-information"><i class="fa fa-check"></i><b>8.3.3</b> Building a Tibble containing the necessary information</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#plotting-the-best-subsets-results-using-ggplot2"><i class="fa fa-check"></i><b>8.4</b> Plotting the Best Subsets Results using <code>ggplot2</code></a><ul>
<li class="chapter" data-level="8.4.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-adjusted-r2-plot"><i class="fa fa-check"></i><b>8.4.1</b> The Adjusted R<sup>2</sup> Plot</a></li>
<li class="chapter" data-level="8.4.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#mallows-c_p"><i class="fa fa-check"></i><b>8.4.2</b> Mallows’ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="8.4.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-c_p-plot"><i class="fa fa-check"></i><b>8.4.3</b> The <span class="math inline">\(C_p\)</span> Plot</a></li>
<li class="chapter" data-level="8.4.4" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#all-subsets-regression-and-information-criteria"><i class="fa fa-check"></i><b>8.4.4</b> “All Subsets” Regression and Information Criteria</a></li>
<li class="chapter" data-level="8.4.5" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-bias-corrected-aic-plot"><i class="fa fa-check"></i><b>8.4.5</b> The bias-corrected AIC plot</a></li>
<li class="chapter" data-level="8.4.6" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#the-bic-plot"><i class="fa fa-check"></i><b>8.4.6</b> The BIC plot</a></li>
<li class="chapter" data-level="8.4.7" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#all-four-plots-in-one-figure-via-ggplot2"><i class="fa fa-check"></i><b>8.4.7</b> All Four Plots in One Figure (via ggplot2)</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#table-of-key-results"><i class="fa fa-check"></i><b>8.5</b> Table of Key Results</a></li>
<li class="chapter" data-level="8.6" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#models-worth-considering"><i class="fa fa-check"></i><b>8.6</b> Models Worth Considering?</a></li>
<li class="chapter" data-level="8.7" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#compare-these-candidate-models-in-sample"><i class="fa fa-check"></i><b>8.7</b> Compare these candidate models in-sample?</a><ul>
<li class="chapter" data-level="8.7.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#using-anova-to-compare-nested-models"><i class="fa fa-check"></i><b>8.7.1</b> Using <code>anova</code> to compare nested models</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#aic-and-bic-comparisons-within-the-training-sample"><i class="fa fa-check"></i><b>8.8</b> AIC and BIC comparisons, within the training sample</a></li>
<li class="chapter" data-level="8.9" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#cross-validation-of-candidate-models-out-of-sample"><i class="fa fa-check"></i><b>8.9</b> Cross-Validation of Candidate Models out of Sample</a><ul>
<li class="chapter" data-level="8.9.1" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#fold-cross-validation-of-model-m04"><i class="fa fa-check"></i><b>8.9.1</b> 20-fold Cross-Validation of model <code>m04</code></a></li>
<li class="chapter" data-level="8.9.2" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#fold-cross-validation-of-model-m07"><i class="fa fa-check"></i><b>8.9.2</b> 20-fold Cross-Validation of model <code>m07</code></a></li>
<li class="chapter" data-level="8.9.3" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#fold-cross-validation-of-model-m08"><i class="fa fa-check"></i><b>8.9.3</b> 20-fold Cross-Validation of model <code>m08</code></a></li>
<li class="chapter" data-level="8.9.4" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#comparing-the-results-of-the-cross-validations"><i class="fa fa-check"></i><b>8.9.4</b> Comparing the Results of the Cross-Validations</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="best-subsets-variable-selection-in-our-prostate-cancer-study.html"><a href="best-subsets-variable-selection-in-our-prostate-cancer-study.html#what-about-interaction-terms"><i class="fa fa-check"></i><b>8.10</b> What about Interaction Terms?</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html"><i class="fa fa-check"></i><b>9</b> Adding Non-linear Terms to a Linear Regression Model</a><ul>
<li class="chapter" data-level="9.1" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#the-pollution-data"><i class="fa fa-check"></i><b>9.1</b> The <code>pollution</code> data</a></li>
<li class="chapter" data-level="9.2" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#fitting-a-straight-line-model-to-predict-y-from-x2"><i class="fa fa-check"></i><b>9.2</b> Fitting a straight line model to predict <code>y</code> from <code>x2</code></a></li>
<li class="chapter" data-level="9.3" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#quadratic-polynomial-model-to-predict-y-using-x2"><i class="fa fa-check"></i><b>9.3</b> Quadratic polynomial model to predict <code>y</code> using <code>x2</code></a><ul>
<li class="chapter" data-level="9.3.1" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#the-raw-quadratic-model"><i class="fa fa-check"></i><b>9.3.1</b> The raw quadratic model</a></li>
<li class="chapter" data-level="9.3.2" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#raw-quadratic-fit-after-centering-x2"><i class="fa fa-check"></i><b>9.3.2</b> Raw quadratic fit after centering <code>x2</code></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#orthogonal-polynomials"><i class="fa fa-check"></i><b>9.4</b> Orthogonal Polynomials</a></li>
<li class="chapter" data-level="9.5" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#fit-a-cubic-polynomial-to-predict-y-from-x3"><i class="fa fa-check"></i><b>9.5</b> Fit a cubic polynomial to predict <code>y</code> from <code>x3</code></a></li>
<li class="chapter" data-level="9.6" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#fitting-a-restricted-cubic-spline-in-a-linear-regression"><i class="fa fa-check"></i><b>9.6</b> Fitting a restricted cubic spline in a linear regression</a></li>
<li class="chapter" data-level="9.7" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#spending-degrees-of-freedom"><i class="fa fa-check"></i><b>9.7</b> “Spending” Degrees of Freedom</a><ul>
<li class="chapter" data-level="9.7.1" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#overfitting-and-limits-on-the-of-predictors"><i class="fa fa-check"></i><b>9.7.1</b> Overfitting and Limits on the # of Predictors</a></li>
<li class="chapter" data-level="9.7.2" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#the-importance-of-collinearity"><i class="fa fa-check"></i><b>9.7.2</b> The Importance of Collinearity</a></li>
<li class="chapter" data-level="9.7.3" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#collinearity-in-an-explanatory-model"><i class="fa fa-check"></i><b>9.7.3</b> Collinearity in an Explanatory Model</a></li>
<li class="chapter" data-level="9.7.4" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#collinearity-in-a-prediction-model"><i class="fa fa-check"></i><b>9.7.4</b> Collinearity in a Prediction Model</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#spending-df-on-non-linearity-the-spearman-rho2-plot"><i class="fa fa-check"></i><b>9.8</b> Spending DF on Non-Linearity: The Spearman <span class="math inline">\(\rho^2\)</span> Plot</a><ul>
<li class="chapter" data-level="9.8.1" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#fitting-a-big-model-to-the-pollution-data"><i class="fa fa-check"></i><b>9.8.1</b> Fitting a Big Model to the <code>pollution</code> data</a></li>
<li class="chapter" data-level="9.8.2" data-path="adding-non-linear-terms-to-a-linear-regression-model.html"><a href="adding-non-linear-terms-to-a-linear-regression-model.html#limitations-of-lm-for-fitting-complex-linear-regression-models"><i class="fa fa-check"></i><b>9.8.2</b> Limitations of <code>lm</code> for fitting complex linear regression models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html"><i class="fa fa-check"></i><b>10</b> Using <code>ols</code> from the <code>rms</code> package to fit linear models</a><ul>
<li class="chapter" data-level="10.1" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#fitting-a-model-with-ols"><i class="fa fa-check"></i><b>10.1</b> Fitting a model with <code>ols</code></a><ul>
<li class="chapter" data-level="10.1.1" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#the-model-likelihood-ratio-test"><i class="fa fa-check"></i><b>10.1.1</b> The Model Likelihood Ratio Test</a></li>
<li class="chapter" data-level="10.1.2" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#the-g-statistic"><i class="fa fa-check"></i><b>10.1.2</b> The g statistic</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#anova-for-an-ols-model"><i class="fa fa-check"></i><b>10.2</b> ANOVA for an <code>ols</code> model</a></li>
<li class="chapter" data-level="10.3" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#effect-estimates"><i class="fa fa-check"></i><b>10.3</b> Effect Estimates</a><ul>
<li class="chapter" data-level="10.3.1" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#simultaneous-confidence-intervals"><i class="fa fa-check"></i><b>10.3.1</b> Simultaneous Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#the-predict-function-for-an-ols-model"><i class="fa fa-check"></i><b>10.4</b> The <code>Predict</code> function for an <code>ols</code> model</a></li>
<li class="chapter" data-level="10.5" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#checking-influence-via-dfbeta"><i class="fa fa-check"></i><b>10.5</b> Checking Influence via <code>dfbeta</code></a><ul>
<li class="chapter" data-level="10.5.1" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#using-the-residuals-command-for-dfbetas"><i class="fa fa-check"></i><b>10.5.1</b> Using the <code>residuals</code> command for <code>dfbetas</code></a></li>
<li class="chapter" data-level="10.5.2" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#using-the-residuals-command-for-other-summaries"><i class="fa fa-check"></i><b>10.5.2</b> Using the <code>residuals</code> command for other summaries</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#model-validation-and-correcting-for-optimism"><i class="fa fa-check"></i><b>10.6</b> Model Validation and Correcting for Optimism</a></li>
<li class="chapter" data-level="10.7" data-path="using-ols-from-the-rms-package-to-fit-linear-models.html"><a href="using-ols-from-the-rms-package-to-fit-linear-models.html#building-a-nomogram-for-our-model"><i class="fa fa-check"></i><b>10.7</b> Building a Nomogram for Our Model</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html"><i class="fa fa-check"></i><b>11</b> Other Variable Selection Strategies</a><ul>
<li class="chapter" data-level="11.1" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#why-not-use-stepwise-procedures"><i class="fa fa-check"></i><b>11.1</b> Why not use stepwise procedures?</a></li>
<li class="chapter" data-level="11.2" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#ridge-regression"><i class="fa fa-check"></i><b>11.2</b> Ridge Regression</a><ul>
<li class="chapter" data-level="11.2.1" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#assessing-a-ridge-regression-approach"><i class="fa fa-check"></i><b>11.2.1</b> Assessing a Ridge Regression Approach</a></li>
<li class="chapter" data-level="11.2.2" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#the-lm.ridge-plot---where-do-coefficients-stabilize"><i class="fa fa-check"></i><b>11.2.2</b> The <code>lm.ridge</code> plot - where do coefficients stabilize?</a></li>
<li class="chapter" data-level="11.2.3" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#ridge-regression-the-bottom-line"><i class="fa fa-check"></i><b>11.2.3</b> Ridge Regression: The Bottom Line</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#the-lasso"><i class="fa fa-check"></i><b>11.3</b> The Lasso</a><ul>
<li class="chapter" data-level="11.3.1" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#consequences-of-the-lasso-approach"><i class="fa fa-check"></i><b>11.3.1</b> Consequences of the Lasso Approach</a></li>
<li class="chapter" data-level="11.3.2" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#how-the-lasso-works"><i class="fa fa-check"></i><b>11.3.2</b> How The Lasso Works</a></li>
<li class="chapter" data-level="11.3.3" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#cross-validation-with-the-lasso"><i class="fa fa-check"></i><b>11.3.3</b> Cross-Validation with the Lasso</a></li>
<li class="chapter" data-level="11.3.4" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#what-value-of-the-key-fraction-minimizes-cross-validated-mse"><i class="fa fa-check"></i><b>11.3.4</b> What value of the key fraction minimizes cross-validated MSE?</a></li>
<li class="chapter" data-level="11.3.5" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#coefficients-for-the-model-identified-by-the-cross-validation"><i class="fa fa-check"></i><b>11.3.5</b> Coefficients for the Model Identified by the Cross-Validation</a></li>
<li class="chapter" data-level="11.3.6" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#obtaining-fitted-values-from-lasso"><i class="fa fa-check"></i><b>11.3.6</b> Obtaining Fitted Values from Lasso</a></li>
<li class="chapter" data-level="11.3.7" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#complete-set-of-fitted-values-from-the-lasso"><i class="fa fa-check"></i><b>11.3.7</b> Complete Set of Fitted Values from the Lasso</a></li>
<li class="chapter" data-level="11.3.8" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#when-is-the-lasso-most-useful"><i class="fa fa-check"></i><b>11.3.8</b> When is the Lasso Most Useful?</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="other-variable-selection-strategies.html"><a href="other-variable-selection-strategies.html#applying-the-lasso-to-the-pollution-data"><i class="fa fa-check"></i><b>11.4</b> Applying the Lasso to the <code>pollution</code> data</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html"><i class="fa fa-check"></i><b>12</b> Logistic Regression: The Foundations</a><ul>
<li class="chapter" data-level="12.1" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#a-first-attempt-a-linear-probability-model"><i class="fa fa-check"></i><b>12.1</b> A First Attempt: A Linear Probability Model</a></li>
<li class="chapter" data-level="12.2" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#logistic-regression"><i class="fa fa-check"></i><b>12.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="12.3" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#the-logistic-regression-model"><i class="fa fa-check"></i><b>12.3</b> The Logistic Regression Model</a></li>
<li class="chapter" data-level="12.4" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#the-link-function"><i class="fa fa-check"></i><b>12.4</b> The Link Function</a></li>
<li class="chapter" data-level="12.5" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#the-logit-or-log-odds"><i class="fa fa-check"></i><b>12.5</b> The logit or log odds</a></li>
<li class="chapter" data-level="12.6" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#interpreting-the-coefficients-of-a-logistic-regression-model"><i class="fa fa-check"></i><b>12.6</b> Interpreting the Coefficients of a Logistic Regression Model</a></li>
<li class="chapter" data-level="12.7" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#the-logistic-regression-has-non-constant-variance"><i class="fa fa-check"></i><b>12.7</b> The Logistic Regression has non-constant variance</a></li>
<li class="chapter" data-level="12.8" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#fitting-a-logistic-regression-model-to-our-simulated-data"><i class="fa fa-check"></i><b>12.8</b> Fitting a Logistic Regression Model to our Simulated Data</a></li>
<li class="chapter" data-level="12.9" data-path="logistic-regression-the-foundations.html"><a href="logistic-regression-the-foundations.html#plotting-the-logistic-regression-model"><i class="fa fa-check"></i><b>12.9</b> Plotting the Logistic Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html"><i class="fa fa-check"></i><b>13</b> Logistic Regression and the <code>resect</code> data</a><ul>
<li class="chapter" data-level="13.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#the-resect-data"><i class="fa fa-check"></i><b>13.1</b> The <code>resect</code> data</a></li>
<li class="chapter" data-level="13.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#running-a-simple-logistic-regression-model"><i class="fa fa-check"></i><b>13.2</b> Running A Simple Logistic Regression Model</a><ul>
<li class="chapter" data-level="13.2.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#logistic-regression-can-be-harder-than-linear-regression"><i class="fa fa-check"></i><b>13.2.1</b> Logistic Regression Can Be Harder than Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#logistic-regression-using-glm"><i class="fa fa-check"></i><b>13.3</b> Logistic Regression using <code>glm</code></a><ul>
<li class="chapter" data-level="13.3.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-coefficients-of-a-logistic-regression-model-1"><i class="fa fa-check"></i><b>13.3.1</b> Interpreting the Coefficients of a Logistic Regression Model</a></li>
<li class="chapter" data-level="13.3.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#using-predict-to-describe-the-models-fits"><i class="fa fa-check"></i><b>13.3.2</b> Using <code>predict</code> to describe the model’s fits</a></li>
<li class="chapter" data-level="13.3.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#odds-ratio-interpretation-of-coefficients"><i class="fa fa-check"></i><b>13.3.3</b> Odds Ratio interpretation of Coefficients</a></li>
<li class="chapter" data-level="13.3.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-rest-of-the-model-output-from-glm"><i class="fa fa-check"></i><b>13.3.4</b> Interpreting the rest of the model output from <code>glm</code></a></li>
<li class="chapter" data-level="13.3.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#deviance-and-comparing-our-model-to-the-null-model"><i class="fa fa-check"></i><b>13.3.5</b> Deviance and Comparing Our Model to the Null Model</a></li>
<li class="chapter" data-level="13.3.6" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#using-glance-with-a-logistic-regression-model"><i class="fa fa-check"></i><b>13.3.6</b> Using <code>glance</code> with a logistic regression model</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-model-summary"><i class="fa fa-check"></i><b>13.4</b> Interpreting the Model Summary</a><ul>
<li class="chapter" data-level="13.4.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#wald-z-tests-for-coefficients-in-a-logistic-regression"><i class="fa fa-check"></i><b>13.4.1</b> Wald Z tests for Coefficients in a Logistic Regression</a></li>
<li class="chapter" data-level="13.4.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#confidence-intervals-for-the-coefficients"><i class="fa fa-check"></i><b>13.4.2</b> Confidence Intervals for the Coefficients</a></li>
<li class="chapter" data-level="13.4.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#deviance-residuals"><i class="fa fa-check"></i><b>13.4.3</b> Deviance Residuals</a></li>
<li class="chapter" data-level="13.4.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#dispersion-parameter"><i class="fa fa-check"></i><b>13.4.4</b> Dispersion Parameter</a></li>
<li class="chapter" data-level="13.4.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#fisher-scoring-iterations"><i class="fa fa-check"></i><b>13.4.5</b> Fisher Scoring iterations</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-a-simple-logistic-regression-model"><i class="fa fa-check"></i><b>13.5</b> Plotting a Simple Logistic Regression Model</a><ul>
<li class="chapter" data-level="13.5.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#using-augment-to-capture-the-fitted-probabilities"><i class="fa fa-check"></i><b>13.5.1</b> Using <code>augment</code> to capture the fitted probabilities</a></li>
<li class="chapter" data-level="13.5.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-a-logistic-regression-models-fitted-values"><i class="fa fa-check"></i><b>13.5.2</b> Plotting a Logistic Regression Model’s Fitted Values</a></li>
<li class="chapter" data-level="13.5.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-a-simple-logistic-model-using-binomial_smooth"><i class="fa fa-check"></i><b>13.5.3</b> Plotting a Simple Logistic Model using <code>binomial_smooth</code></a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#how-well-does-model-a-classify-subjects"><i class="fa fa-check"></i><b>13.6</b> How well does Model A classify subjects?</a></li>
<li class="chapter" data-level="13.7" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#receiver-operating-characteristic-curve-analysis"><i class="fa fa-check"></i><b>13.7</b> Receiver Operating Characteristic Curve Analysis</a><ul>
<li class="chapter" data-level="13.7.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-area-under-the-roc-curve"><i class="fa fa-check"></i><b>13.7.1</b> Interpreting the Area under the ROC curve</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#the-roc-plot-for-res_moda"><i class="fa fa-check"></i><b>13.8</b> The ROC Plot for <code>res_modA</code></a><ul>
<li class="chapter" data-level="13.8.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#another-way-to-plot-the-roc-curve"><i class="fa fa-check"></i><b>13.8.1</b> Another way to plot the ROC Curve</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#assessing-residual-plots-from-model-a"><i class="fa fa-check"></i><b>13.9</b> Assessing Residual Plots from Model A</a></li>
<li class="chapter" data-level="13.10" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#model-b-a-kitchen-sink-logistic-regression-model"><i class="fa fa-check"></i><b>13.10</b> Model B: A “Kitchen Sink” Logistic Regression Model</a><ul>
<li class="chapter" data-level="13.10.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#comparing-model-a-to-model-b"><i class="fa fa-check"></i><b>13.10.1</b> Comparing Model A to Model B</a></li>
<li class="chapter" data-level="13.10.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-model-b"><i class="fa fa-check"></i><b>13.10.2</b> Interpreting Model B</a></li>
</ul></li>
<li class="chapter" data-level="13.11" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-model-b"><i class="fa fa-check"></i><b>13.11</b> Plotting Model B</a><ul>
<li class="chapter" data-level="13.11.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#using-augment-to-capture-the-fitted-probabilities-1"><i class="fa fa-check"></i><b>13.11.1</b> Using <code>augment</code> to capture the fitted probabilities</a></li>
<li class="chapter" data-level="13.11.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-model-b-fits-by-observed-mortality"><i class="fa fa-check"></i><b>13.11.2</b> Plotting Model B Fits by Observed Mortality</a></li>
<li class="chapter" data-level="13.11.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#the-roc-curve-for-model-b"><i class="fa fa-check"></i><b>13.11.3</b> The ROC curve for Model B</a></li>
<li class="chapter" data-level="13.11.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#residuals-leverage-and-influence"><i class="fa fa-check"></i><b>13.11.4</b> Residuals, Leverage and Influence</a></li>
</ul></li>
<li class="chapter" data-level="13.12" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#logistic-regression-using-lrm"><i class="fa fa-check"></i><b>13.12</b> Logistic Regression using <code>lrm</code></a><ul>
<li class="chapter" data-level="13.12.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-nagelkerke-r2"><i class="fa fa-check"></i><b>13.12.1</b> Interpreting Nagelkerke R<sup>2</sup></a></li>
<li class="chapter" data-level="13.12.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#interpreting-the-c-statistic-and-plotting-the-roc-curve"><i class="fa fa-check"></i><b>13.12.2</b> Interpreting the C statistic and Plotting the ROC Curve</a></li>
<li class="chapter" data-level="13.12.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#the-c-statistic-and-somers-d"><i class="fa fa-check"></i><b>13.12.3</b> The C statistic and Somers’ D</a></li>
<li class="chapter" data-level="13.12.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#validating-the-logistic-regression-model-summary-statistics"><i class="fa fa-check"></i><b>13.12.4</b> Validating the Logistic Regression Model Summary Statistics</a></li>
<li class="chapter" data-level="13.12.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-the-summary-of-the-lrm-approach"><i class="fa fa-check"></i><b>13.12.5</b> Plotting the Summary of the <code>lrm</code> approach</a></li>
<li class="chapter" data-level="13.12.6" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plot-in-sample-predictions-for-model-c"><i class="fa fa-check"></i><b>13.12.6</b> Plot In-Sample Predictions for Model C</a></li>
<li class="chapter" data-level="13.12.7" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#anova-from-the-lrm-approach"><i class="fa fa-check"></i><b>13.12.7</b> ANOVA from the <code>lrm</code> approach</a></li>
<li class="chapter" data-level="13.12.8" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#are-any-points-particularly-influential"><i class="fa fa-check"></i><b>13.12.8</b> Are any points particularly influential?</a></li>
<li class="chapter" data-level="13.12.9" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#a-plot-of-the-models-calibration-curve"><i class="fa fa-check"></i><b>13.12.9</b> A plot of the model’s calibration curve</a></li>
<li class="chapter" data-level="13.12.10" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#a-nomogram-for-model-c"><i class="fa fa-check"></i><b>13.12.10</b> A Nomogram for Model C</a></li>
</ul></li>
<li class="chapter" data-level="13.13" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#model-d-an-augmented-kitchen-sink-model"><i class="fa fa-check"></i><b>13.13</b> Model D: An Augmented Kitchen Sink Model</a><ul>
<li class="chapter" data-level="13.13.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#spearman-rho2-plot"><i class="fa fa-check"></i><b>13.13.1</b> Spearman <span class="math inline">\(\rho^2\)</span> Plot</a></li>
<li class="chapter" data-level="13.13.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#fitting-model-d-using-lrm"><i class="fa fa-check"></i><b>13.13.2</b> Fitting Model D using <code>lrm</code></a></li>
<li class="chapter" data-level="13.13.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#assessing-model-d-using-lrms-tools"><i class="fa fa-check"></i><b>13.13.3</b> Assessing Model D using <code>lrm</code>’s tools</a></li>
<li class="chapter" data-level="13.13.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#anova-and-wald-tests-for-model-d"><i class="fa fa-check"></i><b>13.13.4</b> ANOVA and Wald Tests for Model D</a></li>
<li class="chapter" data-level="13.13.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#effect-sizes-in-model-d"><i class="fa fa-check"></i><b>13.13.5</b> Effect Sizes in Model D</a></li>
<li class="chapter" data-level="13.13.6" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plot-in-sample-predictions-for-model-d"><i class="fa fa-check"></i><b>13.13.6</b> Plot In-Sample Predictions for Model D</a></li>
<li class="chapter" data-level="13.13.7" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plotting-the-roc-curve-for-model-d"><i class="fa fa-check"></i><b>13.13.7</b> Plotting the ROC curve for Model D</a></li>
<li class="chapter" data-level="13.13.8" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#validation-of-model-d-summaries"><i class="fa fa-check"></i><b>13.13.8</b> Validation of Model D summaries</a></li>
<li class="chapter" data-level="13.13.9" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#calibration-plot-for-model-d"><i class="fa fa-check"></i><b>13.13.9</b> Calibration Plot for Model D</a></li>
</ul></li>
<li class="chapter" data-level="13.14" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#model-e-fitting-a-reduced-model-in-light-of-model-d"><i class="fa fa-check"></i><b>13.14</b> Model E: Fitting a Reduced Model in light of Model D</a><ul>
<li class="chapter" data-level="13.14.1" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#a-plot-comparing-the-two-intubation-groups"><i class="fa fa-check"></i><b>13.14.1</b> A Plot comparing the two intubation groups</a></li>
<li class="chapter" data-level="13.14.2" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#nomogram-for-model-e"><i class="fa fa-check"></i><b>13.14.2</b> Nomogram for Model E</a></li>
<li class="chapter" data-level="13.14.3" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#effect-sizes-from-model-e"><i class="fa fa-check"></i><b>13.14.3</b> Effect Sizes from Model E</a></li>
<li class="chapter" data-level="13.14.4" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#plot-in-sample-predictions-for-model-e"><i class="fa fa-check"></i><b>13.14.4</b> Plot In-Sample Predictions for Model E</a></li>
<li class="chapter" data-level="13.14.5" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#anova-for-model-e"><i class="fa fa-check"></i><b>13.14.5</b> ANOVA for Model E</a></li>
<li class="chapter" data-level="13.14.6" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#validation-of-model-e"><i class="fa fa-check"></i><b>13.14.6</b> Validation of Model E</a></li>
<li class="chapter" data-level="13.14.7" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#do-any-points-seem-particularly-influential"><i class="fa fa-check"></i><b>13.14.7</b> Do any points seem particularly influential?</a></li>
<li class="chapter" data-level="13.14.8" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#fitting-model-e-using-glm-to-get-plots-about-influence"><i class="fa fa-check"></i><b>13.14.8</b> Fitting Model E using <code>glm</code> to get plots about influence</a></li>
</ul></li>
<li class="chapter" data-level="13.15" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#concordance-comparing-model-c-d-and-es-predictions"><i class="fa fa-check"></i><b>13.15</b> Concordance: Comparing Model C, D and E’s predictions</a></li>
<li class="chapter" data-level="13.16" data-path="logistic-regression-and-the-resect-data.html"><a href="logistic-regression-and-the-resect-data.html#conclusions"><i class="fa fa-check"></i><b>13.16</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html"><i class="fa fa-check"></i><b>14</b> Logistic Regression and the <code>smartcle1</code> data</a><ul>
<li class="chapter" data-level="14.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#the-smartcle1-data"><i class="fa fa-check"></i><b>14.1</b> The <code>smartcle1</code> data</a></li>
<li class="chapter" data-level="14.2" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#thinking-about-non-linear-terms"><i class="fa fa-check"></i><b>14.2</b> Thinking About Non-Linear Terms</a></li>
<li class="chapter" data-level="14.3" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#a-first-model-for-exerany-complete-case-analysis"><i class="fa fa-check"></i><b>14.3</b> A First Model for <code>exerany</code> (Complete Case Analysis)</a></li>
<li class="chapter" data-level="14.4" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#building-a-larger-model-spearman-rho2-plot"><i class="fa fa-check"></i><b>14.4</b> Building a Larger Model: Spearman <span class="math inline">\(\rho^2\)</span> Plot</a></li>
<li class="chapter" data-level="14.5" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#a-second-model-for-exerany-complete-cases"><i class="fa fa-check"></i><b>14.5</b> A Second Model for <code>exerany</code> (Complete Cases)</a></li>
<li class="chapter" data-level="14.6" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#dealing-with-missing-data-via-simple-imputation"><i class="fa fa-check"></i><b>14.6</b> Dealing with Missing Data via Simple Imputation</a><ul>
<li class="chapter" data-level="14.6.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#omit-cases-where-the-outcome-is-missing"><i class="fa fa-check"></i><b>14.6.1</b> Omit cases where the outcome is missing</a></li>
<li class="chapter" data-level="14.6.2" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#plot-the-remaining-missingness"><i class="fa fa-check"></i><b>14.6.2</b> Plot the remaining missingness</a></li>
<li class="chapter" data-level="14.6.3" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#use-simple-imputation-build-a-new-data-set"><i class="fa fa-check"></i><b>14.6.3</b> Use simple imputation, build a new data set</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#refitting-model-1-with-simply-imputed-data"><i class="fa fa-check"></i><b>14.7</b> Refitting Model 1 with simply imputed data</a><ul>
<li class="chapter" data-level="14.7.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#validating-summary-statistics"><i class="fa fa-check"></i><b>14.7.1</b> Validating Summary Statistics</a></li>
<li class="chapter" data-level="14.7.2" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#anova-for-the-model"><i class="fa fa-check"></i><b>14.7.2</b> ANOVA for the model</a></li>
<li class="chapter" data-level="14.7.3" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#summarizing-effect-size"><i class="fa fa-check"></i><b>14.7.3</b> Summarizing Effect Size</a></li>
<li class="chapter" data-level="14.7.4" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#plotting-the-model-with-ggplot-and-predict"><i class="fa fa-check"></i><b>14.7.4</b> Plotting the Model with <code>ggplot</code> and <code>Predict</code></a></li>
<li class="chapter" data-level="14.7.5" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#plotting-the-model-with-a-nomogram"><i class="fa fa-check"></i><b>14.7.5</b> Plotting the model with a nomogram</a></li>
<li class="chapter" data-level="14.7.6" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#checking-the-calibration-of-our-model"><i class="fa fa-check"></i><b>14.7.6</b> Checking the Calibration of our model</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#refitting-model-2-with-simply-imputed-data"><i class="fa fa-check"></i><b>14.8</b> Refitting Model 2 with simply imputed data</a><ul>
<li class="chapter" data-level="14.8.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#validating-summary-statistics-1"><i class="fa fa-check"></i><b>14.8.1</b> Validating Summary Statistics</a></li>
<li class="chapter" data-level="14.8.2" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#anova-for-the-model-1"><i class="fa fa-check"></i><b>14.8.2</b> ANOVA for the model</a></li>
<li class="chapter" data-level="14.8.3" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#summarizing-effect-size-1"><i class="fa fa-check"></i><b>14.8.3</b> Summarizing Effect Size</a></li>
<li class="chapter" data-level="14.8.4" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#plotting-the-model-with-ggplot-and-predict-1"><i class="fa fa-check"></i><b>14.8.4</b> Plotting the Model with <code>ggplot</code> and <code>Predict</code></a></li>
<li class="chapter" data-level="14.8.5" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#plotting-the-model-with-a-nomogram-1"><i class="fa fa-check"></i><b>14.8.5</b> Plotting the model with a nomogram</a></li>
<li class="chapter" data-level="14.8.6" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#checking-the-calibration-of-our-model-1"><i class="fa fa-check"></i><b>14.8.6</b> Checking the Calibration of our model</a></li>
</ul></li>
<li class="chapter" data-level="14.9" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#comparing-model-2-to-model-1-after-simple-imputation"><i class="fa fa-check"></i><b>14.9</b> Comparing Model 2 to Model 1 after simple imputation</a><ul>
<li class="chapter" data-level="14.9.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#comparison-by-analysis-of-deviance"><i class="fa fa-check"></i><b>14.9.1</b> Comparison by Analysis of Deviance</a></li>
<li class="chapter" data-level="14.9.2" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#comparing-aic-and-bic"><i class="fa fa-check"></i><b>14.9.2</b> Comparing AIC and BIC</a></li>
</ul></li>
<li class="chapter" data-level="14.10" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#dealing-with-missing-data-via-multiple-imputation"><i class="fa fa-check"></i><b>14.10</b> Dealing with Missing Data via Multiple Imputation</a><ul>
<li class="chapter" data-level="14.10.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#using-aregimpute-to-fit-a-multiple-imputation-model"><i class="fa fa-check"></i><b>14.10.1</b> Using <code>aregImpute</code> to fit a multiple imputation model</a></li>
</ul></li>
<li class="chapter" data-level="14.11" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#combining-the-imputation-and-outcome-models"><i class="fa fa-check"></i><b>14.11</b> Combining the Imputation and Outcome Models</a><ul>
<li class="chapter" data-level="14.11.1" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#model-1-with-multiple-imputation"><i class="fa fa-check"></i><b>14.11.1</b> Model 1 with Multiple Imputation</a></li>
<li class="chapter" data-level="14.11.2" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#model-2-with-multiple-imputation"><i class="fa fa-check"></i><b>14.11.2</b> Model 2 with Multiple Imputation</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="logistic-regression-and-the-smartcle1-data.html"><a href="logistic-regression-and-the-smartcle1-data.html#models-with-and-without-imputation"><i class="fa fa-check"></i><b>14.12</b> Models with and without Imputation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science for Biological, Medical and Health Research: Notes for 432</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="other-variable-selection-strategies" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Other Variable Selection Strategies</h1>
<div id="why-not-use-stepwise-procedures" class="section level2">
<h2><span class="header-section-number">11.1</span> Why not use stepwise procedures?</h2>
<ol style="list-style-type: decimal">
<li>The R<sup>2</sup> for a model selected in a stepwise manner is biased, high.</li>
<li>The coefficient estimates and standard errors are biased.</li>
<li>The <span class="math inline">\(p\)</span> values for the individual-variable t tests are too small.</li>
<li>In stepwise analyses of prediction models, the final model represented noise 20-74% of the time.</li>
<li>In stepwise analyses, the final model usually contained less than half of the actual number of real predictors.</li>
<li>It is not logical that a population regression coefficient would be exactly zero just because its estimate was not statistically significant.</li>
</ol>
<p>This last comment applies to things like our “best subsets” approach as well as standard stepwise procedures.</p>
<p>Sander Greenland’s comments on parsimony and stepwise approaches to model selection are worth addressing…</p>
<ul>
<li>Stepwise variable selection on confounders leaves important confounders uncontrolled.</li>
<li>Shrinkage approaches (like ridge regression and the lasso) are far superior to variable selection.</li>
<li>Variable selection does more damage to confidence interval widths than to point estimates.</li>
</ul>
<p>If we are seriously concerned about <strong>overfitting</strong> - winding up with a model that doesn’t perform well on new data - then stepwise approaches generally don’t help.</p>
<p><span class="citation">Vittinghoff et al. (<a href="#ref-Vittinghoff2012">2012</a>)</span> suggest four strategies for minimizing the chance of overfitting</p>
<ol style="list-style-type: decimal">
<li>Pre-specify well-motivated predictors and how to model them.</li>
<li>Eliminate predictors without using the outcome.</li>
<li>Use the outcome, but cross-validate the target measure of prediction error.</li>
<li>Use the outcome, and <strong>shrink</strong> the coefficient estimates.</li>
</ol>
<p>The best subsets methods we have studied either include a variable or drop it from the model. Often, this choice is based on only a tiny difference in the quality of a fit to data.</p>
<ul>
<li><span class="citation">Harrell (<a href="#ref-Harrell2001">2001</a>)</span>: not reasonable to assume that a population regression coefficient would be exactly zero just because it failed to meet a criterion for significance.</li>
<li>Brad Efron has suggested that a stepwise approach is “overly greedy, impulsively eliminating covariates which are correlated with other covariates.”</li>
</ul>
<p>So, what’s the alternative?</p>
</div>
<div id="ridge-regression" class="section level2">
<h2><span class="header-section-number">11.2</span> Ridge Regression</h2>
<p><strong>Ridge regression</strong> involves a more smooth transition between useful and not useful predictors which can be obtained by constraining the overall size of the regression coefficients.</p>
<p>Ridge regression assumes that the regression coefficients (after normalization) should not be very large. This is reasonable to assume when you have lots of predictors and you believe <em>many</em> of them have some effect on the outcome.</p>
<p>Pros:</p>
<ol style="list-style-type: decimal">
<li>Some nice statistical properties</li>
<li>Can be calculated using only standard least squares approaches, so it’s been around for a while.</li>
<li>Available in the <code>MASS</code> package.</li>
</ol>
<p>Ridge regression takes the sum of the squared estimated standardized regression coefficients and constrains that sum to only be as large as some value <span class="math inline">\(k\)</span>.</p>
<p><span class="math display">\[
\sum \hat{\beta_j}^2 \leq k.
\]</span></p>
<p>The value <span class="math inline">\(k\)</span> is one of several available measures of the amount of shrinkage, but the main one used in the <code>MASS</code> package is a value <span class="math inline">\(\lambda\)</span>. As <span class="math inline">\(\lambda\)</span> increases, the amount of shrinkage goes up, and <span class="math inline">\(k\)</span> goes down.</p>
<div id="assessing-a-ridge-regression-approach" class="section level3">
<h3><span class="header-section-number">11.2.1</span> Assessing a Ridge Regression Approach</h3>
<p>We’ll look at a plot produced by the <code>lm.ridge</code> function for a ridge regression for the prostate cancer study we worked on when studying Stepwise Regression and Best Subsets methods earlier.</p>
<ul>
<li>Several (here 101) different values for <span class="math inline">\(\lambda\)</span>, our shrinkage parameter, will be tested.</li>
<li>Results are plotted so that we see the coefficients across the various (standardized) predictors.
<ul>
<li>Each selection of a <span class="math inline">\(\lambda\)</span> value implies a different vector of covariate values across the predictors we are studying.</li>
<li>The idea is to pick a value of <span class="math inline">\(\lambda\)</span> for which the coefficients seem relatively stable.</li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">preds &lt;-<span class="st"> </span><span class="kw">with</span>(prost, <span class="kw">cbind</span>(lcavol, lweight, age, bph_f,
                           svi_f, lcp, gleason_f, pgg45))

x &lt;-<span class="st"> </span><span class="kw">lm.ridge</span>(prost<span class="op">$</span>lpsa <span class="op">~</span><span class="st"> </span>preds, <span class="dt">lambda =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">100</span>)
    
<span class="kw">plot</span>(x)
<span class="kw">title</span>(<span class="st">&quot;Ridge Regression for prost data&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/ridge_prost_code-1.png" width="672" /></p>
<p>Usually, you need to use trial and error to decide the range of <span class="math inline">\(\lambda\)</span> to be tested. Here, <code>0:100</code> means going from 0 (no shrinkage) to 100 in steps of 1.</p>
</div>
<div id="the-lm.ridge-plot---where-do-coefficients-stabilize" class="section level3">
<h3><span class="header-section-number">11.2.2</span> The <code>lm.ridge</code> plot - where do coefficients stabilize?</h3>
<p>Does <span class="math inline">\(\lambda = 20\)</span> seem like a stable spot here?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">lm.ridge</span>(prost<span class="op">$</span>lpsa <span class="op">~</span><span class="st"> </span>preds, <span class="dt">lambda =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">100</span>)
<span class="kw">plot</span>(x)
<span class="kw">title</span>(<span class="st">&quot;Ridge Regression for prost data&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">20</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/ridge_prost_20-1.png" width="672" /></p>
<p>The coefficients at <span class="math inline">\(\lambda\)</span> = 20 can be determined from the <code>lm.ridge</code> output. These are fully standardized coefficients. The original predictors are centered by their means and then scaled by their standard deviations and the outcome has also been centered, in these models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(x<span class="op">$</span>coef[,<span class="dv">20</span>],<span class="dv">3</span>)</code></pre></div>
<pre><code>   predslcavol   predslweight       predsage     predsbph_f     predssvi_f 
         0.482          0.248         -0.091          0.097          0.252 
      predslcp predsgleason_f     predspgg45 
         0.009         -0.099          0.061 </code></pre>
<p>Was an intercept used?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x<span class="op">$</span>Inter</code></pre></div>
<pre><code>[1] 1</code></pre>
<p>Yes, it was. There is an automated way to pick <span class="math inline">\(\lambda\)</span>. Use the <code>select</code> function in the <code>MASS</code> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MASS<span class="op">::</span><span class="kw">select</span>(x)</code></pre></div>
<pre><code>modified HKB estimator is 4.210238 
modified L-W estimator is 3.32223 
smallest value of GCV  at 6 </code></pre>
<p>I’ll use the GCV = generalized cross-validation to select <span class="math inline">\(\lambda\)</span> = 6 instead.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">lm.ridge</span>(prost<span class="op">$</span>lpsa <span class="op">~</span><span class="st"> </span>preds, <span class="dt">lambda =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">100</span>)
<span class="kw">plot</span>(x)
<span class="kw">title</span>(<span class="st">&quot;Ridge Regression for prost data&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">6</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/ridge%20for%20ptsdmale%20with%2040%20line-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x<span class="op">$</span>coef[,<span class="dv">6</span>]</code></pre></div>
<pre><code>   predslcavol   predslweight       predsage     predsbph_f     predssvi_f 
    0.58911149     0.26773757    -0.13715070     0.11862949     0.29491008 
      predslcp predsgleason_f     predspgg45 
   -0.09389545    -0.10477578     0.07250609 </code></pre>
</div>
<div id="ridge-regression-the-bottom-line" class="section level3">
<h3><span class="header-section-number">11.2.3</span> Ridge Regression: The Bottom Line</h3>
<p>The main problem with ridge regression is that all it does is shrink the coefficient estimates, but it’s not so useful in practical settings because it still includes all variables.</p>
<ol style="list-style-type: decimal">
<li>It’s been easy to do ridge regression for many years, so you see it occasionally in the literature.</li>
<li>It leads to the <strong>lasso</strong>, which incorporates the positive features of shrinking regression coefficients with the ability to wisely select some variables to be eliminated from the predictor pool.</li>
</ol>
</div>
</div>
<div id="the-lasso" class="section level2">
<h2><span class="header-section-number">11.3</span> The Lasso</h2>
<p>The lasso works by takes the sum of the absolute values of the estimated standardized regression coefficients and constrains it to only be as large as some value k.</p>
<p><span class="math display">\[
\sum \hat{|\beta_j|} \leq k.
\]</span></p>
<p>This looks like a minor change, but it’s not.</p>
<div id="consequences-of-the-lasso-approach" class="section level3">
<h3><span class="header-section-number">11.3.1</span> Consequences of the Lasso Approach</h3>
<ol style="list-style-type: decimal">
<li>In ridge regression, while the individual coefficients shrink and sometimes approach zero, they seldom reach zero and are thus excluded from the model. With the lasso, some coefficients do reach zero and thus, those predictors do drop out of the model.
<ul>
<li>So the lasso leads to more parsimonious models than does ridge regression.</li>
<li>Ridge regression is a method of shrinkage but not model selection. The lasso accomplishes both tasks.</li>
</ul></li>
<li>If k is chosen to be too small, then the model may not capture important characteristics of the data. If k is too large, the model may over-fit the data in the sample and thus not represent the population of interest accurately.</li>
<li>The lasso is far more difficult computationally than ridge regression (the problem requires an algorithm called least angle regression published in 2004), although R has a library (<code>lars</code>) which can do the calculations pretty efficiently.</li>
</ol>
<p>The lasso is not an acronym, but rather refers to cowboys using a rope to pull cattle from the herd, much as we will pull predictors from a model.</p>
</div>
<div id="how-the-lasso-works" class="section level3">
<h3><span class="header-section-number">11.3.2</span> How The Lasso Works</h3>
<p>The <code>lars</code> package lets us compute the lasso coefficient estimates <strong>and</strong> do cross-validation to determine the appropriate amount of shrinkage. The main tool is a pair of graphs.</p>
<ol style="list-style-type: decimal">
<li>The first plot shows what coefficients get selected as we move from constraining all of the coefficients to zero (complete shrinkage) towards fewer constraints all the way up to ordinary least squares, showing which variables are included in the model at each point.</li>
<li>The second plot suggests where on the first plot we should look for a good model choice, according to a cross-validation approach.</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## requires lars package
lasso1 &lt;-<span class="st"> </span><span class="kw">lars</span>(preds, prost<span class="op">$</span>lpsa, <span class="dt">type=</span><span class="st">&quot;lasso&quot;</span>)
<span class="kw">plot</span>(lasso1)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/lasso_graph1_forprost-1.png" width="672" /></p>
<ul>
<li>The y axis shows standardized regression coefficients.
<ul>
<li>The <code>lars</code> package standardizes all variables so the shrinkage doesn’t penalize some coefficients because of their scale.</li>
</ul></li>
<li>The x-axis is labeled <code>|beta|/max|beta|</code>.
<ul>
<li>This ranges from 0 to 1.</li>
<li>0 means that the sum of the <span class="math inline">\(|\hat{\beta_j}|\)</span> is zero (completely shrunk)</li>
<li>1 means the ordinary least squares unbiased estimates.</li>
</ul></li>
</ul>
<p>The lasso graph starts at constraining all of the coefficients to zero, and then moves toward ordinary least squares.</p>
<p>Identifiers for the predictors (numbers) are shown to the right of the graph.</p>
<p>The vertical lines in the lasso plot show when a variable has been eliminated from the model, and in fact these are the only points that are actually shown in the default lasso graph. The labels on the top of the graph tell you how many predictors are in the model at that stage.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(lasso1)</code></pre></div>
<pre><code>LARS/LASSO
Call: lars(x = preds, y = prost$lpsa, type = &quot;lasso&quot;)
  Df     Rss       Cp
0  1 127.918 168.1835
1  2  76.392  64.1722
2  3  70.247  53.5293
3  4  50.598  15.1017
4  5  49.065  13.9485
5  6  48.550  14.8898
6  7  46.284  12.2276
7  8  44.002   9.5308
8  9  42.772   9.0000</code></pre>
<p>Based on the C<sub>p</sub> statistics, it looks like the improvements continue throughout, and don’t really finish happening until we get pretty close to the full model with 9 df.</p>
</div>
<div id="cross-validation-with-the-lasso" class="section level3">
<h3><span class="header-section-number">11.3.3</span> Cross-Validation with the Lasso</h3>
<p>Normally, cross-validation methods are used to determine how much shrinkage should be used. We’ll use the <code>cv.lars</code> function.</p>
<ul>
<li>10-fold (K = 10) cross-validation
<ul>
<li>the data are randomly divided into 10 groups.</li>
<li>Nine groups are used to predict the remaining group for each group in turn.</li>
<li>Overall prediction performance is computed, and the machine calculates a cross-validation criterion (mean squared error) and standard error for that criterion.</li>
</ul></li>
</ul>
<p>The cross-validation plot is the second lasso plot.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">432</span>)
lassocv &lt;-<span class="st"> </span><span class="kw">cv.lars</span>(preds, prost<span class="op">$</span>lpsa, <span class="dt">K=</span><span class="dv">10</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/lasso_graph2-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## default cv.lars K is 10</code></pre></div>
<p>We’re looking to minimize cross-validated mean squared error in this plot, which doesn’t seem to happen until the fraction gets very close to 1.</p>
</div>
<div id="what-value-of-the-key-fraction-minimizes-cross-validated-mse" class="section level3">
<h3><span class="header-section-number">11.3.4</span> What value of the key fraction minimizes cross-validated MSE?</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">frac &lt;-<span class="st"> </span>lassocv<span class="op">$</span>index[<span class="kw">which.min</span>(lassocv<span class="op">$</span>cv)]
frac</code></pre></div>
<pre><code>[1] 0.989899</code></pre>
<p>The cross-validation plot suggests we use a fraction of about 0.3, that’s suggesting a model with 4-5 predictors, based on the top LASSO plot.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))
lasso1 &lt;-<span class="st"> </span><span class="kw">lars</span>(preds, prost<span class="op">$</span>lpsa, <span class="dt">type=</span><span class="st">&quot;lasso&quot;</span>)
<span class="kw">plot</span>(lasso1)
<span class="kw">set.seed</span>(<span class="dv">432</span>)
lassocv &lt;-<span class="st"> </span><span class="kw">cv.lars</span>(preds, prost<span class="op">$</span>lpsa, <span class="dt">K=</span><span class="dv">10</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/lasso_bothplots-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
</div>
<div id="coefficients-for-the-model-identified-by-the-cross-validation" class="section level3">
<h3><span class="header-section-number">11.3.5</span> Coefficients for the Model Identified by the Cross-Validation</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coef.cv &lt;-<span class="st"> </span><span class="kw">coef</span>(lasso1, <span class="dt">s=</span>frac, <span class="dt">mode=</span><span class="st">&quot;fraction&quot;</span>)
<span class="kw">round</span>(coef.cv,<span class="dv">4</span>)</code></pre></div>
<pre><code>   lcavol   lweight       age     bph_f     svi_f       lcp gleason_f 
   0.5529    0.6402   -0.0217    0.1535    0.7750   -0.1155   -0.1826 
    pgg45 
   0.0030 </code></pre>
<p>So the model suggested by the lasso still includes all sight of these predictors.</p>
</div>
<div id="obtaining-fitted-values-from-lasso" class="section level3">
<h3><span class="header-section-number">11.3.6</span> Obtaining Fitted Values from Lasso</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fits.cv &lt;-<span class="st"> </span><span class="kw">predict.lars</span>(lasso1, preds, <span class="dt">s=</span>frac, 
                        <span class="dt">type=</span><span class="st">&quot;fit&quot;</span>, <span class="dt">mode=</span><span class="st">&quot;fraction&quot;</span>)
fits.cv</code></pre></div>
<pre><code>$s
[1] 0.989899

$fraction
[1] 0.989899

$mode
[1] &quot;fraction&quot;

$fit
 [1] 0.7995838 0.7493971 0.5111634 0.6098520 1.7001847 0.8338020 1.8288518
 [8] 2.1302316 1.2487955 1.2661752 1.4704969 0.7782005 2.0755860 1.9129272
[15] 2.1533975 1.8124981 1.2713610 2.3993624 1.3232566 1.7709029 1.9757841
[22] 2.7451649 1.1658326 2.4825521 1.8036338 1.9112578 2.0144298 1.7829219
[29] 1.9706111 2.1688199 2.0377131 1.8657882 1.6955904 1.3580186 1.0516394
[36] 2.9097450 2.1898622 1.0454123 3.8896481 1.7971270 2.0932871 2.3253395
[43] 2.0809295 2.5303655 2.4451523 2.5827203 4.0692397 2.6845105 2.7034959
[50] 1.9590266 2.4522082 2.9801227 2.1902084 3.0559124 3.3447025 2.9765233
[57] 1.7620182 2.3424646 2.2856404 2.6188548 2.3056410 3.5568662 2.9756755
[64] 3.6764122 2.5097586 2.6579014 2.9482717 3.0892917 1.5113015 3.0282296
[71] 3.2887119 2.1083273 2.8889223 3.4903026 3.6959516 3.6070031 3.2749993
[78] 3.4518575 3.4049180 3.1814731 2.0496216 2.8986175 3.6743113 3.3292860
[85] 2.6965297 3.8339856 2.9892543 3.0555536 4.2903885 3.0986508 3.3784385
[92] 4.0205201 3.8309974 4.7531590 3.6290575 4.1347645 4.0982744</code></pre>
</div>
<div id="complete-set-of-fitted-values-from-the-lasso" class="section level3">
<h3><span class="header-section-number">11.3.7</span> Complete Set of Fitted Values from the Lasso</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(fits.cv<span class="op">$</span>fit,<span class="dv">3</span>)</code></pre></div>
<pre><code> [1] 0.800 0.749 0.511 0.610 1.700 0.834 1.829 2.130 1.249 1.266 1.470
[12] 0.778 2.076 1.913 2.153 1.812 1.271 2.399 1.323 1.771 1.976 2.745
[23] 1.166 2.483 1.804 1.911 2.014 1.783 1.971 2.169 2.038 1.866 1.696
[34] 1.358 1.052 2.910 2.190 1.045 3.890 1.797 2.093 2.325 2.081 2.530
[45] 2.445 2.583 4.069 2.685 2.703 1.959 2.452 2.980 2.190 3.056 3.345
[56] 2.977 1.762 2.342 2.286 2.619 2.306 3.557 2.976 3.676 2.510 2.658
[67] 2.948 3.089 1.511 3.028 3.289 2.108 2.889 3.490 3.696 3.607 3.275
[78] 3.452 3.405 3.181 2.050 2.899 3.674 3.329 2.697 3.834 2.989 3.056
[89] 4.290 3.099 3.378 4.021 3.831 4.753 3.629 4.135 4.098</code></pre>
<p>To assess the quality of these predictions, we might plot them against the observed values of our outcome (<code>lpsa</code>), or we might look at residuals vs. these fitted values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prost_lasso_res &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">fitted =</span> fits.cv<span class="op">$</span>fit, 
                             <span class="dt">actual =</span> prost<span class="op">$</span>lpsa, 
                             <span class="dt">resid =</span> actual <span class="op">-</span><span class="st"> </span>fitted)

<span class="kw">ggplot</span>(prost_lasso_res, <span class="kw">aes</span>(<span class="dt">x =</span> actual, <span class="dt">y =</span> fitted)) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">intercept =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;Fitted log(PSA) from Cross-Validated LASSO&quot;</span>,
         <span class="dt">x =</span> <span class="st">&quot;Observed values of log(PSA)&quot;</span>,
         <span class="dt">title =</span> <span class="st">&quot;Fitted vs. Actual Values of log(PSA)&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-133-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(prost_lasso_res, <span class="kw">aes</span>(<span class="dt">x =</span> fitted, <span class="dt">y =</span> resid)) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">se =</span> F) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;LASSO-fitted log(PSA)&quot;</span>,
         <span class="dt">y =</span> <span class="st">&quot;Residuals from Cross-Validated LASSO model&quot;</span>,
         <span class="dt">title =</span> <span class="st">&quot;Residuals vs. Fitted Values of log(PSA) from LASSO&quot;</span>,
         <span class="dt">subtitle =</span> <span class="st">&quot;with loess smooth&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-134-1.png" width="672" /></p>
</div>
<div id="when-is-the-lasso-most-useful" class="section level3">
<h3><span class="header-section-number">11.3.8</span> When is the Lasso Most Useful?</h3>
<p>As <span class="citation">Faraway (<a href="#ref-Faraway2015">2015</a>)</span> suggests, the lasso is particularly useful when we believe the effects are sparse, in the sense that we believe that few of the many predictors we are evaluating have a meaningful effect.</p>
<p>Consider, for instance, the analysis of gene expression data, where we have good reason to believe that only a small number of genes have an influence on our response of interest.</p>
<p>Or, in medical claims data, where we can have thousands of available codes to search through that may apply to some of the people included in a large analysis relating health care costs to outcomes.</p>
</div>
</div>
<div id="applying-the-lasso-to-the-pollution-data" class="section level2">
<h2><span class="header-section-number">11.4</span> Applying the Lasso to the <code>pollution</code> data</h2>
<p>Let’s consider the lasso approach in application to the <code>pollution</code> data we’ve seen previously. Recall that we have 60 observations on an outcome, <code>y</code>, and 15 predictors, labeled x1 through x15.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">preds &lt;-<span class="st"> </span><span class="kw">with</span>(pollution, <span class="kw">cbind</span>(x1, x2, x3, x4, x5, x6, x7,
                               x8, x9, x10, x11, x12, x13,
                               x14, x15))

lasso_p1 &lt;-<span class="st"> </span><span class="kw">lars</span>(preds, pollution<span class="op">$</span>y, <span class="dt">type=</span><span class="st">&quot;lasso&quot;</span>)
<span class="kw">plot</span>(lasso_p1)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/lasso_graph1_forpollution-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(lasso_p1)</code></pre></div>
<pre><code>LARS/LASSO
Call: lars(x = preds, y = pollution$y, type = &quot;lasso&quot;)
   Df    Rss       Cp
0   1 228311 129.1367
1   2 185419  95.9802
2   3 149370  68.4323
3   4 143812  65.8764
4   5  92077  25.4713
5   6  83531  20.4668
6   7  69532  10.9922
7   8  67682  11.4760
8   9  60689   7.7445
9  10  60167   9.3163
10 11  59609  10.8588
11 12  58287  11.7757
12 13  57266  12.9383
13 14  56744  14.5107
14 13  56159  12.0311
15 14  55238  13.2765
16 15  53847  14.1361
17 16  53681  16.0000</code></pre>
<p>Based on the C<sub>p</sub> statistics, it looks like the big improvements occur somewhere around the move from 6 to 7 df. Let’s look at the cross-validation</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">432012</span>)
pollution_lassocv &lt;-<span class="st"> </span><span class="kw">cv.lars</span>(preds, pollution<span class="op">$</span>y, <span class="dt">K=</span><span class="dv">10</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-135-1.png" width="672" /></p>
<p>Here it looks like cross-validated MSE happens somewhere between a fraction of 0.2 and 0.4.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">frac &lt;-<span class="st"> </span>pollution_lassocv<span class="op">$</span>index[<span class="kw">which.min</span>(pollution_lassocv<span class="op">$</span>cv)]
frac</code></pre></div>
<pre><code>[1] 0.3535354</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))
lasso_p1 &lt;-<span class="st"> </span><span class="kw">lars</span>(preds, pollution<span class="op">$</span>y, <span class="dt">type=</span><span class="st">&quot;lasso&quot;</span>)
<span class="kw">plot</span>(lasso_p1)
<span class="kw">set.seed</span>(<span class="dv">432012</span>)
pollution_lassocv &lt;-<span class="st"> </span><span class="kw">cv.lars</span>(preds, pollution<span class="op">$</span>y, <span class="dt">K=</span><span class="dv">10</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-137-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>It looks like a model with 6-8 predictors will be the most useful. The cross-validated coefficients are as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">poll.cv &lt;-<span class="st"> </span><span class="kw">coef</span>(lasso_p1, <span class="dt">s=</span>frac, <span class="dt">mode=</span><span class="st">&quot;fraction&quot;</span>)
<span class="kw">round</span>(poll.cv,<span class="dv">3</span>)</code></pre></div>
<pre><code>     x1      x2      x3      x4      x5      x6      x7      x8      x9 
  1.471  -1.164  -1.102   0.000   0.000 -10.610  -0.457   0.003   3.918 
    x10     x11     x12     x13     x14     x15 
  0.000   0.000   0.000   0.000   0.228   0.000 </code></pre>
<p>Note that by this cross-validated lasso selection, not only are the coefficients for the 8 variables remaining in the model shrunken, but variables <code>x4</code>, <code>x5</code>, <code>x10</code>, <code>x11</code>, <code>x12</code>, <code>x13</code> and <code>x15</code> are all dropped from the model, and model <code>x8</code> almost is, as well.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">poll_fits &lt;-<span class="st"> </span><span class="kw">predict.lars</span>(lasso_p1, preds, <span class="dt">s=</span>frac, 
                        <span class="dt">type=</span><span class="st">&quot;fit&quot;</span>, <span class="dt">mode=</span><span class="st">&quot;fraction&quot;</span>)
<span class="kw">round</span>(poll_fits<span class="op">$</span>fit,<span class="dv">3</span>)</code></pre></div>
<pre><code> [1]  932.627  918.415  921.904  987.396 1050.184 1065.837  912.424
 [8]  916.605  949.647  926.168  996.625 1017.362  977.730  954.550
[15]  931.455  894.263  931.551  868.599  973.471  940.937  881.867
[22]  906.666  973.609  919.640  933.821  956.352  913.018  925.650
[29]  874.528  983.829 1042.870  915.002  937.760  885.464  989.947
[36]  931.709 1013.795  969.729 1003.962  983.813  896.042  918.446
[43]  934.609 1004.565  910.273  976.747  831.132  907.996  826.485
[50]  895.082  909.398  917.969  926.777  917.381  991.266  879.972
[57]  942.867  913.737  960.952  949.030</code></pre>
<p>Here’s a plot of the actual <code>pollution</code> y values, against these fitted values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">poll_lasso_res &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">fitted =</span> poll_fits<span class="op">$</span>fit, 
                             <span class="dt">actual =</span> pollution<span class="op">$</span>y, 
                             <span class="dt">resid =</span> actual <span class="op">-</span><span class="st"> </span>fitted)

<span class="kw">ggplot</span>(poll_lasso_res, <span class="kw">aes</span>(<span class="dt">x =</span> actual, <span class="dt">y =</span> fitted)) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">intercept =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;Fitted y values from Cross-Validated LASSO&quot;</span>,
         <span class="dt">x =</span> <span class="st">&quot;Observed values of y = Age-Adjusted Mortality Rate&quot;</span>,
         <span class="dt">title =</span> <span class="st">&quot;Fitted vs. Actual Values of Age-Adjusted Mortality&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-139-1.png" width="672" /></p>
<p>And now, here’s a plot or residuals vs. fitted values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(poll_lasso_res, <span class="kw">aes</span>(<span class="dt">x =</span> fitted, <span class="dt">y =</span> resid)) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">se =</span> F) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;LASSO-fitted Age-Adjusted Mortality&quot;</span>,
         <span class="dt">y =</span> <span class="st">&quot;Residuals from Cross-Validated LASSO model&quot;</span>,
         <span class="dt">title =</span> <span class="st">&quot;Residuals vs. Fitted Values of Age-Adjusted Mortality from LASSO&quot;</span>,
         <span class="dt">subtitle =</span> <span class="st">&quot;with loess smooth&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-140-1.png" width="672" /></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Vittinghoff2012">
<p>Vittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E. McCulloch. 2012. <em>Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models</em>. Second Edition. Springer-Verlag, Inc. <a href="http://www.biostat.ucsf.edu/vgsm/" class="uri">http://www.biostat.ucsf.edu/vgsm/</a>.</p>
</div>
<div id="ref-Harrell2001">
<p>Harrell, Frank E. 2001. <em>Regression Modeling Strategies</em>. New York: Springer.</p>
</div>
<div id="ref-Faraway2015">
<p>Faraway, Julian J. 2015. <em>Linear Models with R</em>. Second. Boca Raton, FL: CRC Press.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="using-ols-from-the-rms-package-to-fit-linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic-regression-the-foundations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/11_ridgeandlasso.Rmd",
"text": "Edit"
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
