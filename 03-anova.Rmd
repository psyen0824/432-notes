# Analysis of Variance and Analysis of Covariance

## The `bonding` data: A Designed Dental Experiment

The `bonding` data describe a designed experiment into the properties of four different resin types (`resin` = A, B, C, D) and two different curing light sources (`light` = Halogen, LED) as they relate to the resulting bonding strength (measured in MPa^[The MPa is defined as the failure load (in Newtons) divided by the entire bonded area, in mm^2^.]) on the surface of teeth. The source is @Kim2014.

The experiment involved making measurements of bonding strength under a total of 80 experimental setups, or runs, with 10 runs completed at each of the eight combinations of a light source and a resin type. The data are gathered in the `bonding.csv` file.

```{r c3_bonding_tibble}
bonding
```

## A One-Factor Analysis of Variance

Suppose we are interested in the distribution of the `strength` values for the four different types of `resin`. 

```{r}
bonding %>% group_by(resin) %>% summarize(n = n(), mean(strength), median(strength))
```

I'd begin serious work with a plot.

### Look at the Data!

```{r c3_oneway_bonding_resin_boxplot}
ggplot(bonding, aes(x = resin, y = strength)) +
    geom_boxplot()
```

Another good plot for this purpose is a ridgeline plot.

```{r c3_oneway_bonding_resin_ridgelineplot}
ggplot(bonding, aes(x = strength, y = resin, fill = resin)) +
    geom_density_ridges2() +
    guides(fill = FALSE)
```

### Table of Summary Statistics

With the small size of this experiment (*n* = 20 for each `resin` type), graphical summaries may not perform as well as they often do. We'll also produce a quick table of summary statistics for `strength` within each `resin` type, with the `skim()` function.

```{r}
bonding %>% group_by(resin) %>% skim(strength)
```

Since the means and medians are fairly close, and the distributions (with the possible exception of `resin` D) are reasonably well approximated by the Normal, I'll fit an ANOVA model.

```{r}
anova(lm(strength ~ resin, data = bonding))
```

It appears that the `resin` types have a significant association with mean `strength` of the bonds. Can we identify which `resin` types have generally higher or lower `strength`?

```{r}
TukeyHSD(aov(lm(strength ~ resin, data = bonding)))
```

Based on these confidence intervals (which have a family-wise 95% confidence level), we see that D is associated with significantly larger mean `strength` than A or B or C, and that C is also associated with significantly larger mean `strength` than A. This may be easier to see in a plot of these confidence intervals.

```{r}
plot(TukeyHSD(aov(lm(strength ~ resin, data = bonding))))
```

## A Two-Way ANOVA: Looking at Two Factors

Now, we'll now add consideration of the `light` source into our study. We can look at the distribution of the `strength` values at the combinations of both `light` and `resin`, with a plot like this one...

```{r c3_bonding_points_plot}
ggplot(bonding, aes(x = resin, y = strength, color = light)) +
    geom_point(size = 2, alpha = 0.5) +
    facet_wrap(~ light) +
    guides(color = FALSE) +
    scale_color_manual(values = c("purple", "darkorange")) +
    theme_bw() 
```

## A Means Plot (with standard deviations) to check for interaction

Sometimes, we'll instead look at a plot simply of the means (and, often, the standard deviations) of `strength` at each combination of `light` and `resin`. We'll start by building up a data set with the summaries we want to plot.

```{r c3_group_to_build_means_plot_bonding}
bond.sum <- bonding %>% 
    group_by(resin, light) %>%
    summarize(mean.str = mean(strength), sd.str = sd(strength))

bond.sum
```

Now, we'll use this new data set to plot the means and standard deviations of `strength` at each combination of `resin` and `light`. 

```{r c3_ggplot_means_plot_bonding}
## The error bars will overlap unless we adjust the position.
pd <- position_dodge(0.2) # move them .1 to the left and right

ggplot(bond.sum, aes(x = resin, y = mean.str, col = light)) +
    geom_errorbar(aes(ymin = mean.str - sd.str, 
                      ymax = mean.str + sd.str),
                  width = 0.2, position = pd) +
    geom_point(size = 2, position = pd) + 
    geom_line(aes(group = light), position = pd) +
    scale_color_manual(values = c("purple", "darkorange")) +
    theme_bw() +
    labs(y = "Bonding Strength (MPa)", x = "Resin Type",
         title = "Observed Means (+/- SD) of Bonding Strength")
```

Is there evidence of a meaningful interaction between the resin type and the `light` source on the bonding strength in this plot? 

- Sure. A meaningful interaction just means that the strength associated with different `resin` types depends on the `light` source. 
    - With LED `light`, it appears that `resin` C leads to the strongest bonding strength.
    - With Halogen `light`, though, it seems that `resin` D is substantially stronger.
- Note that the lines we see here connecting the `light` sources aren't in parallel (as they would be if we had zero interaction between `resin` and `light`), but rather, they cross.

### Skimming the data after grouping by `resin` and `light`

We might want to look at a numerical summary of the `strengths` within these groups, too.

```{r c3_bonding_anova_skim}
bonding %>%
    group_by(resin, light) %>%
    skim(strength) 
```

## Fitting the Two-Way ANOVA model with Interaction

```{r c3_bonding_anova_with_interaction}
c3_m1 <- lm(strength ~ resin * light, data = bonding)

summary(c3_m1)
```

### The ANOVA table for our model

In a two-way ANOVA model, we begin by assessing the interaction term. If it's important, then our best model is the model including the interaction. If it's not important, we will often move on to consider a new model, fit without an interaction.

The ANOVA table is especially helpful in this case, because it lets us look specifically at the interaction effect.

```{r}
anova(c3_m1)
```

### Is the interaction important?

In this case, the interaction:

- is evident in the means plot, and
- is highly statistically significant, and
- accounts for a sizeable fraction (27%) of the overall variation

$$ 
\eta^2_{interaction} = \frac{\mbox{SS(resin:light)}}{SS(Total)}
= \frac{1571.96}{1999.72 + 34.72 + 1571.96 + 2258.52} = 0.268
$$

If the interaction were *either* large or significant we would be inclined to keep it in the model. In this case, it's both, so there's no real reason to remove it.

### Interpreting the Interaction

Recall the model equation, which is:

```{r show_c3_m1_model}
c3_m1
```

so we have:

$$
strength = 17.77 + 2.13 resinB + 4.77 resinC + 22.53 resinD \\
+ 1.29 lightLED + 3.37 resinB*lightLED \\
+ 3.94 resinC*lightLED - 17.74 resinD*lightLED
$$

So, if `light` = Halogen, our equation is:

$$
strength = 17.77 + 2.13 resinB + 4.77 resinC + 22.53 resinD 
$$

And if `light` = LED, our equation is:

$$
strength = 19.06 + 5.50 resinB + 8.71 resinC + 4.79 resinD 
$$

Note that both the intercept and the slopes change as a result of the interaction. The model yields a different prediction for every possible combination of a `resin` type and a `light` source.

## Comparing Individual Combinations of `resin` and `light`

To make comparisons between individual combinations of a `resin` type and a `light` source, using something like Tukey's HSD approach for multiple comparisons, we first refit the model using the `aov` structure, rather than `lm`.

```{r aov_fit_for_c3_m1}
c3m1_aov <- aov(strength ~ resin * light, data = bonding)

summary(c3m1_aov)
```

And now, we can obtain Tukey HSD comparisons (which will maintain an overall 95% family-wise confidence level) across the `resin` types, the `light` sources, and the combinations, with the TukeyHSD command. This approach is only completely appropriate if these comparisons are pre-planned, and if the design is balanced (as this is, with the same sample size for each combination of a `light` source and `resin` type.)

```{r Tukey_HSD_for_aov_fit_for_c3_m1}
TukeyHSD(c3m1_aov)
```

One conclusion from this is that the combination of D and Halogen is significantly stronger than each of the other seven combinations.

## The `bonding` model without Interaction

It seems incorrect in this situation to fit a model without the interaction term, but we'll do so just so you can see what's involved.

```{r c3_m2_bonding_anova_without_interaction}
c3_m2 <- lm(strength ~ resin + light, data = bonding)

summary(c3_m2)
```

In the no-interaction model, if `light` = Halogen, our equation is:

$$
strength = 19.07 + 3.82 resinB + 6.74 resinC + 13.66 resinD
$$

And if `light` = LED, our equation is:

$$
strength = 17.75 + 3.82 resinB + 6.74 resinC + 13.66 resinD
$$

So, in the no-interaction model, only the intercept changes.

```{r anova_c3_m2_without_interaction}
anova(c3_m2)
```

And, it appears, if we ignore the interaction, then `resin` type has a significant impact on `strength` but `light` source doesn't. This is a bit clearer, when we look at boxplots of the separated `light` and `resin` groups. 

```{r boxplots_c3_bonding_without_interaction}
p1 <- ggplot(bonding, aes(x = light, y = strength)) + 
    geom_boxplot()
p2 <- ggplot(bonding, aes(x = resin, y = strength)) +
    geom_boxplot()

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

## `cortisol`: A Hypothetical Clinical Trial

156 adults who complained of problems with a high-stress lifestyle were enrolled in a hypothetical clinical trial of the effectiveness of a behavioral intervention designed to help reduce stress levels, as measured by salivary cortisol. 

The subjects were randomly assigned to one of three intervention groups (usual care, low dose, and high dose.) The "low dose" subjects received a one-week intervention with a follow-up at week 5. The "high dose" subjects received a more intensive three-week intervention, with follow up at week 5. 

Since cortisol levels rise and fall with circadian rhythms, the cortisol measurements were taken just after rising for all subjects. These measurements were taken at baseline, and again at five weeks. The difference (baseline - week 5) in cortisol level (in micrograms / l) serves as the primary outcome.

### Codebook and Raw Data for `cortisol`

The data are gathered in the `cortisol` data set. Included are:

Variable  | Description
--------: | --------------------------------
`subject` | subject identification code
`interv`  | intervention group (UC = usual care, Low, High)
`waist`   | waist circumference at baseline (in inches)
`sex`     | male or female
`cort.1`  | salivary cortisol level (microg/l) week 1
`cort.5`  | salivary cortisol level (microg/l) week 5

```{r c3_raw_cortisol_data}
cortisol
```

## Creating a factor combining sex and waist

Next, we'll put the `waist` and `sex` data in the `cortisol` example together. We want to build a second categorical variable (called `fat_est`) combining this information, to indicate "healthy" vs. "unhealthy" levels of fat around the waist. 

- Male subjects whose waist circumference is 40 inches or more, and
- Female subjects whose waist circumference is 35 inches or more, will fall in the "unhealthy" group.

```{r create c3_cortisol_fat_est}
cortisol <- cortisol %>%
    mutate(
        fat_est = factor(case_when(
            sex == "M" & waist >= 40 ~ "unhealthy",
            sex == "F" & waist >= 35 ~ "unhealthy",
            TRUE                     ~ "healthy")),
        cort_diff = cort.1 - cort.5)

summary(cortisol)
```

## A Means Plot for the `cortisol` trial (with standard errors)

Again, we'll start by building up a data set with the summaries we want to plot.

```{r c3_group_to_build_means_plot_cortisol}
cort.sum <- cortisol %>% 
    group_by(interv, fat_est) %>%
    summarize(mean.cort = mean(cort_diff), 
              se.cort = sd(cort_diff)/sqrt(n()))

cort.sum
```

Now, we'll use this new data set to plot the means and standard errors. 

```{r c3_ggplot_means_plot_cortisol}
## The error bars will overlap unless we adjust the position.
pd <- position_dodge(0.2) # move them .1 to the left and right

ggplot(cort.sum, aes(x = interv, y = mean.cort, col = fat_est)) +
    geom_errorbar(aes(ymin = mean.cort - se.cort, 
                      ymax = mean.cort + se.cort),
                  width = 0.2, position = pd) +
    geom_point(size = 2, position = pd) + 
    geom_line(aes(group = fat_est), position = pd) +
    scale_color_manual(values = c("royalblue", "darkred")) +
    theme_bw() +
    labs(y = "Salivary Cortisol Level", x = "Intervention Group",
         title = "Observed Means (+/- SE) of Salivary Cortisol")
```

## A Two-Way ANOVA model for `cortisol` with Interaction

```{r anova_c3_cortisol_with_interaction}
c3_m3 <- lm(cort_diff ~ interv * fat_est, data = cortisol)

anova(c3_m3)
```

Does it seem like we need the interaction term in this case?

```{r}
summary(c3_m3)
```

How do you reconcile the apparent difference in significance levels between this regression summary and the ANOVA table above?

## A Two-Way ANOVA model for `cortisol` without Interaction

### The Graph

```{r boxplots_c3_cortisol_without_interaction}
p1 <- ggplot(cortisol, aes(x = interv, y = cort_diff)) + 
    geom_boxplot()
p2 <- ggplot(cortisol, aes(x = fat_est, y = cort_diff)) +
    geom_boxplot()

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

### The ANOVA Model

```{r anova_c3_cortisol_without_interaction}
c3_m4 <- lm(cort_diff ~ interv + fat_est, data = cortisol)

anova(c3_m4)
```

How do these results compare to those we saw in the model with interaction?

### The Regression Summary

```{r}
summary(c3_m4)
```

### Tukey HSD Comparisons

Without the interaction term, we can make direct comparisons between levels of the intervention, and between levels of the `fat_est` variable. This is probably best done here in a Tukey HSD comparison.

```{r}
TukeyHSD(aov(cort_diff ~ interv + fat_est, data = cortisol))
```

What conclusions can we draw, at a 5% significance level?

## An Emphysema Study: Analysis of Covariance

My source for this example is @Riffenburgh2006, section 18.4. Serum theophylline levels (in mg/dl) were measured in 16 patients with emphysema at baseline, then 5 days later (at the end of a course of antibiotics) and then at 10 days after baseline. Clinicians anticipate that the antibiotic will increase the theophylline level. The data are stored in the `emphysema.csv` data file, and note that the age for patient 5 is not available. 

### Codebook

Variable  | Description
--------: | --------------------------------------------------------
`patient` | ID code
`age`     | patient's age in years
`sex`     | patient's sex (F or M)
`st_base` | patient's serum theophylline at baseline (mg/dl)
`st_day5` | patient's serum theophylline at day 5 (mg/dl)
`st_day10` | patient's serum theophylline at day 10 (mg/dl)

We're going to look at the change from baseline to day 5 as our outcome of interest, since the clinical expectation is that the antibiotic (azithromycin) will increase theophylline levels.

```{r}
emphysema <- emphysema %>% 
    mutate(st_delta = st_day5 - st_base)

emphysema
```

### Does `sex` affect the mean change in theophylline? 

```{r}
emphysema %>% skim(st_delta)
```

```{r}
emphysema %>% group_by(sex) %>% skim(st_delta)
```

Overall, the mean change in theophylline during the course of the antibiotic is -0.99, but this is -3.33 for female patients and 0.41 for male patients.

A one-way ANOVA model looks like this:

```{r}
anova(lm(st_delta ~ sex, data = emphysema))
```

The ANOVA F test finds a statistically significant difference between the mean `st_delta` among males and the mean `st_delta` among females. But is there more to the story?

### Is there an association between `age` and `sex` in this study?

```{r}
emphysema %>% group_by(sex) %>% skim(age)
```

But we note that the male patients are also older than the female patients, on average (mean age for males is 66.4, for females 63.3) 

- Does the fact that male patients are older affect change in theophylline level? 
- And how should we deal with the one missing `age` value (in a male patient)?

### Adding a quantitative covariate, `age`, to the model

We could fit an ANOVA model to predict `st_delta` using `sex` and `age` directly, but only if we categorized `age` into two or more groups. Because `age` is not categorical, we cannot include it in an ANOVA. But if age is an influence, and we don't adjust for it, it may well bias the outcome of our initial ANOVA. With a quantitative variable like `age`, we will need a method called ANCOVA, for **analysis of covariance**.

#### The ANCOVA model

ANCOVA in this case is just an ANOVA model with our outcome (`st_delta`) adjusted for a continuous covariate, called `age`. For the moment, we'll ignore the one subject with missing `age` and simply fit the regression model with `sex` and `age`.

```{r}
summary(lm(st_delta ~ sex + age, data = emphysema))
```

This model assumes that the slope of the regression line between `st_delta` and `age` is the same for both sexes. 

Note that the model yields `st_delta` = -6.9 + 3.52 (`sex` = male) + 0.056 `age`, or 

- `st_delta` = -3.38 + 0.056 `age` for female patients, and
- `st_delta` = -6.9 + 0.056 `age` for male patients.

Note that we can test this assumption of equal slopes by fitting an alternative model (with a product term between `sex` and `age`) that doesn't require the assumption, and we'll do that later. 

#### The ANCOVA Table 

First, though, we'll look at the ANCOVA table.

```{r}
anova(lm(st_delta ~ sex + age, data = emphysema))
```

When we tested `sex` without accounting for `age`, we found a *p* value of 0.032, which is less than our usual cutpoint of 0.05. But when we adjusted for `age`, we find that `sex` loses significance, even though `age` is not a significant influence on `st_delta` by itself, according to the ANCOVA table.

### Rerunning the ANCOVA model after simple imputation

We could have *imputed* the missing `age` value for patient 5, rather than just deleting that patient. Suppose we do the simplest potentially reasonable thing to do: insert the mean `age` in where the NA value currently exists.

```{r}
emph_imp <- replace_na(emphysema, list(age = mean(emphysema$age, na.rm = TRUE)))

emph_imp
```

More on simple imputation and missing data is coming soon.

For now, we can rerun the ANCOVA model on this new data set, after imputation...

```{r}
anova(lm(st_delta ~ sex + age, data = emph_imp))
```

When we do this, we see that now the `sex` variable returns to a *p* value below 0.05. Our complete case analysis (which omitted patient 5) gives us a different result than the ANCOVA based on the data after mean imputation.

### Looking at a factor-covariate interaction

Let's run a model including the interaction (product) term between `age` and `sex`, which implies that the slope of `age` on our outcome (`st_delta`) depends on the patient's sex. We'll use the imputed data again. Here is the new ANCOVA table, which suggests that the interaction of `age` and `sex` is small (because it accounts for only a small amount of the total Sum of Squares) and not significant (p = 0.91).

```{r}
anova(lm(st_delta ~ sex * age, data = emph_imp))
```

Since the interaction term is neither substantial nor significant, we probably don't need it here. But let's look at its interpretation anyway, just to fix ideas. To do that, we'll need the coefficients from the underlying regression model.

```{r}
tidy(lm(st_delta ~ sex * age, data = emph_imp))
```

Our ANCOVA model for `st_delta` incorporating the `age` x `sex` product term is -5.65 + 1.72 (sex = M) + 0.037 age + 0.029 (sex = M)(age). So that means:

- our model for females is `st_delta` = -5.65 + 0.037 `age`
- our model for males is `st_delta` = (-5.65 + 1.72) + (0.037 + 0.029) `age`, or -3.93 + 0.066 `age`

but, again, our conclusion from the ANCOVA table is that this increase in complexity (letting both the slope and intercept vary by `sex`) doesn't add much in the way of predictive value for our `st_delta` outcome.

