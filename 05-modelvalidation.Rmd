## (DRAFT material) How might we validate this model? 

Here's some early code for that issue, which is built on some material by David Robinson at https://rpubs.com/dgrtwo/cv-modelr

This bit of code performs what is called *10-crossfold separation*. In words, this approach splits the 896 observations in our data into 10 exclusive partitions of about 90% into a training sample, and the remaining 10% in a test sample. The next part of the code maps a modeling step to the training data, and then fits the resulting model on the test data using the `broom` package's `augment` function. 

I've selected the variables in this case so that the model we'll fit is the `m2_c7` model we've been looking at, although there are several ways to accomplish this.

```{r validation_c2_m7_first_approach}
set.seed(4320118)

models <- smartcle2 %>%
    select(bmi, female, exerany, sleephrs, 
           internet30, alcdays, genhealth) %>%
    crossv_kfold(k = 10) %>%
    mutate(model = map(train, ~ lm(bmi ~ ., data = .)))

predictions <- models %>%
    unnest(map2(model, test, ~ augment(.x, newdata = .y)))

predictions
```

The results are a set of predictions based on the splits into training and test groups (remember there are 10 of them, indexed by `.id`) that describe the complete set of 896 respondents again.

What this lets us now do is calculate the root Mean Squared Prediction Error (RMSE) and Mean Absolute Prediction Error (MAE) for this model (the `c2_m7` model) across these observations, and also to compare that error to a model that simply predicts the mean `bmi` across all patients (the `intercept only` model.) In practice, we could consider two distinct models in doing this work.

```{r validation_c2_m7_first_approach_predsummary}
predictions %>%
    summarize(RMSE_c2_m7 = sqrt(mean((bmi - .fitted) ^2)),
              MAE_c2_m7 = mean(abs(bmi - .fitted)),
              RMSE_interceptonly = sqrt(mean((bmi - mean(bmi))^2)),
              MAE_interceptonly = mean(abs(bmi - mean(bmi))))
```

Another thing we could do with this tibble of predictions we have created is to graph the size of the prediction errors (observed `bmi` minus predicted values in `.fitted`) that our modeling approach makes.

```{r validation_c2_m7_first_approach_errorhistogram}
predictions %>%
    mutate(errors = bmi - .fitted) %>%
    ggplot(., aes(x = errors)) +
    geom_histogram(bins = 30, fill = "darkviolet", col = "yellow") + 
    labs(title = "Cross-Validated Errors in Prediction of BMI",
         subtitle = "Using a model (`c2_m7`) including 6 regression inputs",
         caption = "SMART BRFSS 2016 data for Cleveland-Elyria MMSA, n = 896",
         x = "Error in predicting BMI")
```

## Coming Soon ...

1. Would stepwise regression help us build a better model for `bmi`?
    - Is there a better approach for variable selection? What's this I hear about "best subsets", for example?
2. How should we think about potential transformations of these predictors?
    - What's a Spearman rho-squared plot, and how might it help us decide how to spend degrees of freedom on non-linear terms better?
3. How do we deal with missing data in fitting and evaluating a linear regression model if we don't actually want to drop all of the incomplete cases?
4. How can we use the `ols` tool in the `rms` package to fit regression models?
5. How can we use the tools in the `arm` package to fit and evaluate regression models?

